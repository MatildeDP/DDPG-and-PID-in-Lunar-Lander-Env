<!doctype html><html lang="en"><head><script defer src="https://cdn.optimizely.com/js/16180790160.js"></script><title data-rh="true">Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming | by James Dellinger | Towards Data Science</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2019-04-04T17:08:37.064Z"/><meta data-rh="true" name="title" content="Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming | by James Dellinger | Towards Data Science"/><meta data-rh="true" property="og:title" content="Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming"/><meta data-rh="true" property="twitter:title" content="Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming"/><meta data-rh="true" name="twitter:site" content="@TDataScience"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/954fb9b47c79"/><meta data-rh="true" property="al:android:url" content="medium://p/954fb9b47c79"/><meta data-rh="true" property="al:ios:url" content="medium://p/954fb9b47c79"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="I’d like to invite you to join me on an exploration through different approaches to initializing layer weights in neural networks. Step-by-step, through various short experiments and thought…"/><meta data-rh="true" property="og:description" content="Exploring the evolution of initializing layer weights in neural networks: from old-school to Xavier, and arriving finally at Kaiming init."/><meta data-rh="true" property="twitter:description" content="Exploring the evolution of initializing layer weights in neural networks: from old-school to Xavier, and arriving finally at Kaiming init."/><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"/><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*AcZIzXFAJm_ZafRKleF_0g.png"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*AcZIzXFAJm_ZafRKleF_0g.png"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" property="article:author" content="https://medium.com/@jamesdell"/><meta data-rh="true" name="twitter:creator" content="@jamrdell"/><meta data-rh="true" name="author" content="James Dellinger"/><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="11 min read"/><meta data-rh="true" name="parsely-post-id" content="954fb9b47c79"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://medium.com/@jamesdell"/><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/954fb9b47c79"/><link data-rh="true" rel="icon" href="https://miro.medium.com/fit/c/256/256/1*ChFMdf--f5jbm-AYv6VdYA@2x.png"/><link data-rh="true" rel="preload" href="https://miro.medium.com/max/2802/1*sfUruIusLq6tbpLx0sDYZQ.png" as="image"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*AcZIzXFAJm_ZafRKleF_0g.png"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fweight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79","dateCreated":"2019-04-03T08:43:45.838Z","datePublished":"2019-04-03T08:43:45.838Z","dateModified":"2019-04-04T17:08:37.166Z","headline":"Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming","name":"Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming","description":"I’d like to invite you to join me on an exploration through different approaches to initializing layer weights in neural networks. Step-by-step, through various short experiments and thought…","identifier":"954fb9b47c79","keywords":["Lite:true","Tag:Deep Learning","Tag:Neural Networks","Tag:Weight Initialization","Tag:Towards Data Science","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:3"],"author":{"@type":"Person","name":"James Dellinger","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@jamesdell"},"creator":["James Dellinger"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F330\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fweight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"}</script><link rel="preload" href="https://cdn.optimizely.com/js/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="531" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-moz-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-webkit-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-moz-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-webkit-keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{height:25px}.r{fill:rgba(41, 41, 41, 1)}.s{display:block}.t{margin-bottom:36px}.v{width:100%}.w{overflow-x:scroll}.x{white-space:nowrap}.y{scrollbar-width:none}.z{-ms-overflow-style:none}.ab::-webkit-scrollbar{display:none}.ac{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.ae{min-height:184px}.ah{flex-direction:column}.ai{background-color:#355876}.aj{display:none}.al{border-bottom:none}.am{position:relative}.an{z-index:500}.at{max-width:1192px}.au{min-width:0}.av{height:62px}.aw{flex-direction:row}.ax{flex:1 0 auto}.ay{margin-left:0px}.az{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.ba{font-size:14px}.bb{line-height:20px}.bc{color:rgba(197, 210, 225, 1)}.bd{color:rgba(233, 241, 250, 1)}.be{fill:rgba(233, 241, 250, 1)}.bf{font-size:inherit}.bg{border:inherit}.bh{font-family:inherit}.bi{letter-spacing:inherit}.bj{font-weight:inherit}.bk{padding:0}.bl{margin:0}.bp:disabled{cursor:default}.bq:disabled{color:rgba(163, 208, 162, 0.5)}.br:disabled{fill:rgba(163, 208, 162, 0.5)}.bs{min-height:115px}.bt{justify-content:space-between}.bz{align-items:flex-start}.ca{margin-bottom:0px}.cb{margin-top:-32px}.cc{flex-wrap:wrap}.cf{margin-top:32px}.cg{margin-right:24px}.ci{height:35px}.cj{width:112px}.ck{flex:0 0 auto}.cl{justify-self:flex-end}.cm{margin-right:12px}.cn{height:32px}.co{overflow:visible}.cp{border-radius:1000px}.cq{background-color:rgba(53, 88, 118, 0.8)}.cr{fill:rgba(197, 210, 225, 1)}.cs{color:inherit}.ct{fill:rgba(117, 117, 117, 1)}.cu{outline:none}.cv{padding:4px}.cw{margin-left:8px}.cx{margin-right:10px}.cy{display:inline-block}.cz{border:none}.da{font:inherit}.db{font-size:16px}.dc{opacity:0}.dd{background-color:transparent}.de::placeholder{color:rgba(197, 210, 225, 1)}.df{padding:0px}.dg{width:0px}.dh{transition:width 140ms ease-in, padding 140ms ease-in}.di{fill:inherit}.dl:disabled{color:rgba(197, 210, 225, 1)}.dm:disabled{fill:rgba(197, 210, 225, 1)}.dn{padding:4px 12px 6px}.do{background:0}.dp{border-color:rgba(215, 226, 238, 1)}.dr:disabled{cursor:inherit}.ds:disabled{opacity:0.3}.dt:disabled:hover{color:rgba(233, 241, 250, 1)}.du:disabled:hover{fill:rgba(233, 241, 250, 1)}.dv:disabled:hover{border-color:rgba(215, 226, 238, 1)}.dw{border-radius:99em}.dx{border-width:1px}.dy{border-style:solid}.dz{box-sizing:border-box}.ea{text-decoration:none}.eb{fill:rgba(251, 255, 255, 1)}.ec{padding-top:1px}.ed{height:70px}.ef{line-height:24px}.eg:before{margin-bottom:-10px}.eh:before{content:""}.ei:before{display:table}.ej:before{border-collapse:collapse}.ek:after{margin-top:-6px}.el:after{content:""}.em:after{display:table}.en:after{border-collapse:collapse}.eo{color:rgba(117, 117, 117, 1)}.ep{margin-right:32px}.eq{margin-bottom:-16px}.er{margin-top:-14px}.es{color:rgba(255, 255, 255, 1)}.et{padding:7px 16px 9px}.eu{fill:rgba(255, 255, 255, 1)}.ev{background:rgba(102, 138, 170, 1)}.ew{border-color:rgba(102, 138, 170, 1)}.ez:disabled:hover{background:rgba(102, 138, 170, 1)}.fa:disabled:hover{border-color:rgba(102, 138, 170, 1)}.fb{display:inline-flex}.fe:disabled{color:rgba(117, 117, 117, 1)}.ff:disabled{fill:rgba(117, 117, 117, 1)}.fg{margin-left:12px}.fh{margin:0 12px}.fi{position:absolute}.fj{right:24px}.fk{margin:0px}.fl{border:0px}.fm{cursor:pointer}.fn{stroke:rgba(117, 117, 117, 1)}.fq{border-top:none}.fr{left:0}.fs{position:fixed}.ft{right:0}.fu{top:0}.fv{visibility:hidden}.fx{height:60px}.ga{color:rgba(102, 138, 170, 1)}.gb{fill:rgba(102, 138, 170, 1)}.ge{padding-left:24px}.gf{padding-right:24px}.gg{margin-left:auto}.gh{margin-right:auto}.gi{max-width:728px}.gj{background:rgba(255, 255, 255, 1)}.gk{border:1px solid rgba(230, 230, 230, 1)}.gl{border-radius:4px}.gm{box-shadow:0 1px 4px rgba(230, 230, 230, 1)}.gn{max-height:100vh}.go{overflow-y:auto}.gp{top:calc(100vh + 100px)}.gq{bottom:calc(100vh + 100px)}.gr{width:10px}.gs{pointer-events:none}.gt{word-break:break-word}.gu{word-wrap:break-word}.gv:after{display:block}.gw:after{clear:both}.gx{max-width:680px}.gy{line-height:1.23}.gz{letter-spacing:0}.ha{font-style:normal}.hb{font-weight:700}.hw{margin-bottom:-0.27em}.hx{color:rgba(41, 41, 41, 1)}.ib{border-radius:50%}.ic{height:28px}.id{width:28px}.ie{margin:0 4px}.if{margin:0 7px}.ig{align-items:flex-end}.ip{padding-right:8px}.iq{margin-right:8px}.ir{fill:rgba(61, 61, 61, 1)}.is{margin-right:-4px}.iv{line-height:1.58}.iw{letter-spacing:-0.004em}.ix{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.jn{margin-top:24px}.jo{margin-bottom:-0.46em}.ju{text-decoration:underline}.jv{line-height:1.18}.jw{letter-spacing:-0.022em}.jx{font-weight:500}.ks{margin-bottom:-0.31em}.ky{max-width:1400px}.le{clear:both}.lg{cursor:zoom-in}.lh{z-index:auto}.lj{transition:opacity 100ms 400ms}.lk{height:100%}.ll{overflow:hidden}.lm{will-change:transform}.ln{transform:translateZ(0)}.lo{margin:auto}.lp{background-color:rgba(242, 242, 242, 1)}.lq{padding-bottom:6.428571428571429%}.lr{height:0}.ls{filter:blur(20px)}.lt{transform:scale(1.1)}.lu{visibility:visible}.lv{font-style:italic}.lw{padding-bottom:19.285714285714285%}.lx{padding-bottom:28.428571428571427%}.ly{padding-bottom:25.714285714285715%}.lz{padding-bottom:15.142857142857142%}.ma{padding:20px}.mb{background:rgba(242, 242, 242, 1)}.mc{overflow-x:auto}.md{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.me{margin-top:-0.09em}.mf{margin-bottom:-0.09em}.mg{white-space:pre-wrap}.mh{padding-bottom:43.99999999999999%}.mi{padding-bottom:31.71428571428571%}.mj{padding-bottom:44.14285714285714%}.mk{max-width:371px}.ml{padding-bottom:71.1590296495957%}.mm{margin-top:10px}.mn{text-align:center}.mq{padding-bottom:33.142857142857146%}.mr{padding-bottom:23.571428571428573%}.ms{padding-bottom:16.142857142857142%}.mt{padding-bottom:33.85714285714286%}.mu{padding-bottom:34.285714285714285%}.mv{padding-bottom:32.14285714285714%}.mw{max-width:357px}.mx{padding-bottom:77.87114845938375%}.my{padding-bottom:39.42857142857142%}.mz{padding-bottom:10.428571428571429%}.na{padding-bottom:31.857142857142854%}.nb{list-style-type:decimal}.nc{margin-left:30px}.nd{padding-left:0px}.nj{padding-bottom:36.142857142857146%}.nk{padding-bottom:26.142857142857146%}.nl{padding-bottom:57.85714285714286%}.nm{will-change:opacity}.nn{width:188px}.no{left:50%}.np{transform:translateX(406px)}.nq{top:calc(65px + 54px + 14px)}.nt{will-change:opacity, transform}.nu{transform:translateY(159px)}.nw{width:131px}.nx{padding-bottom:28px}.ny{border-bottom:1px solid rgba(230, 230, 230, 1)}.nz{padding-bottom:5px}.oa{padding-top:14px}.ob{padding-top:28px}.oc{margin-bottom:19px}.od{margin-left:-3px}.oj{outline:0}.ok{border:0}.ol{user-select:none}.om> svg{pointer-events:none}.oo{-webkit-user-select:none}.oy button{text-align:left}.oz{opacity:1}.pa{padding-right:9px}.pj{margin-top:40px}.pk{border-top:3px solid rgba(102, 138, 170, 1)}.pl{padding:32px 32px 26px 32px}.pm{margin-top:8px}.pn{margin-bottom:25px}.po{background-color:rgba(250, 250, 250, 1)}.pq{padding-bottom:0px}.pr{padding-top:4px}.ps{font-size:13px}.pt{padding-bottom:10px}.pu{padding-top:8px}.qf{margin:10px 20px 10px 0}.qh{padding:7px 20px 9px}.qi{margin:10px 0 10px 0}.qj{max-width:380px}.qk{padding-bottom:25px}.ql{margin-top:25px}.qm{max-width:155px}.qq{top:1px}.re{margin-left:-1px}.rf{margin-left:-4px}.rn{padding-bottom:40px}.ro{list-style-type:none}.rp{margin-bottom:8px}.rq{line-height:22px}.rr{border-radius:3px}.rs{padding:5px 10px}.rt{padding-bottom:4px}.ru{padding-top:32px}.sf{text-overflow:ellipsis}.sg{display:-webkit-box}.sh{-webkit-line-clamp:1}.si{-webkit-box-orient:vertical}.sk{padding-top:5px}.sl{padding-right:168px}.sm{padding-top:25px}.ss{max-width:100%}.st{margin-bottom:96px}.su{padding:32px 0}.sv{background-color:rgba(0, 0, 0, 0.9)}.sz:disabled{color:rgba(255, 255, 255, 0.7)}.ta:disabled{fill:rgba(255, 255, 255, 0.7)}.tb{height:22px}.tc{width:200px}.te{color:rgba(255, 255, 255, 0.98)}.ti{color:rgba(255, 255, 255, 0.7)}.tl{margin-right:16px}.bm:hover{cursor:pointer}.bn:hover{color:rgba(251, 255, 255, 1)}.bo:hover{fill:rgba(251, 255, 255, 1)}.dj:hover{color:rgba(242, 248, 253, 1)}.dk:hover{fill:rgba(242, 248, 253, 1)}.dq:hover{border-color:rgba(251, 255, 255, 1)}.ex:hover{background:rgba(90, 118, 144, 1)}.ey:hover{border-color:rgba(90, 118, 144, 1)}.fc:hover{color:rgba(25, 25, 25, 1)}.fd:hover{fill:rgba(25, 25, 25, 1)}.gc:hover{color:rgba(90, 118, 144, 1)}.gd:hover{fill:rgba(90, 118, 144, 1)}.it:hover{fill:rgba(8, 8, 8, 1)}.oq:hover{fill:rgba(117, 117, 117, 1)}.sx:hover{color:rgba(255, 255, 255, 0.99)}.sy:hover{fill:rgba(255, 255, 255, 0.99)}.tf:hover{text-decoration:underline}.iu:focus{fill:rgba(8, 8, 8, 1)}.li:focus{transform:scale(1.01)}.op:focus{fill:rgba(117, 117, 117, 1)}.on:active{border-style:none}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.as{margin:0 64px}.hs{font-size:46px}.ht{margin-top:0.6em}.hu{line-height:56px}.hv{letter-spacing:-0.011em}.in{margin-left:30px}.jk{font-size:21px}.jl{line-height:32px}.jm{letter-spacing:-0.003em}.jt{margin-top:2em}.ko{font-size:22px}.kp{margin-top:1.72em}.kq{line-height:28px}.kr{letter-spacing:0}.kx{margin-top:0.86em}.ld{margin-top:56px}.ni{margin-top:1.05em}.oi{margin-right:5px}.ox{margin-top:5px}.pi{padding-left:6px}.qd{font-size:16px}.qe{line-height:24px}.qs{display:inline-block}.qx{margin-left:7px}.qy{margin-top:8px}.rd{width:25px}.rl{padding-left:7px}.rm{top:3px}.sd{font-size:20px}.se{max-height:24px}.sr{margin:0}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.im{margin-left:30px}.mo{margin-left:auto}.mp{text-align:center}.oh{margin-right:5px}.ow{margin-top:5px}.ph{padding-left:6px}.qr{display:inline-block}.qv{margin-left:7px}.qw{margin-top:8px}.rc{width:25px}.rj{padding-left:7px}.rk{top:3px}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.il{margin-left:30px}.og{margin-right:5px}.ov{margin-top:5px}.pf{padding-left:6px}.pg{top:3px}.qp{display:inline-block}.qt{margin-left:7px}.qu{margin-top:8px}.rb{width:15px}.ri{padding-left:3px}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.u{margin-bottom:20px}.af{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.ag{min-height:230px}.ak{display:block}.bu{min-height:98px}.bv{display:flex}.bw{align-items:flex-start}.bx{flex-direction:column}.by{justify-content:flex-end}.cd{margin-bottom:28px}.ce{margin-top:0px}.ch{margin-top:28px}.ee{margin:0}.fo{border-top:1px solid rgba(230, 230, 230, 1)}.fp{border-bottom:1px solid rgba(230, 230, 230, 1)}.fy{align-items:center}.fz{flex:1 0 auto}.hz{margin-top:32px}.ia{flex-direction:column-reverse}.ij{margin-bottom:30px}.ik{margin-left:0px}.of{margin-left:8px}.ot{margin-top:2px}.ou{margin-right:8px}.pd{padding-left:6px}.pe{top:3px}.pp{padding:24px 24px 28px 24px}.qo{display:inline-block}.ra{width:15px}.rh{padding-left:3px}.sw{padding:32px 0}.td{width:140px}.tg{margin-bottom:16px}.th{margin-top:30px}.tj{width:100%}.tk{flex-direction:row}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ao{margin:0 24px}.hc{font-size:32px}.hd{margin-top:0.64em}.he{line-height:40px}.hf{letter-spacing:-0.016em}.hy{margin-top:32px}.ih{margin-bottom:30px}.ii{margin-left:0px}.iy{font-size:18px}.iz{line-height:28px}.ja{letter-spacing:-0.003em}.jp{margin-top:1.56em}.jy{font-size:20px}.jz{margin-top:1.23em}.ka{line-height:24px}.kb{letter-spacing:0}.kt{margin-top:0.67em}.kz{margin-top:40px}.ne{margin-top:1.34em}.oe{margin-left:8px}.or{margin-top:2px}.os{margin-right:8px}.pb{padding-left:6px}.pc{top:3px}.pv{font-size:14px}.pw{line-height:20px}.qg{margin:10px 0 0 0}.qn{display:inline-block}.qz{width:15px}.rg{padding-left:3px}.rv{font-size:16px}.rw{max-height:20px}.sn{margin:0}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ar{margin:0 64px}.ho{font-size:46px}.hp{margin-top:0.6em}.hq{line-height:56px}.hr{letter-spacing:-0.011em}.jh{font-size:21px}.ji{line-height:32px}.jj{letter-spacing:-0.003em}.js{margin-top:2em}.kk{font-size:22px}.kl{margin-top:1.72em}.km{line-height:28px}.kn{letter-spacing:0}.kw{margin-top:0.86em}.lc{margin-top:56px}.nh{margin-top:1.05em}.qb{font-size:16px}.qc{line-height:24px}.sb{font-size:20px}.sc{max-height:24px}.sq{margin:0}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.aq{margin:0 48px}.hk{font-size:46px}.hl{margin-top:0.6em}.hm{line-height:56px}.hn{letter-spacing:-0.011em}.je{font-size:21px}.jf{line-height:32px}.jg{letter-spacing:-0.003em}.jr{margin-top:2em}.kg{font-size:22px}.kh{margin-top:1.72em}.ki{line-height:28px}.kj{letter-spacing:0}.kv{margin-top:0.86em}.lb{margin-top:56px}.ng{margin-top:1.05em}.pz{font-size:16px}.qa{line-height:24px}.rz{font-size:20px}.sa{max-height:24px}.sp{margin:0}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ap{margin:0 24px}.hg{font-size:32px}.hh{margin-top:0.64em}.hi{line-height:40px}.hj{letter-spacing:-0.016em}.jb{font-size:18px}.jc{line-height:28px}.jd{letter-spacing:-0.003em}.jq{margin-top:1.56em}.kc{font-size:20px}.kd{margin-top:1.23em}.ke{line-height:24px}.kf{letter-spacing:0}.ku{margin-top:0.67em}.la{margin-top:40px}.nf{margin-top:1.34em}.px{font-size:14px}.py{line-height:20px}.rx{font-size:16px}.ry{max-height:20px}.so{margin:0}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="print">.io{display:none}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.fw{animation:k2 .2s ease-in-out both}.lf{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.nr{transition:opacity 200ms}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (max-width: 1230px)">.ns{display:none}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="all and (max-width: 1198px)">.nv{display:none}</style><style type="text/css" data-fela-rehydration="531" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.sj{max-height:none}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><div class="s"><div class="t s u"><div class="ac ae s af ag"><div class="n ah ai"><div class="aj ak"><div class="al s am an"><div class="n p"><div class="ao ap aq ar as at au v"><div class="av n o"><div class="n o aw ax"><div class="ay aj ak"><span class="az b ba bb bc"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F954fb9b47c79&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----954fb9b47c79--------------------------------" class="bd be bf bg bh bi bj bk bl bm bn bo bp bq br" rel="noopener nofollow">Open in app</a></span></div></div><a href="https://medium.com/?source=post_page-----954fb9b47c79--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q be"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><div class="n p"><div class="ao ap aq ar as at au v"><div class="bs n o aw bt bu bv bw bx by"><div class="v n bz bt"><div class="n v"><div class="ca cb v n o aw cc cd ce bv bw bx"><div class="cf cg s ch"><a href="/?source=post_page-----954fb9b47c79--------------------------------" aria-label="Publication Homepage" rel="noopener"><div class="ci cj s"><img alt="Towards Data Science" class="" src="https://miro.medium.com/max/224/1*AGyTPCaRzVqL77kFwUwHKg.png" width="112" height="35"/></div></a></div></div></div><div class="n o ck cl an g"><div class="cm cn co n o"><div class="cp cq"><div class="n" aria-hidden="false" aria-describedby="publicationMenu" aria-labelledby="publicationMenu"><button aria-controls="publicationMenu" aria-expanded="false" aria-label="Publication Menu" class="cs ct bf bg bh bi bj bk bl cu bm"><div class="cv s"><svg class="cr bo" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></div></button></div></div><div class="cw cx s"><div class="cp cq"><div class="cy" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"><div class="n"><button class="cs di bf bg bh bi bj bk bl bm dj dk bp dl dm"><span class="cv s"><svg width="25" height="25" viewBox="0 0 25 25" class="cr"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></span></button><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-hidden="true" tabindex="-1" class="cz cu da db bb dc dd bd de am df dg dh" placeholder="Search" value=""/></div></div></div></div><div class="cm s g"><div><a href="https://medium.com/plans?source=upgrade_membership---nav_full----------------------------------" class="az b ba bb bd dn be do dp bn bo dq bm dr ds dt du dv dw dx dy dz cy ea" rel="noopener">Upgrade</a></div></div></div><a href="https://medium.com/?source=post_page-----954fb9b47c79--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q eb"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><div class="s ak"><div class="n p"><div class="ao ap aq ar as at au v"><div class="w x y z ab"><div class="ec ed n o"><div class="s ee"><span class="az b db ef eg eh ei ej ek el em en eo"><div class="n o"><div class="ep s"><div class="eq er s"><div class="cy" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><button class="az b ba bb es et eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea" aria-controls="collectionFollowPopover" aria-expanded="false"><div class="n aw">Follow</div></button></div></div></div><div class="cm fb ah"><a class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener" href="/followers?source=post_page-----954fb9b47c79--------------------------------">581K Followers</a></div><div class="fg s g">·</div><div class="fg s g"><nav class="n o"><span class="fh n ah"><a href="https://towardsdatascience.com/tagged/editors-pick?source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">Editors&#x27; Picks</a></span><span class="fh n ah"><a href="https://towardsdatascience.com/tagged/tds-features?source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">Features</a></span><span class="fh n ah"><a href="https://towardsdatascience.com/tagged/deep-dives?source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">Deep Dives</a></span><span class="fh n ah"><a class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener" href="/how-to-get-the-most-out-of-towards-data-science-3bf37f75a345?source=post_page-----954fb9b47c79--------------------------------">Grow</a></span><span class="fh n ah"><a class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener" href="/questions-96667b06af5?source=post_page-----954fb9b47c79--------------------------------">Contribute</a></span></nav></div><div class="fg n ah g"><a href="/about?source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">About</a></div></div></span></div><div class="aj fi fj ak"><button class="n o p fk fl df fm" aria-label="Expand navbar"><svg width="14" height="14" class="fn"><path d="M0 .5h14M0 7h14M0 13.5h14"></path></svg></button></div></div></div></div></div></div></div><div class="fo fp fq al c fr dc fs ft fu fv an fw"><div class="n p"><div class="ao ap aq ar as at au v"><div class="fx v aj fu an bv fy"><div class="aj bv fy fz"><div class="ay aj ak"><span class="az b ba bb eo"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F954fb9b47c79&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----954fb9b47c79--------------------------------" class="ga gb bf bg bh bi bj bk bl bm gc gd bp bq br" rel="noopener nofollow">Open in app</a></span></div></div><a href="https://medium.com/?source=post_page-----954fb9b47c79--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><article><section class="ge gf gg gh v gi dz s"></section><span class="s"></span><div><div class="fi fr gp gq gr gs"></div><section class="gt gu gv el gw"><div class="n p"><div class="ao ap aq ar as gx au v"><div class=""><h1 id="18f9" class="gy gz ha az hb hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx">Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming</h1><div class="cf"><div class="n bt hy hz ia"><div class="o n"><div><a href="https://medium.com/@jamesdell?source=post_page-----954fb9b47c79--------------------------------" rel="noopener"><img alt="James Dellinger" class="s ib ic id" src="https://miro.medium.com/fit/c/56/56/1*VWdH331WvzjQpoZxCJoqFA.jpeg" width="28" height="28"/></a></div><div class="fg v n cc"><div class="n"><div style="flex:1"><span class="az b ba bb hx"><a href="https://medium.com/@jamesdell?source=post_page-----954fb9b47c79--------------------------------" class="" rel="noopener"><p class="az b ba bb ga">James Dellinger</p></a></span></div></div><span class="az b ba bb eo"><a class="" rel="noopener" href="/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79?source=post_page-----954fb9b47c79--------------------------------"><p class="az b ba bb eo"><span class="ie"></span>Apr 3, 2019<span class="if">·</span>11 min read</p></a></span></div></div><div class="n ig ih ii ij ik il im in io"><div class="n o"><div class="ip s"><div class="cy" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="iq s"><div><div class="ir"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="s ax"><div class="cy" aria-hidden="false" aria-describedby="creatorActionOverflowMenu" aria-labelledby="creatorActionOverflowMenu"><div class="cy" aria-hidden="false" aria-describedby="removeFromPublicationPopover" aria-labelledby="removeFromPublicationPopover"><div class="is s ck"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-controls="creatorActionOverflowMenu" aria-expanded="false" aria-label="More options"><svg class="r it iu" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><p id="4960" class="iv iw ha ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo gt hx">I’d like to invite you to join me on an exploration through different approaches to initializing layer weights in neural networks. Step-by-step, through various short experiments and thought exercises, we’ll discover why adequate weight initialization is so important in training deep neural nets. Along the way we’ll cover various approaches that researchers have proposed over the years, and finally drill down on what works best for the contemporary network architectures that you’re most likely to be working with.</p><p id="5721" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">The examples to follow<span id="rmm"> </span>come from my own <a href="https://nbviewer.jupyter.org/github/jamesdellinger/fastai_deep_learning_course_part2_v3/blob/master/02_fully_connected_my_reimplementation.ipynb?flush_cache=true" class="cs ju" rel="noopener nofollow">re-implementation</a> of a set of notebooks that <a href="https://www.usfca.edu/faculty/jeremy-howard" class="cs ju" rel="noopener nofollow">Jeremy Howard</a> covered in the <a href="https://www.fast.ai/2019/03/06/fastai-swift/" class="cs ju" rel="noopener nofollow">latest version</a> of fast.ai’s Deep Learning Part II course, currently <a href="https://www.usfca.edu/data-institute/certificates/deep-learning-part-two" class="cs ju" rel="noopener nofollow">being held this spring, 2019, at USF’s Data Institute</a>.</p><h2 id="a36f" class="jv jw ha az jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hx">Why Initialize Weights</h2><p id="f4a5" class="iv iw ha ix b iy kt iz ja jb ku jc jd je kv jf jg jh kw ji jj jk kx jl jm jo gt hx">The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all.</p><p id="d79e" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Matrix multiplication is the essential math operation of a neural network. In deep neural nets with several layers, one forward pass simply entails performing consecutive matrix multiplications at each layer, between that layer’s inputs and weight matrix. The product of this multiplication at one layer becomes the inputs of the subsequent layer, and so on and so forth.</p><p id="a600" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">For a quick-and-dirty example that illustrates this, let’s pretend that we have a vector <strong class="ix hb">x</strong> that contains some network inputs. It’s standard practice when training neural networks to ensure that our inputs’ values are scaled such that they fall inside such a normal distribution with a mean of 0 and a standard deviation of 1.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="lq lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*ne6gE3vogzJp53lwAYxtGw.png?q=20" width="1400" height="90" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="90" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*ne6gE3vogzJp53lwAYxtGw.png" width="1400" height="90" srcSet="https://miro.medium.com/max/552/1*ne6gE3vogzJp53lwAYxtGw.png 276w, https://miro.medium.com/max/1104/1*ne6gE3vogzJp53lwAYxtGw.png 552w, https://miro.medium.com/max/1280/1*ne6gE3vogzJp53lwAYxtGw.png 640w, https://miro.medium.com/max/1400/1*ne6gE3vogzJp53lwAYxtGw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="f011" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Let’s also pretend that we have a simple 100-layer network with no activations , and that each layer has a matrix <strong class="ix hb">a</strong> that contains the layer’s weights. In order to complete a single forward pass we’ll have to perform a matrix multiplication between layer inputs and weights at each of the hundred layers, which will make for a grand total of <em class="lv">100</em> consecutive matrix multiplications.</p><p id="efc6" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">It turns out that initializing the values of layer weights from the same standard normal distribution to which we scaled our inputs is never a good idea. To see why, we can simulate a forward pass through our hypothetical network.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="lw lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*IK15xc15E1LJGlDP1FEXFA.png?q=20" width="1400" height="270" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="270" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*IK15xc15E1LJGlDP1FEXFA.png" width="1400" height="270" srcSet="https://miro.medium.com/max/552/1*IK15xc15E1LJGlDP1FEXFA.png 276w, https://miro.medium.com/max/1104/1*IK15xc15E1LJGlDP1FEXFA.png 552w, https://miro.medium.com/max/1280/1*IK15xc15E1LJGlDP1FEXFA.png 640w, https://miro.medium.com/max/1400/1*IK15xc15E1LJGlDP1FEXFA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="6f3f" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Whoa! Somewhere during those 100 multiplications, the layer outputs got so big that even the computer wasn’t able to recognize their standard deviation and mean as numbers. We can actually see exactly how long that took to happen.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="lx lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*_yr6lnXY-1aXVUWb1MJIYA.png?q=20" width="1400" height="398" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="398" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*_yr6lnXY-1aXVUWb1MJIYA.png" width="1400" height="398" srcSet="https://miro.medium.com/max/552/1*_yr6lnXY-1aXVUWb1MJIYA.png 276w, https://miro.medium.com/max/1104/1*_yr6lnXY-1aXVUWb1MJIYA.png 552w, https://miro.medium.com/max/1280/1*_yr6lnXY-1aXVUWb1MJIYA.png 640w, https://miro.medium.com/max/1400/1*_yr6lnXY-1aXVUWb1MJIYA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="0ba2" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">The activation outputs exploded within 29 of our network’s layers. We clearly initialized our weights to be too large.</p><p id="ab92" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Unfortunately, we also have to worry about preventing layer outputs from vanishing. To see what happens when we initialize network weights to be too small — we’ll scale our weight values such that, while they still fall inside a normal distribution with a mean of 0, they have a standard deviation of 0.01.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="ly lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*z89-cC4jCGPOkp3cnbyr8A.png?q=20" width="1400" height="360" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="360" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*z89-cC4jCGPOkp3cnbyr8A.png" width="1400" height="360" srcSet="https://miro.medium.com/max/552/1*z89-cC4jCGPOkp3cnbyr8A.png 276w, https://miro.medium.com/max/1104/1*z89-cC4jCGPOkp3cnbyr8A.png 552w, https://miro.medium.com/max/1280/1*z89-cC4jCGPOkp3cnbyr8A.png 640w, https://miro.medium.com/max/1400/1*z89-cC4jCGPOkp3cnbyr8A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="6271" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">During the course of the above hypothetical forward pass, the activation outputs completely vanished.</p><p id="08c0" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">To sum it up, if weights are initialized too large, the network won’t learn well. The same happens when weights are initialized too small.</p><h2 id="da92" class="jv jw ha az jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hx">How can we find the sweet spot?</h2><p id="7698" class="iv iw ha ix b iy kt iz ja jb ku jc jd je kv jf jg jh kw ji jj jk kx jl jm jo gt hx">Remember that as mentioned above, the math required to complete a forward pass through a neural network entails nothing more than a succession of matrix multiplications. If we have an output <strong class="ix hb">y</strong> that is the product of a matrix multiplication between our input vector <strong class="ix hb">x</strong> and weight matrix <strong class="ix hb">a</strong>, each element <em class="lv">i</em> in <strong class="ix hb">y</strong> is defined as</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="lz lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*K3AIFVUelCr0z646zrQjPw.png?q=20" width="1400" height="212" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="212" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*K3AIFVUelCr0z646zrQjPw.png" width="1400" height="212" srcSet="https://miro.medium.com/max/552/1*K3AIFVUelCr0z646zrQjPw.png 276w, https://miro.medium.com/max/1104/1*K3AIFVUelCr0z646zrQjPw.png 552w, https://miro.medium.com/max/1280/1*K3AIFVUelCr0z646zrQjPw.png 640w, https://miro.medium.com/max/1400/1*K3AIFVUelCr0z646zrQjPw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="15e5" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">where <em class="lv">i</em> is a given row-index of weight matrix <strong class="ix hb">a</strong>, <em class="lv">k</em> is both a given column-index in weight matrix <strong class="ix hb">a</strong> and element-index in input vector <strong class="ix hb">x</strong>, and <em class="lv">n</em> is the range or total number of elements in <strong class="ix hb">x</strong>. This can also be defined in Python as:</p><pre class="kz la lb lc ld ma mb mc"><span id="316e" class="hx jv jw ha md b db me mf s mg">y[i] = sum([c*d for c,d in zip(a[i], x)])</span></pre><p id="d276" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">We can demonstrate that at a given layer, the matrix product of our inputs <strong class="ix hb">x</strong> and weight matrix <strong class="ix hb">a</strong> that we initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections, which in our example is √512.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mh lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*oJDRa2HOPhe5JV7co5Ig-A.png?q=20" width="1400" height="616" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="616" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*oJDRa2HOPhe5JV7co5Ig-A.png" width="1400" height="616" srcSet="https://miro.medium.com/max/552/1*oJDRa2HOPhe5JV7co5Ig-A.png 276w, https://miro.medium.com/max/1104/1*oJDRa2HOPhe5JV7co5Ig-A.png 552w, https://miro.medium.com/max/1280/1*oJDRa2HOPhe5JV7co5Ig-A.png 640w, https://miro.medium.com/max/1400/1*oJDRa2HOPhe5JV7co5Ig-A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="6b2f" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">This property isn’t surprising if we view it in terms of how matrix multiplication is defined: in order to calculate <strong class="ix hb">y</strong> we sum 512 products of the element-wise multiplication of one element of the inputs <strong class="ix hb">x</strong> by one column of the weights <strong class="ix hb">a</strong>. In our example where both <strong class="ix hb">x</strong> and <strong class="ix hb">a</strong> are initialized using standard normal distributions, each of these 512 products would have a mean of 0 and standard deviation of 1.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mi lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*wMTdWrSPOXh8C6XxoSO7pg.png?q=20" width="1400" height="444" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="444" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*wMTdWrSPOXh8C6XxoSO7pg.png" width="1400" height="444" srcSet="https://miro.medium.com/max/552/1*wMTdWrSPOXh8C6XxoSO7pg.png 276w, https://miro.medium.com/max/1104/1*wMTdWrSPOXh8C6XxoSO7pg.png 552w, https://miro.medium.com/max/1280/1*wMTdWrSPOXh8C6XxoSO7pg.png 640w, https://miro.medium.com/max/1400/1*wMTdWrSPOXh8C6XxoSO7pg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="0a28" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">It then follows that the <em class="lv">sum</em> of these 512 products would have a mean of 0, variance of 512, and therefore a standard deviation of √512.</p><p id="b501" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">And this is why in our example above we saw our layer outputs exploding after 29 consecutive matrix multiplications. In the case of our bare-bones 100-layer network architecture, what we’d like is for each layer’s outputs to have a standard deviation of about 1. This conceivably would allow us to repeat matrix multiplications across as many network layers as we want, without activations exploding or vanishing.</p><p id="95b6" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">If we first scale the weight matrix <strong class="ix hb">a</strong> by dividing all its randomly chosen values by √512, the element-wise multiplication that fills in one element of the outputs <strong class="ix hb">y</strong> would now, on average, have a variance of only 1/√512.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mj lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*qDpw22Z8XSDZe5MLLOLUew.png?q=20" width="1400" height="618" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="618" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*qDpw22Z8XSDZe5MLLOLUew.png" width="1400" height="618" srcSet="https://miro.medium.com/max/552/1*qDpw22Z8XSDZe5MLLOLUew.png 276w, https://miro.medium.com/max/1104/1*qDpw22Z8XSDZe5MLLOLUew.png 552w, https://miro.medium.com/max/1280/1*qDpw22Z8XSDZe5MLLOLUew.png 640w, https://miro.medium.com/max/1400/1*qDpw22Z8XSDZe5MLLOLUew.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="ac76" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">This means that the standard deviation of the matrix <strong class="ix hb">y</strong>, which contains each of the 512 values that are generated by way of the matrix multiplication between inputs <strong class="ix hb">x</strong> and weights <strong class="ix hb">a</strong>, would be 1. Let’s confirm this experimentally.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mi lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*5LAW2b1nDhPB9k7gt68yZg.png?q=20" width="1400" height="444" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="444" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*5LAW2b1nDhPB9k7gt68yZg.png" width="1400" height="444" srcSet="https://miro.medium.com/max/552/1*5LAW2b1nDhPB9k7gt68yZg.png 276w, https://miro.medium.com/max/1104/1*5LAW2b1nDhPB9k7gt68yZg.png 552w, https://miro.medium.com/max/1280/1*5LAW2b1nDhPB9k7gt68yZg.png 640w, https://miro.medium.com/max/1400/1*5LAW2b1nDhPB9k7gt68yZg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="a5c0" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Now let’s re-run our quick-and-dirty 100-layer network. As before, we first choose layer weights at random from standard normal distribution inside [-1,1], but this time we scale those weights by 1/√<em class="lv">n</em>, where <em class="lv">n</em> is the number of network input connections at a layer, which is 512 in our example.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="ly lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*8-f9bmfeVVWwLLqSoQADSA.png?q=20" width="1400" height="360" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="360" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*8-f9bmfeVVWwLLqSoQADSA.png" width="1400" height="360" srcSet="https://miro.medium.com/max/552/1*8-f9bmfeVVWwLLqSoQADSA.png 276w, https://miro.medium.com/max/1104/1*8-f9bmfeVVWwLLqSoQADSA.png 552w, https://miro.medium.com/max/1280/1*8-f9bmfeVVWwLLqSoQADSA.png 640w, https://miro.medium.com/max/1400/1*8-f9bmfeVVWwLLqSoQADSA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="b143" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Success! Our layer outputs neither exploded nor vanished, even after 100 of our hypothetical layers.</p><p id="57e4" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">While at first glance it may seem like at this point we can call it a day, real-world neural networks aren’t quite as simple as our first example may seem to indicate. For the sake of simplicity, activation functions were omitted. However, we’d never do this in real life. It’s thanks to the placement of these non-linear activation functions at the tail end of network layers, that deep neural nets are able create close approximations of intricate functions that describe real-world phenomena, which can then be used to generate astoundingly impressive predictions, such as the classification of handwriting samples.</p><h2 id="169e" class="jv jw ha az jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hx">Xavier Initialization</h2><p id="3231" class="iv iw ha ix b iy kt iz ja jb ku jc jd je kv jf jg jh kw ji jj jk kx jl jm jo gt hx">Up until a few years ago, most commonly used activation functions were symmetric about a given value, and had ranges that asymptotically approached values that were plus/minus a certain distance from this midpoint. The hyperbolic tangent and softsign functions exemplify this class of activations.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div class="gg gh mk"><div class="lo s am lp"><div class="ml lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*39Dm-zzV98YO-WKCfXgeVg.png?q=20" width="371" height="264" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="371" height="264" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/742/1*39Dm-zzV98YO-WKCfXgeVg.png" width="371" height="264" srcSet="https://miro.medium.com/max/552/1*39Dm-zzV98YO-WKCfXgeVg.png 276w, https://miro.medium.com/max/742/1*39Dm-zzV98YO-WKCfXgeVg.png 371w" sizes="371px" role="presentation"/></noscript></div></div></div><figcaption class="mm mn gi gg gh mo mp az b ba bb eo">Tanh and softsign activation functions. Credit: Sefik Ilkin Serengil’s <a href="https://sefiks.com/2017/11/10/softsign-as-a-neural-networks-activation-function/" class="cs ju" rel="noopener nofollow">blog.</a></figcaption></figure><p id="5a52" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">We’ll add a hyperbolic tangent activation function after each layer our hypothetical 100-layer network, and then see what happens when we use our home-grown weight initialization scheme where layer weights are scaled by 1/√<em class="lv">n.</em></p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mq lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*5Ko4_sp9-58wK_hFL2GCHQ.png?q=20" width="1400" height="464" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="464" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*5Ko4_sp9-58wK_hFL2GCHQ.png" width="1400" height="464" srcSet="https://miro.medium.com/max/552/1*5Ko4_sp9-58wK_hFL2GCHQ.png 276w, https://miro.medium.com/max/1104/1*5Ko4_sp9-58wK_hFL2GCHQ.png 552w, https://miro.medium.com/max/1280/1*5Ko4_sp9-58wK_hFL2GCHQ.png 640w, https://miro.medium.com/max/1400/1*5Ko4_sp9-58wK_hFL2GCHQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="d3a0" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">The standard deviation of activation outputs of the 100th layer is down to about 0.06. This is definitely on the small side, but at least activations haven’t totally vanished!</p><p id="fafa" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">As intuitive as the journey to discovering our home-grown weight init strategy may now seem in retrospect, you may be surprised to hear that as recently as 2010, this was not the conventional approach for initializing weight layers.</p><p id="04d8" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">When Xavier Glorot and Yoshua Bengio published their landmark paper titled <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" class="cs ju" rel="noopener nofollow"><em class="lv">Understanding the difficulty of training deep feedforward neural networks</em></a>, the “commonly used heuristic” to which they compared their experiments was that of initializing weights from a <em class="lv">uniform</em> distribution in [-1,1] and then scaling by 1/√<em class="lv">n</em>.</p><p id="7aad" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">It turns out this “standard” approach doesn’t actually work that well.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mr lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*RJ2AxAObnSszBlJ6_LZKrQ.png?q=20" width="1400" height="330" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="330" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*RJ2AxAObnSszBlJ6_LZKrQ.png" width="1400" height="330" srcSet="https://miro.medium.com/max/552/1*RJ2AxAObnSszBlJ6_LZKrQ.png 276w, https://miro.medium.com/max/1104/1*RJ2AxAObnSszBlJ6_LZKrQ.png 552w, https://miro.medium.com/max/1280/1*RJ2AxAObnSszBlJ6_LZKrQ.png 640w, https://miro.medium.com/max/1400/1*RJ2AxAObnSszBlJ6_LZKrQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="33ac" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Re-running our 100-layer tanh network with “standard” weight initialization caused activation gradients to become infinitesimally small — they’re just about as good as vanished.</p><p id="43db" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">This poor performance is actually what spurred Glorot and Bengio to propose their own weight initialization strategy, which they referred to as “normalized initialization” in their paper, and which is now popularly termed “Xavier initialization.”</p><p id="54ed" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="ms lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*H6t3yYBLlinNRUwmL-d7vw.png?q=20" width="1400" height="226" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="226" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*H6t3yYBLlinNRUwmL-d7vw.png" width="1400" height="226" srcSet="https://miro.medium.com/max/552/1*H6t3yYBLlinNRUwmL-d7vw.png 276w, https://miro.medium.com/max/1104/1*H6t3yYBLlinNRUwmL-d7vw.png 552w, https://miro.medium.com/max/1280/1*H6t3yYBLlinNRUwmL-d7vw.png 640w, https://miro.medium.com/max/1400/1*H6t3yYBLlinNRUwmL-d7vw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="9c07" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">where <em class="lv">nᵢ</em> is the number of incoming network connections, or “fan-in,” to the layer, and <em class="lv">nᵢ₊₁</em> is the number of outgoing network connections from that layer, also known as the “fan-out.”</p><p id="9fb4" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network. In their experiments they observed that Xavier initialization enabled a 5-layer network to maintain near identical variances of its weight gradients across layers.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mt lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*Gelf2ZcKYowsLf5FT4n0BQ.png?q=20" width="1400" height="474" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="474" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*Gelf2ZcKYowsLf5FT4n0BQ.png" width="1400" height="474" srcSet="https://miro.medium.com/max/552/1*Gelf2ZcKYowsLf5FT4n0BQ.png 276w, https://miro.medium.com/max/1104/1*Gelf2ZcKYowsLf5FT4n0BQ.png 552w, https://miro.medium.com/max/1280/1*Gelf2ZcKYowsLf5FT4n0BQ.png 640w, https://miro.medium.com/max/1400/1*Gelf2ZcKYowsLf5FT4n0BQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="mm mn gi gg gh mo mp az b ba bb eo">With Xavier init. Credit: <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" class="cs ju" rel="noopener nofollow">Glorot &amp; Bengio</a>.</figcaption></figure><p id="b727" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Conversely, it turned out that using “standard” initialization brought about a much bigger gap in variance between weight gradients at the network’s lower layers, which were higher, and those at its top-most layers, which were approaching zero.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mu lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*mDilNz4ADDbbr8Qb4d4RqQ.png?q=20" width="1400" height="480" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="480" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*mDilNz4ADDbbr8Qb4d4RqQ.png" width="1400" height="480" srcSet="https://miro.medium.com/max/552/1*mDilNz4ADDbbr8Qb4d4RqQ.png 276w, https://miro.medium.com/max/1104/1*mDilNz4ADDbbr8Qb4d4RqQ.png 552w, https://miro.medium.com/max/1280/1*mDilNz4ADDbbr8Qb4d4RqQ.png 640w, https://miro.medium.com/max/1400/1*mDilNz4ADDbbr8Qb4d4RqQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="mm mn gi gg gh mo mp az b ba bb eo">Without Xavier init. Credit: <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" class="cs ju" rel="noopener nofollow">Glorot &amp; Bengio</a>.</figcaption></figure><p id="fc04" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">To drive the point home, Glorot and Bengio demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy on the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="cs ju" rel="noopener nofollow">CIFAR-10 image classification task</a>.</p><p id="4c2a" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Let’s re-run our 100-layer tanh network once more, this time using Xavier initialization:</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mv lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*3NiBW8yi-gYrOpsy70PNEg.png?q=20" width="1400" height="450" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="450" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*3NiBW8yi-gYrOpsy70PNEg.png" width="1400" height="450" srcSet="https://miro.medium.com/max/552/1*3NiBW8yi-gYrOpsy70PNEg.png 276w, https://miro.medium.com/max/1104/1*3NiBW8yi-gYrOpsy70PNEg.png 552w, https://miro.medium.com/max/1280/1*3NiBW8yi-gYrOpsy70PNEg.png 640w, https://miro.medium.com/max/1400/1*3NiBW8yi-gYrOpsy70PNEg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="92d5" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">In our experimental network, Xavier initialization performs pretty identical to the home-grown method that we derived earlier, where we sampled values from a random normal distribution and scaled by the square root of number of incoming network connections, <em class="lv">n</em>.</p><h2 id="9dba" class="jv jw ha az jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hx">Kaiming Initialization</h2><p id="e8d9" class="iv iw ha ix b iy kt iz ja jb ku jc jd je kv jf jg jh kw ji jj jk kx jl jm jo gt hx">Conceptually, it makes sense that when using activation functions that are symmetric about zero and have outputs inside [-1,1], such as softsign and tanh, we’d want the activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. This is precisely what our home-grown method and Xavier both enable.</p><p id="863f" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">But what if we’re using ReLU activation functions? Would it still make sense to want to scale random initial weight values in the same way?</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div class="gg gh mw"><div class="lo s am lp"><div class="mx lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*njuH4XVXf-l9pR_RorUOrA.png?q=20" width="357" height="278" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="357" height="278" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/714/1*njuH4XVXf-l9pR_RorUOrA.png" width="357" height="278" srcSet="https://miro.medium.com/max/552/1*njuH4XVXf-l9pR_RorUOrA.png 276w, https://miro.medium.com/max/714/1*njuH4XVXf-l9pR_RorUOrA.png 357w" sizes="357px" role="presentation"/></noscript></div></div></div><figcaption class="mm mn gi gg gh mo mp az b ba bb eo">ReLU activation function. Credit: Kanchan Sarkar’s <a href="https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec" class="cs ju" rel="noopener">blog</a>.</figcaption></figure><p id="6233" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">To see what would happen, let’s use a ReLU activation instead of tanh in one of our hypothetical network’s layers and observe the expected standard deviation of its outputs.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="my lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*0C5Beclgsv-_HEaOfV8eJA.png?q=20" width="1400" height="552" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="552" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*0C5Beclgsv-_HEaOfV8eJA.png" width="1400" height="552" srcSet="https://miro.medium.com/max/552/1*0C5Beclgsv-_HEaOfV8eJA.png 276w, https://miro.medium.com/max/1104/1*0C5Beclgsv-_HEaOfV8eJA.png 552w, https://miro.medium.com/max/1280/1*0C5Beclgsv-_HEaOfV8eJA.png 640w, https://miro.medium.com/max/1400/1*0C5Beclgsv-_HEaOfV8eJA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="1765" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">It turns out that when using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, <em class="lv">divided by the square root of two</em>, or √512/√2 in our example.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="mz lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*dZ5UcHx2RTeLXL46YxKgbw.png?q=20" width="1400" height="146" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="146" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*dZ5UcHx2RTeLXL46YxKgbw.png" width="1400" height="146" srcSet="https://miro.medium.com/max/552/1*dZ5UcHx2RTeLXL46YxKgbw.png 276w, https://miro.medium.com/max/1104/1*dZ5UcHx2RTeLXL46YxKgbw.png 552w, https://miro.medium.com/max/1280/1*dZ5UcHx2RTeLXL46YxKgbw.png 640w, https://miro.medium.com/max/1400/1*dZ5UcHx2RTeLXL46YxKgbw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="edc4" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Scaling the values of the weight matrix <strong class="ix hb">a</strong> by this number will cause each individual ReLU layer to have a standard deviation of 1 on average.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="na lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*1G5dke8TAGumS88OKa3TfA.png?q=20" width="1400" height="446" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="446" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*1G5dke8TAGumS88OKa3TfA.png" width="1400" height="446" srcSet="https://miro.medium.com/max/552/1*1G5dke8TAGumS88OKa3TfA.png 276w, https://miro.medium.com/max/1104/1*1G5dke8TAGumS88OKa3TfA.png 552w, https://miro.medium.com/max/1280/1*1G5dke8TAGumS88OKa3TfA.png 640w, https://miro.medium.com/max/1400/1*1G5dke8TAGumS88OKa3TfA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="f1cf" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">As we showed before, keeping the standard deviation of layers’ activations around 1 will allow us to stack several more layers in a deep neural network without gradients exploding or vanishing.</p><p id="3f29" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">This exploration into how to best initialize weights in networks with ReLU-like activations is what motivated Kaiming He et. al. to <a href="https://arxiv.org/pdf/1502.01852.pdf" class="cs ju" rel="noopener nofollow">propose their own initialization scheme</a> that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations.</p><p id="f0db" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">In their 2015 paper, He et. al. demonstrated that deep networks (e.g. a 22-layer CNN) would converge much earlier if the following input weight initialization strategy is employed:</p><ol class=""><li id="1f94" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo nb nc nd hx">Create a tensor with the dimensions appropriate for a weight matrix at a given layer, and populate it with numbers randomly chosen from a standard normal distribution.</li><li id="e8ea" class="iv iw ha ix b iy ne iz ja jb nf jc jd je ng jf jg jh nh ji jj jk ni jl jm jo nb nc nd hx">Multiply each randomly chosen number by <em class="lv">√</em>2/<em class="lv">√n</em> where <em class="lv">n</em> is the number of incoming connections coming into a given layer from the previous layer’s output (also known as the “fan-in”).</li><li id="0e8d" class="iv iw ha ix b iy ne iz ja jb nf jc jd je ng jf jg jh nh ji jj jk ni jl jm jo nb nc nd hx">Bias tensors are initialized to zero.</li></ol><p id="bc86" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">We can follow these directions to implement our own version of Kaiming initialization and verify that it can indeed prevent activation outputs from exploding or vanishing if ReLU is used at all layers of our hypothetical 100-layer network.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="nj lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*RD_c4ayThCkRvODAIMfOEQ.png?q=20" width="1400" height="506" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="506" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*RD_c4ayThCkRvODAIMfOEQ.png" width="1400" height="506" srcSet="https://miro.medium.com/max/552/1*RD_c4ayThCkRvODAIMfOEQ.png 276w, https://miro.medium.com/max/1104/1*RD_c4ayThCkRvODAIMfOEQ.png 552w, https://miro.medium.com/max/1280/1*RD_c4ayThCkRvODAIMfOEQ.png 640w, https://miro.medium.com/max/1400/1*RD_c4ayThCkRvODAIMfOEQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="dbc9" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">As a final comparison, here’s what would happen if we were to use Xavier initialization, instead.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="nk lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*_1RKMYJbwsIa4SYAj8ol-A.png?q=20" width="1400" height="366" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="366" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*_1RKMYJbwsIa4SYAj8ol-A.png" width="1400" height="366" srcSet="https://miro.medium.com/max/552/1*_1RKMYJbwsIa4SYAj8ol-A.png 276w, https://miro.medium.com/max/1104/1*_1RKMYJbwsIa4SYAj8ol-A.png 552w, https://miro.medium.com/max/1280/1*_1RKMYJbwsIa4SYAj8ol-A.png 640w, https://miro.medium.com/max/1400/1*_1RKMYJbwsIa4SYAj8ol-A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="ff7b" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Ouch! When using Xavier to initialize weights, activation outputs have almost completely vanished by the 100th layer!</p><p id="944b" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Incidentally, when they trained even deeper networks that used ReLUs, He et. al. found that a 30-layer CNN using Xavier initialization stalled completely and didn’t learn at all. However, when the same network was initialized according to the three-step procedure outlined above, it enjoyed substantially greater convergence.</p><figure class="kz la lb lc ld le gg gh paragraph-image"><div role="button" tabindex="0" class="lf lg am lh v li"><div class="gg gh ky"><div class="lo s am lp"><div class="nl lr s"><div class="dc lj fi fu fr lk v ll lm ln"><img alt="" class="fi fu fr lk v ls lt lu" src="https://miro.medium.com/max/60/1*AcZIzXFAJm_ZafRKleF_0g.png?q=20" width="1400" height="810" role="presentation"/></div><img alt="" class="dc lj fi fu fr lk v c" width="1400" height="810" role="presentation"/><noscript><img alt="" class="fi fu fr lk v" src="https://miro.medium.com/max/2800/1*AcZIzXFAJm_ZafRKleF_0g.png" width="1400" height="810" srcSet="https://miro.medium.com/max/552/1*AcZIzXFAJm_ZafRKleF_0g.png 276w, https://miro.medium.com/max/1104/1*AcZIzXFAJm_ZafRKleF_0g.png 552w, https://miro.medium.com/max/1280/1*AcZIzXFAJm_ZafRKleF_0g.png 640w, https://miro.medium.com/max/1400/1*AcZIzXFAJm_ZafRKleF_0g.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="mm mn gi gg gh mo mp az b ba bb eo">Convergence of a <strong class="az jx">30-layer</strong> CNN thanks to Kaiming init. Credit: <a href="https://arxiv.org/pdf/1502.01852.pdf" class="cs ju" rel="noopener nofollow">He et. al.</a></figcaption></figure><p id="049e" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">The moral of the story for us is that any network we train from scratch, especially for computer vision applications, will almost certainly contain ReLU activation functions and be several layers deep. In such cases, Kaiming should be our go-to weight init strategy.</p><h2 id="e55f" class="jv jw ha az jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hx">Yes, You Too Can Be a Researcher</h2><p id="343f" class="iv iw ha ix b iy kt iz ja jb ku jc jd je kv jf jg jh kw ji jj jk kx jl jm jo gt hx">Even more importantly, I’m not ashamed to admit that I felt intimidated when I saw the Xavier and Kaiming formulas for the first time. What with their respective square roots of six and two, part of me couldn’t help but feel like they must have been the result of some sort of oracular wisdom I couldn’t hope to fathom on my own. And let’s face it, sometimes the math in deep learning papers can look <a href="https://twitter.com/seluappendix" class="cs ju" rel="noopener nofollow">a lot like hieroglyphics</a>, except with no <a href="https://en.wikipedia.org/wiki/Rosetta_Stone" class="cs ju" rel="noopener nofollow">Rosetta Stone</a> to aid in translation.</p><p id="4889" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">But I think the journey we took here showed us that this knee-jerk response of feeling of intimidated, while wholly understandable, is by no means unavoidable. Although the Kaiming and (especially) the Xavier papers do contain their fair share of math, we saw firsthand how experiments, empirical observation, and some straightforward common sense were enough to help derive the core set of principals underpinning what is currently the most widely-used weight initialization scheme.</p><p id="c773" class="iv iw ha ix b iy jp iz ja jb jq jc jd je jr jf jg jh js ji jj jk jt jl jm jo gt hx">Alternately put: when in doubt, be courageous, try things out, and see what happens!</p></div></div></section></div></article><div class="dc gs fs nt v nu fu nr nv" data-test-id="post-sidebar"><div class="n p"><div class="ao ap aq ar as at au v"><div class="nw n ah"><div class="gs"><div><div class="nx ny s"><div class="nz s"><a href="https://medium.com/@jamesdell?source=post_sidebar--------------------------post_sidebar-----------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener"><h2 class="az jx db bb gz hx gt">James Dellinger</h2></a></div><div class="oa s"><button class="az b ba bb es et eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea">Follow</button></div></div><div class="ob oc od n"><div class="n o"><div class="s am oe of og oh oi"><div class=""><button class="bk oj ok ol fm om on oo r op oq"><svg width="29" height="29" aria-label="clap"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s or os ot ou ov ow ox"><div class="oy"><p class="az b ba bb eo"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff">3.6K<!-- --> </button></p></div></div></div></div><div class="oc s"><button class="fm ok bk"><div class="pa n o aw"><svg width="25" height="25" class="r oz fm oq" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg><div class="s am pb pc pd pe pf pg ph pi"><p class="az b ba bb eo">23<!-- --> </p></div></div></button></div><div><div class="ir"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="dc gs nm fs nn no np nq nr ns"></div><div><div class="pj le n ah p"><div class="n p"><div class="ao ap aq ar as gx au v"><div class="n cc"></div><div class="n o cc"></div><div class="pk pl pm pn po pp"><div class="pq s"><h2 class="az jx jy ka kb kc ke kf kg ki kj kk km kn ko kq kr hx">Sign up for The Variable</h2></div><div class="pr s"><h3 class="az b ps bb hx">By Towards Data Science</h3></div><div class="pt pu s"><p class="az b pv pw px py pz qa qb qc qd qe hx">Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don&#x27;t want to miss.<!-- --> <a href="https://medium.com/towards-data-science/newsletters/the-variable?source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="cs di bf bg bh bi bj bk bl bm bp fe ff ju" rel="noopener">Take a look.</a></p></div><div class="n cc"><div class="qf s qg"><button class="az b db ef es qh eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea"><span class="iq" aria-hidden="true"><svg width="20" height="16" viewBox="0 0 20 16"><path d="M0 .35v15.3h20V.35H0zm6.95 9.38l3.05 2.5 3.05-2.5 4.88 4.73H2.07l4.88-4.73zM1.2 13.64V5.02l4.82 3.94-4.82 4.68zm12.78-4.68l4.82-3.94v8.62l-4.82-4.68zm4.82-7.42v1.94l-8.8 7.2-8.8-7.2V1.54h17.6z"></path></svg></span>Get this newsletter</button></div><div class="qi qj s"><p class="az b ps bb hx">Emails will be sent to <!-- -->mat0503@hotmail.com<!-- -->.<div class="s"><span><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79&amp;source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener"><button class="cs di bf bg bh bi bj bk bl bm bp fe ff ju" target="_blank">Not you?</button></a></span></div></p></div></div></div><div class="qk pj s"><div class="ql n bt io"><div class="n aw"><div class="qm s"><span class="s qn qo qp e d"><div class="n o"><div class="s am oe of og oh oi"><div class=""><div><div class="cy" role="tooltip" aria-hidden="false"><button class="bk oj ok ol fm om on oo r op oq"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s or os ot ou ov ow ox"><div class="am qq oy"><p class="az b ba bb hx"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff">3.6K<span class="s h g f qr qs"> </span></button><span class="s h g f qr qs"></span></p></div></div></div></span><span class="s h g f qr qs"><div class="n bz"><div class="s am oe of"><div class=""><div><div class="cy" role="tooltip" aria-hidden="false"><button class="bk oj ok ol fm om on oo r op oq"><svg width="33" height="33" viewBox="0 0 33 33" aria-label="clap"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="s or os ot ou qt qu qv qw qx qy"><div class="am qq oy"><p class="az b ba bb hx"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff">3.6K<span class="s h g f qr qs"> </span></button><span class="s h g f qr qs"></span></p></div></div></div></span></div><div class="s qz ra rb rc rd"></div><button class="fm ok bk"><div class="pa n o aw"><span class="re s h g f qr qs"><svg width="33" height="33" viewBox="0 0 33 33" fill="none" class="r oz fm oq" aria-label="responses"><path fill-rule="evenodd" clip-rule="evenodd" d="M24.28 25.5l.32-.29c2.11-1.94 3.4-4.61 3.4-7.56C28 11.83 22.92 7 16.5 7S5 11.83 5 17.65s5.08 10.66 11.5 10.66c1.22 0 2.4-.18 3.5-.5l.5-.15.41.33a8.86 8.86 0 0 0 4.68 2.1 7.34 7.34 0 0 1-1.3-4.15v-.43zm1 .45c0 1.5.46 2.62 1.69 4.44.22.32.01.75-.38.75a9.69 9.69 0 0 1-6.31-2.37c-1.2.35-2.46.54-3.78.54C9.6 29.3 4 24.09 4 17.65 4 11.22 9.6 6 16.5 6S29 11.22 29 17.65c0 3.25-1.42 6.18-3.72 8.3z"></path></svg></span><span class="rf s qn qo qp e d"><svg width="25" height="25" class="r oz fm oq" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></span><div class="s am rg pc rh pe ri pg rj rk rl rm"><p class="az b ba bb eo">23<!-- --> </p></div></div></button></div><div class="n o"><div class="ip s"><div class="cy" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="ip s ck"><div><div class="ir"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="cy" aria-hidden="false" aria-describedby="creatorActionOverflowMenu" aria-labelledby="creatorActionOverflowMenu"><div class="cy" aria-hidden="false" aria-describedby="removeFromPublicationPopover" aria-labelledby="removeFromPublicationPopover"><div class="is s ck"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-controls="creatorActionOverflowMenu" aria-expanded="false" aria-label="More options"><svg class="r it iu" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div><div class="rn ql s"><ul class="bk bl"><li class="cy ro iq rp"><a href="/tagged/deep-learning" class="az b ps rq eo rr rs ea s mb">Deep Learning</a></li><li class="cy ro iq rp"><a href="/tagged/neural-networks" class="az b ps rq eo rr rs ea s mb">Neural Networks</a></li><li class="cy ro iq rp"><a href="/tagged/weight-initialization" class="az b ps rq eo rr rs ea s mb">Weight Initialization</a></li><li class="cy ro iq rp"><a href="/tagged/towards-data-science" class="az b ps rq eo rr rs ea s mb">Towards Data Science</a></li></ul></div></div></div><div><div class="n p"><div class="ao ap aq ar as gx au v"></div></div><div class="s io"><div class="rt ru s po"><div class="n p"><div class="ao ap aq ar as gx au v"><div class="n o bt"><h2 class="az jx rv pw rw kb rx py ry kf rz qa sa kj sb qc sc kn sd qe se kr ll sf sg sh si sj hx"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">More from Towards Data Science</a></h2><div class="cy" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><button class="az b ba bb es et eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea" aria-controls="collectionFollowPopover" aria-expanded="false"><div class="n aw">Follow</div></button></div></div><div class="sk sl s"><p class="az b ba bb eo">Your home for data science. A Medium publication sharing concepts, ideas and codes.</p></div></div></div></div></div><div class="sm s po io"><div class="n p"><div class="sn so sp sq sr ss au v"><div class="st pj s"><div class="ss s mn"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="az b ba bb es et eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea" rel="noopener">Read more from <!-- -->Towards Data Science</a></div></div></div></div></div><div class="s gj io"><div class="n p"><div class="ao ap aq ar as at au v"></div></div></div></div></div></div><div class="su s sv sw"><div class="n p"><div class="ao ap aq ar as at au v"><div class="n ah"><div class="n o bt"><a href="https://medium.com/?source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm sx sy bp sz ta" rel="noopener"><svg viewBox="0 0 3940 610" class="eu tb"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><div class="pu tc n bt td bv"><p class="az b db ef te"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm tf bp sz ta" rel="noopener">About</a></p><p class="az b db ef te"><a href="https://help.medium.com/hc/en-us?source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm tf bp sz ta" rel="noopener">Help</a></p><p class="az b db ef te"><a href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm tf bp sz ta" rel="noopener">Legal</a></p></div></div><div class="aj tg th bv"><p class="az b db ef ti">Get the Medium app</p></div><div class="aj tg tj bv tk"><div class="tl s"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm sx sy bp sz ta" rel="noopener nofollow"><img alt="A button that says &#x27;Download on the App Store&#x27;, and if clicked it will lead you to the iOS App store" class="" src="https://miro.medium.com/max/270/1*Crl55Tm6yDNMoucPo1tvDg.png" width="135" height="41"/></a></div><div class="s"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----954fb9b47c79--------------------------------" class="cs di bf bg bh bi bj bk bl bm sx sy bp sz ta" rel="noopener nofollow"><img alt="A button that says &#x27;Get it on, Google Play&#x27;, and if clicked it will lead you to the Google Play store" class="" src="https://miro.medium.com/max/270/1*W_RAPQ62h0em559zluJLdQ.png" width="135" height="41"/></a></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__ = "main-20210416-233736-e40e16886c"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"auroraPage":{"isAuroraPageEnabled":true},"bookReader":{"assets":{},"reader":{"currentAsset":null,"settingsPanelIsOpen":false,"settings":{"fontFamily":"CHARTER","fontScale":"M","publisherStyling":true,"textAlignment":"start","theme":"White","lineSpacing":0,"wordSpacing":0,"letterSpacing":0},"internalNavCounter":0,"currentSelection":null,"highlights":[]}},"cache":{"experimentGroupSet":false,"reason":"This request is not using the cache middleware worker","group":"disabled","tags":[],"serverVariantState":"","middlewareEnabled":false,"cacheStatus":"DYNAMIC"},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true}},"debug":{"requestId":"24e7c454-7caf-4943-be26-a1dc07959c36","branchDeployConfig":null,"hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"61c0a387417c0f5d","ot-tracer-traceid":"6ef7baa2d84b1c96","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fweight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"tracing":{},"config":{"nodeEnv":"production","version":"main-20210416-233736-e40e16886c","isTaggedVersion":false,"target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"main-20210416-233736-e40e16886c"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"main-20210416-233736-e40e16886c","commit":"e40e16886cfaae60b10df8d14b12e22abd7846b3"}},"datacenter":"us"},"googleAnalyticsCode":"UA-24232453-2","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium"},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e","9dc80918cc93","8a9336e5bb4","cef6983b292","54c98c43354d","193b68bd4fba","b7e45b22fec3","55760f21cdc5"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl"},"session":{"xsrf":"2879a5b70437"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"android_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"assign_default_topic_to_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"bane_add_user","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"bane_verify_domain","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"branch_seo_metadata","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"default_seo_post_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"deindex_from_external_search_threshold","valueType":{"__typename":"VariantFlagString","value":"1577865600000"}},{"__typename":"VariantFlag","name":"disable_android_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_resume_reading_toast","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_mobile_featured_chunk","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_post_recommended_from_friends_provider","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_local_currency","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_annual_renewal_reminder_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_parse_expires_at","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook_renewal_failure","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_about_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_general_admission","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_profile_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_sticky_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_tag_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_autotier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automated_mission_control_triggers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_blogrolls","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_text_me_the_app","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding_fonts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cleansweep_double_writes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_client_error_tracking","valueType":{"__typename":"VariantFlagString","value":"none"}},{"__typename":"VariantFlag","name":"enable_confirm_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cta_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_domain_v2_settings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_dedicated_series_tab_api_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_feature_logging","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_generation_pipeline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_earn_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_edit_alt_text","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_email_sign_in_captcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_embedding_based_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_end_of_post_cleanup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_evhead_com_to_ev_medium_com_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expanded_feature_chunk_pool","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_by_resend_rules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_expire_processor","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_from_creators_you_are_enjoying_below_todays_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_global_susi_modal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook_subscription_cancelled","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_highlander_member_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hightower_user_minimum_guarantee","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_who_to_follow_module","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_write_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_post_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_json_logs_trained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex_app_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kiln_for_digest_followed_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_notifications","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pay_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_homepage_for_selected_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_stories","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_unread_notification_count_mutation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_login_code_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_media_resource_try_catch","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_member_only_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_membership_remove_section_a","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_miro_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mission_control","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_digest_rendering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_welcome_digest_rendering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mute","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_checkout_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_collaborative_filtering_data","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_login_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_member_welcome_email_enhancement","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_three_dot_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_user_onboarding_email_enhancement","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_nsfw_filtering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_parsely","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_patronus_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_popularity_feature","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_page_nav_stickiness_removal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_tax_status_clarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_primary_topic_for_mobile","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_design_reminder","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_page_seo_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_publish_to_email_for_publication_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_receipt_notes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_all","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_edit_and_delete","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_moderation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_follow_feed_cache","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rtr_channel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_s3_sites","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_save_to_medium","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_signup_friction","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace_ranker_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_stripegate","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipalti_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trending_posts_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_triton_predictions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trumpland_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_twitter_auth_suggestions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_unfiltered_cf","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"entity_driven_subscription_milestone_2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"filter_last_seen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound"}},{"__typename":"VariantFlag","name":"google_sign_in_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_generic_home_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_pub_follow_email_opt_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"is_not_medium_subscriber","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_fastrak","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_stripe_express","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"low_signal_writer_level","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"make_nav_sticky","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"new_transition_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"provider_for_credit_card_form","valueType":{"__typename":"VariantFlagString","value":"BRAINTREE"}},{"__typename":"VariantFlag","name":"pub_sidebar","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefine_average_post_reading_time","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_evergreen_section","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_low_quality_content_from_tags","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_low_quality_posts_from_internal_search","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_post_post_similarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_suggested_topics_section","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"retrained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"sign_up_with_email_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"skip_sign_in_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"suppress_apple_missing_expires_date_alert","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"use_new_admin_topic_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":{"__ref":"User:dc52fe4ca374"},"meterPost({\"postId\":\"954fb9b47c79\",\"postMeteringOptions\":{\"referrer\":\"\",\"sk\":null,\"source\":null}})":{"__ref":"MeteringInfo:{}"},"postResult({\"id\":\"954fb9b47c79\"})":{"__ref":"Post:954fb9b47c79"}},"User:dc52fe4ca374":{"id":"dc52fe4ca374","__typename":"User","username":"matildedeplace","name":"Matilde de Place","imageId":"0*4ukGw7lV6x1VWFVv","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"mat0503@hotmail.com","unverifiedEmail":"","createdAt":1590164250674,"isAuroraVisible":true,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"styleEditorOnboardingVersionSeen":0,"allowEmailAddressSharingEditorWriter":false,"hasSubdomain":false,"dismissableFlags":[],"hasWebMembershipTrialEnabled":false,"twitterScreenName":"","geolocation":{"__typename":"Geolocation","country":"DK"},"atsQualifiedAt":0},"MeteringInfo:{}":{"__typename":"MeteringInfo","postIds":[],"maxUnlockCount":3,"unlocksRemaining":0},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"CustomStyleSheet:63d23b36fcaa":{"id":"63d23b36fcaa","__typename":"CustomStyleSheet","global":{"__typename":"GlobalStyles","colorPalette":{"__typename":"StyleSheetColorPalette","primary":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}}},"background":null},"fonts":{"__typename":"StyleSheetFonts","font1":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font2":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font3":{"__typename":"StyleSheetFont","name":"SERIF_2"}}},"header":{"__typename":"HeaderStyles","backgroundColor":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"rgb":"355876","alpha":"99"},"postBackgroundColor":null,"backgroundImage":{"__ref":"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png"},"headerScale":"HEADER_SCALE_MEDIUM","horizontalAlignment":"CENTER","backgroundImageDisplayMode":"IMAGE_DISPLAY_MODE_FILL","backgroundImageVerticalAlignment":"END","backgroundColorDisplayMode":"COLOR_DISPLAY_MODE_SOLID","secondaryBackgroundColor":null,"nameColor":null,"nameTreatment":"NAME_TREATMENT_LOGO","postNameTreatment":"NAME_TREATMENT_LOGO","logoImage":{"__ref":"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png"},"logoScale":"HEADER_SCALE_LARGE","taglineColor":{"__typename":"ColorValue","rgb":"ffffff","alpha":"ff"},"taglineTreatment":"TAGLINE_TREATMENT_HEADER"},"navigation":{"__typename":"HeaderNavigation","navItems":[{"__typename":"HeaderNavigationItem","name":"Editors' Picks","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:editors-pick"}],"tagSlugs":["editors-pick"]},{"__typename":"HeaderNavigationItem","name":"Features","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:tds-features"}],"tagSlugs":["tds-features"]},{"__typename":"HeaderNavigationItem","name":"Deep Dives","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:deep-dives"}],"tagSlugs":["deep-dives"]},{"__typename":"HeaderNavigationItem","name":"Grow","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-get-the-most-out-of-towards-data-science-3bf37f75a345","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]},{"__typename":"HeaderNavigationItem","name":"Contribute","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fquestions-96667b06af5","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]}]},"postBody":null,"blogroll":null},"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png":{"id":"1*sfUruIusLq6tbpLx0sDYZQ.png","__typename":"ImageMetadata","originalWidth":1401},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","__typename":"ImageMetadata","originalWidth":337,"originalHeight":122},"User:7e12c71dfa81":{"id":"7e12c71dfa81","__typename":"User","atsQualifiedAt":1612205680542},"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png":{"id":"1*eLxNtw6hQ4-3HrHda5BCCw.png","__typename":"ImageMetadata"},"NewsletterV3:d6fe9076899":{"id":"d6fe9076899","__typename":"NewsletterV3","slug":"the-variable","isSubscribed":false,"showPromo":true,"name":"The Variable","description":"Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.","type":"NEWSLETTER_TYPE_COLLECTION","user":{"__ref":"User:895063a310f4"},"collection":{"__ref":"Collection:7f60cf5620c9"}},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","__typename":"Collection","domain":"towardsdatascience.com","googleAnalyticsId":null,"slug":"towards-data-science","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","isAuroraVisible":true,"favicon":{"__ref":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png"},"name":"Towards Data Science","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"customStyleSheet":{"__ref":"CustomStyleSheet:63d23b36fcaa"},"tagline":"A Medium publication sharing concepts, ideas and codes.","isAuroraEligible":true,"viewerIsEditor":false,"logo":{"__ref":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"},"navItems":[{"__typename":"NavItem","title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Video","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fvideo\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"★","url":"https:\u002F\u002Ftowardsdatascience.com\u002Feditors-picks\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"About","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fabout-us\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM"}],"creator":{"__ref":"User:7e12c71dfa81"},"subscriberCount":581456,"avatar":{"__ref":"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png"},"newsletterV3":{"__ref":"NewsletterV3:d6fe9076899"},"viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"canToggleEmail":true,"isUserSubscribedToCollectionEmails":false,"viewerIsMuting":false,"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","status":"ACTIVE","isSubdomain":false}},"ptsQualifiedAt":1616092952992},"User:97c4870a6508":{"id":"97c4870a6508","__typename":"User","isFollowing":false,"viewerIsUser":false,"customStyleSheet":null,"isSuspended":false,"name":"James Dellinger","hasCompletedProfile":false,"bio":"","imageId":"1*VWdH331WvzjQpoZxCJoqFA.jpeg","username":"jamesdell","customDomainState":null,"isAuroraVisible":true,"createdAt":0,"mediumMemberAt":0,"lastPostCreatedAt":0,"socialStats":{"__typename":"SocialStats","followerCount":423,"followingCount":10},"hasSubdomain":false,"isAllowEdsEnabled":false,"isBlocking":false,"isMuting":false,"allowNotes":true,"newsletterV3":null,"twitterScreenName":"jamrdell","followedCollections":1,"atsQualifiedAt":1612205461410},"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png":{"id":"1*AGyTPCaRzVqL77kFwUwHKg.png","__typename":"ImageMetadata","originalWidth":1376,"originalHeight":429},"Tag:editors-pick":{"id":"editors-pick","__typename":"Tag","normalizedTagSlug":""},"Tag:tds-features":{"id":"tds-features","__typename":"Tag","normalizedTagSlug":""},"Tag:deep-dives":{"id":"deep-dives","__typename":"Tag","normalizedTagSlug":""},"Topic:1eca0103fff3":{"id":"1eca0103fff3","__typename":"Topic","name":"Machine Learning","slug":"machine-learning","isFollowing":null},"Paragraph:74b5e907e58e_0":{"id":"74b5e907e58e_0","__typename":"Paragraph","name":"18f9","text":"Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_1":{"id":"74b5e907e58e_1","__typename":"Paragraph","name":"4960","text":"I’d like to invite you to join me on an exploration through different approaches to initializing layer weights in neural networks. Step-by-step, through various short experiments and thought exercises, we’ll discover why adequate weight initialization is so important in training deep neural nets. Along the way we’ll cover various approaches that researchers have proposed over the years, and finally drill down on what works best for the contemporary network architectures that you’re most likely to be working with.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_2":{"id":"74b5e907e58e_2","__typename":"Paragraph","name":"5721","text":"The examples to follow come from my own re-implementation of a set of notebooks that Jeremy Howard covered in the latest version of fast.ai’s Deep Learning Part II course, currently being held this spring, 2019, at USF’s Data Institute.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":40,"end":57,"type":"A","href":"https:\u002F\u002Fnbviewer.jupyter.org\u002Fgithub\u002Fjamesdellinger\u002Ffastai_deep_learning_course_part2_v3\u002Fblob\u002Fmaster\u002F02_fully_connected_my_reimplementation.ipynb?flush_cache=true","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":85,"end":98,"type":"A","href":"https:\u002F\u002Fwww.usfca.edu\u002Ffaculty\u002Fjeremy-howard","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":114,"end":128,"type":"A","href":"https:\u002F\u002Fwww.fast.ai\u002F2019\u002F03\u002F06\u002Ffastai-swift\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":182,"end":235,"type":"A","href":"https:\u002F\u002Fwww.usfca.edu\u002Fdata-institute\u002Fcertificates\u002Fdeep-learning-part-two","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_3":{"id":"74b5e907e58e_3","__typename":"Paragraph","name":"a36f","text":"Why Initialize Weights","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_4":{"id":"74b5e907e58e_4","__typename":"Paragraph","name":"f4a5","text":"The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_5":{"id":"74b5e907e58e_5","__typename":"Paragraph","name":"d79e","text":"Matrix multiplication is the essential math operation of a neural network. In deep neural nets with several layers, one forward pass simply entails performing consecutive matrix multiplications at each layer, between that layer’s inputs and weight matrix. The product of this multiplication at one layer becomes the inputs of the subsequent layer, and so on and so forth.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_6":{"id":"74b5e907e58e_6","__typename":"Paragraph","name":"a600","text":"For a quick-and-dirty example that illustrates this, let’s pretend that we have a vector x that contains some network inputs. It’s standard practice when training neural networks to ensure that our inputs’ values are scaled such that they fall inside such a normal distribution with a mean of 0 and a standard deviation of 1.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":89,"end":90,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_7":{"id":"74b5e907e58e_7","__typename":"Paragraph","name":"b0c9","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ne6gE3vogzJp53lwAYxtGw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_8":{"id":"74b5e907e58e_8","__typename":"Paragraph","name":"f011","text":"Let’s also pretend that we have a simple 100-layer network with no activations , and that each layer has a matrix a that contains the layer’s weights. In order to complete a single forward pass we’ll have to perform a matrix multiplication between layer inputs and weights at each of the hundred layers, which will make for a grand total of 100 consecutive matrix multiplications.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":114,"end":115,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":341,"end":344,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_9":{"id":"74b5e907e58e_9","__typename":"Paragraph","name":"efc6","text":"It turns out that initializing the values of layer weights from the same standard normal distribution to which we scaled our inputs is never a good idea. To see why, we can simulate a forward pass through our hypothetical network.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_10":{"id":"74b5e907e58e_10","__typename":"Paragraph","name":"e118","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*IK15xc15E1LJGlDP1FEXFA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_11":{"id":"74b5e907e58e_11","__typename":"Paragraph","name":"6f3f","text":"Whoa! Somewhere during those 100 multiplications, the layer outputs got so big that even the computer wasn’t able to recognize their standard deviation and mean as numbers. We can actually see exactly how long that took to happen.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_12":{"id":"74b5e907e58e_12","__typename":"Paragraph","name":"5afa","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*_yr6lnXY-1aXVUWb1MJIYA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_13":{"id":"74b5e907e58e_13","__typename":"Paragraph","name":"0ba2","text":"The activation outputs exploded within 29 of our network’s layers. We clearly initialized our weights to be too large.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_14":{"id":"74b5e907e58e_14","__typename":"Paragraph","name":"ab92","text":"Unfortunately, we also have to worry about preventing layer outputs from vanishing. To see what happens when we initialize network weights to be too small — we’ll scale our weight values such that, while they still fall inside a normal distribution with a mean of 0, they have a standard deviation of 0.01.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_15":{"id":"74b5e907e58e_15","__typename":"Paragraph","name":"4163","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*z89-cC4jCGPOkp3cnbyr8A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_16":{"id":"74b5e907e58e_16","__typename":"Paragraph","name":"6271","text":"During the course of the above hypothetical forward pass, the activation outputs completely vanished.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_17":{"id":"74b5e907e58e_17","__typename":"Paragraph","name":"08c0","text":"To sum it up, if weights are initialized too large, the network won’t learn well. The same happens when weights are initialized too small.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_18":{"id":"74b5e907e58e_18","__typename":"Paragraph","name":"da92","text":"How can we find the sweet spot?","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_19":{"id":"74b5e907e58e_19","__typename":"Paragraph","name":"7698","text":"Remember that as mentioned above, the math required to complete a forward pass through a neural network entails nothing more than a succession of matrix multiplications. If we have an output y that is the product of a matrix multiplication between our input vector x and weight matrix a, each element i in y is defined as","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":191,"end":192,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":265,"end":266,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":285,"end":286,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":306,"end":307,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":301,"end":302,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_20":{"id":"74b5e907e58e_20","__typename":"Paragraph","name":"2429","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*K3AIFVUelCr0z646zrQjPw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_21":{"id":"74b5e907e58e_21","__typename":"Paragraph","name":"15e5","text":"where i is a given row-index of weight matrix a, k is both a given column-index in weight matrix a and element-index in input vector x, and n is the range or total number of elements in x. This can also be defined in Python as:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":46,"end":47,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":97,"end":98,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":133,"end":134,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":186,"end":187,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":6,"end":7,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":49,"end":50,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":140,"end":141,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_22":{"id":"74b5e907e58e_22","__typename":"Paragraph","name":"316e","text":"y[i] = sum([c*d for c,d in zip(a[i], x)])","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_23":{"id":"74b5e907e58e_23","__typename":"Paragraph","name":"d276","text":"We can demonstrate that at a given layer, the matrix product of our inputs x and weight matrix a that we initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections, which in our example is √512.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":75,"end":76,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":95,"end":96,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_24":{"id":"74b5e907e58e_24","__typename":"Paragraph","name":"f5d3","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*oJDRa2HOPhe5JV7co5Ig-A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_25":{"id":"74b5e907e58e_25","__typename":"Paragraph","name":"6b2f","text":"This property isn’t surprising if we view it in terms of how matrix multiplication is defined: in order to calculate y we sum 512 products of the element-wise multiplication of one element of the inputs x by one column of the weights a. In our example where both x and a are initialized using standard normal distributions, each of these 512 products would have a mean of 0 and standard deviation of 1.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":117,"end":118,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":203,"end":204,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":234,"end":235,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":263,"end":264,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":269,"end":270,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_26":{"id":"74b5e907e58e_26","__typename":"Paragraph","name":"acb3","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*wMTdWrSPOXh8C6XxoSO7pg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_27":{"id":"74b5e907e58e_27","__typename":"Paragraph","name":"0a28","text":"It then follows that the sum of these 512 products would have a mean of 0, variance of 512, and therefore a standard deviation of √512.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":25,"end":28,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_28":{"id":"74b5e907e58e_28","__typename":"Paragraph","name":"b501","text":"And this is why in our example above we saw our layer outputs exploding after 29 consecutive matrix multiplications. In the case of our bare-bones 100-layer network architecture, what we’d like is for each layer’s outputs to have a standard deviation of about 1. This conceivably would allow us to repeat matrix multiplications across as many network layers as we want, without activations exploding or vanishing.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_29":{"id":"74b5e907e58e_29","__typename":"Paragraph","name":"95b6","text":"If we first scale the weight matrix a by dividing all its randomly chosen values by √512, the element-wise multiplication that fills in one element of the outputs y would now, on average, have a variance of only 1\u002F√512.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":36,"end":37,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":163,"end":164,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_30":{"id":"74b5e907e58e_30","__typename":"Paragraph","name":"8e87","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*qDpw22Z8XSDZe5MLLOLUew.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_31":{"id":"74b5e907e58e_31","__typename":"Paragraph","name":"ac76","text":"This means that the standard deviation of the matrix y, which contains each of the 512 values that are generated by way of the matrix multiplication between inputs x and weights a, would be 1. Let’s confirm this experimentally.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":53,"end":54,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":164,"end":165,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":178,"end":179,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_32":{"id":"74b5e907e58e_32","__typename":"Paragraph","name":"f3ed","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*5LAW2b1nDhPB9k7gt68yZg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_33":{"id":"74b5e907e58e_33","__typename":"Paragraph","name":"a5c0","text":"Now let’s re-run our quick-and-dirty 100-layer network. As before, we first choose layer weights at random from standard normal distribution inside [-1,1], but this time we scale those weights by 1\u002F√n, where n is the number of network input connections at a layer, which is 512 in our example.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":199,"end":200,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":208,"end":209,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_34":{"id":"74b5e907e58e_34","__typename":"Paragraph","name":"2d92","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*8-f9bmfeVVWwLLqSoQADSA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_35":{"id":"74b5e907e58e_35","__typename":"Paragraph","name":"b143","text":"Success! Our layer outputs neither exploded nor vanished, even after 100 of our hypothetical layers.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_36":{"id":"74b5e907e58e_36","__typename":"Paragraph","name":"57e4","text":"While at first glance it may seem like at this point we can call it a day, real-world neural networks aren’t quite as simple as our first example may seem to indicate. For the sake of simplicity, activation functions were omitted. However, we’d never do this in real life. It’s thanks to the placement of these non-linear activation functions at the tail end of network layers, that deep neural nets are able create close approximations of intricate functions that describe real-world phenomena, which can then be used to generate astoundingly impressive predictions, such as the classification of handwriting samples.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_37":{"id":"74b5e907e58e_37","__typename":"Paragraph","name":"169e","text":"Xavier Initialization","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_38":{"id":"74b5e907e58e_38","__typename":"Paragraph","name":"3231","text":"Up until a few years ago, most commonly used activation functions were symmetric about a given value, and had ranges that asymptotically approached values that were plus\u002Fminus a certain distance from this midpoint. The hyperbolic tangent and softsign functions exemplify this class of activations.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_39":{"id":"74b5e907e58e_39","__typename":"Paragraph","name":"839f","text":"Tanh and softsign activation functions. Credit: Sefik Ilkin Serengil’s blog.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*39Dm-zzV98YO-WKCfXgeVg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":71,"end":76,"type":"A","href":"https:\u002F\u002Fsefiks.com\u002F2017\u002F11\u002F10\u002Fsoftsign-as-a-neural-networks-activation-function\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_40":{"id":"74b5e907e58e_40","__typename":"Paragraph","name":"5a52","text":"We’ll add a hyperbolic tangent activation function after each layer our hypothetical 100-layer network, and then see what happens when we use our home-grown weight initialization scheme where layer weights are scaled by 1\u002F√n.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":223,"end":225,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_41":{"id":"74b5e907e58e_41","__typename":"Paragraph","name":"b29e","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*5Ko4_sp9-58wK_hFL2GCHQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_42":{"id":"74b5e907e58e_42","__typename":"Paragraph","name":"d3a0","text":"The standard deviation of activation outputs of the 100th layer is down to about 0.06. This is definitely on the small side, but at least activations haven’t totally vanished!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_43":{"id":"74b5e907e58e_43","__typename":"Paragraph","name":"fafa","text":"As intuitive as the journey to discovering our home-grown weight init strategy may now seem in retrospect, you may be surprised to hear that as recently as 2010, this was not the conventional approach for initializing weight layers.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_44":{"id":"74b5e907e58e_44","__typename":"Paragraph","name":"04d8","text":"When Xavier Glorot and Yoshua Bengio published their landmark paper titled Understanding the difficulty of training deep feedforward neural networks, the “commonly used heuristic” to which they compared their experiments was that of initializing weights from a uniform distribution in [-1,1] and then scaling by 1\u002F√n.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":75,"end":148,"type":"A","href":"http:\u002F\u002Fproceedings.mlr.press\u002Fv9\u002Fglorot10a\u002Fglorot10a.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":75,"end":148,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":261,"end":268,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":315,"end":316,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_45":{"id":"74b5e907e58e_45","__typename":"Paragraph","name":"7aad","text":"It turns out this “standard” approach doesn’t actually work that well.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_46":{"id":"74b5e907e58e_46","__typename":"Paragraph","name":"6a48","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*RJ2AxAObnSszBlJ6_LZKrQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_47":{"id":"74b5e907e58e_47","__typename":"Paragraph","name":"33ac","text":"Re-running our 100-layer tanh network with “standard” weight initialization caused activation gradients to become infinitesimally small — they’re just about as good as vanished.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_48":{"id":"74b5e907e58e_48","__typename":"Paragraph","name":"43db","text":"This poor performance is actually what spurred Glorot and Bengio to propose their own weight initialization strategy, which they referred to as “normalized initialization” in their paper, and which is now popularly termed “Xavier initialization.”","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_49":{"id":"74b5e907e58e_49","__typename":"Paragraph","name":"54ed","text":"Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_50":{"id":"74b5e907e58e_50","__typename":"Paragraph","name":"3192","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*H6t3yYBLlinNRUwmL-d7vw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_51":{"id":"74b5e907e58e_51","__typename":"Paragraph","name":"9c07","text":"where nᵢ is the number of incoming network connections, or “fan-in,” to the layer, and nᵢ₊₁ is the number of outgoing network connections from that layer, also known as the “fan-out.”","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":6,"end":8,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":87,"end":91,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_52":{"id":"74b5e907e58e_52","__typename":"Paragraph","name":"9fb4","text":"Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network. In their experiments they observed that Xavier initialization enabled a 5-layer network to maintain near identical variances of its weight gradients across layers.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_53":{"id":"74b5e907e58e_53","__typename":"Paragraph","name":"3291","text":"With Xavier init. Credit: Glorot & Bengio.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Gelf2ZcKYowsLf5FT4n0BQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":26,"end":41,"type":"A","href":"http:\u002F\u002Fproceedings.mlr.press\u002Fv9\u002Fglorot10a\u002Fglorot10a.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_54":{"id":"74b5e907e58e_54","__typename":"Paragraph","name":"b727","text":"Conversely, it turned out that using “standard” initialization brought about a much bigger gap in variance between weight gradients at the network’s lower layers, which were higher, and those at its top-most layers, which were approaching zero.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_55":{"id":"74b5e907e58e_55","__typename":"Paragraph","name":"6516","text":"Without Xavier init. Credit: Glorot & Bengio.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*mDilNz4ADDbbr8Qb4d4RqQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":29,"end":44,"type":"A","href":"http:\u002F\u002Fproceedings.mlr.press\u002Fv9\u002Fglorot10a\u002Fglorot10a.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_56":{"id":"74b5e907e58e_56","__typename":"Paragraph","name":"fc04","text":"To drive the point home, Glorot and Bengio demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy on the CIFAR-10 image classification task.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":164,"end":198,"type":"A","href":"https:\u002F\u002Fwww.cs.toronto.edu\u002F~kriz\u002Fcifar.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_57":{"id":"74b5e907e58e_57","__typename":"Paragraph","name":"4c2a","text":"Let’s re-run our 100-layer tanh network once more, this time using Xavier initialization:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_58":{"id":"74b5e907e58e_58","__typename":"Paragraph","name":"92b1","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*3NiBW8yi-gYrOpsy70PNEg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_59":{"id":"74b5e907e58e_59","__typename":"Paragraph","name":"92d5","text":"In our experimental network, Xavier initialization performs pretty identical to the home-grown method that we derived earlier, where we sampled values from a random normal distribution and scaled by the square root of number of incoming network connections, n.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":258,"end":259,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_60":{"id":"74b5e907e58e_60","__typename":"Paragraph","name":"9dba","text":"Kaiming Initialization","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_61":{"id":"74b5e907e58e_61","__typename":"Paragraph","name":"e8d9","text":"Conceptually, it makes sense that when using activation functions that are symmetric about zero and have outputs inside [-1,1], such as softsign and tanh, we’d want the activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. This is precisely what our home-grown method and Xavier both enable.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_62":{"id":"74b5e907e58e_62","__typename":"Paragraph","name":"863f","text":"But what if we’re using ReLU activation functions? Would it still make sense to want to scale random initial weight values in the same way?","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_63":{"id":"74b5e907e58e_63","__typename":"Paragraph","name":"fd85","text":"ReLU activation function. Credit: Kanchan Sarkar’s blog.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*njuH4XVXf-l9pR_RorUOrA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":51,"end":55,"type":"A","href":"https:\u002F\u002Fmedium.com\u002F@kanchansarkar\u002Frelu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_64":{"id":"74b5e907e58e_64","__typename":"Paragraph","name":"6233","text":"To see what would happen, let’s use a ReLU activation instead of tanh in one of our hypothetical network’s layers and observe the expected standard deviation of its outputs.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_65":{"id":"74b5e907e58e_65","__typename":"Paragraph","name":"e89e","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*0C5Beclgsv-_HEaOfV8eJA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_66":{"id":"74b5e907e58e_66","__typename":"Paragraph","name":"1765","text":"It turns out that when using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, divided by the square root of two, or √512\u002F√2 in our example.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":177,"end":210,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_67":{"id":"74b5e907e58e_67","__typename":"Paragraph","name":"4a42","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*dZ5UcHx2RTeLXL46YxKgbw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_68":{"id":"74b5e907e58e_68","__typename":"Paragraph","name":"edc4","text":"Scaling the values of the weight matrix a by this number will cause each individual ReLU layer to have a standard deviation of 1 on average.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":40,"end":41,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_69":{"id":"74b5e907e58e_69","__typename":"Paragraph","name":"e1d6","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*1G5dke8TAGumS88OKa3TfA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_70":{"id":"74b5e907e58e_70","__typename":"Paragraph","name":"f1cf","text":"As we showed before, keeping the standard deviation of layers’ activations around 1 will allow us to stack several more layers in a deep neural network without gradients exploding or vanishing.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_71":{"id":"74b5e907e58e_71","__typename":"Paragraph","name":"3f29","text":"This exploration into how to best initialize weights in networks with ReLU-like activations is what motivated Kaiming He et. al. to propose their own initialization scheme that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":132,"end":171,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1502.01852.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_72":{"id":"74b5e907e58e_72","__typename":"Paragraph","name":"f0db","text":"In their 2015 paper, He et. al. demonstrated that deep networks (e.g. a 22-layer CNN) would converge much earlier if the following input weight initialization strategy is employed:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_73":{"id":"74b5e907e58e_73","__typename":"Paragraph","name":"1f94","text":"Create a tensor with the dimensions appropriate for a weight matrix at a given layer, and populate it with numbers randomly chosen from a standard normal distribution.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_74":{"id":"74b5e907e58e_74","__typename":"Paragraph","name":"e8ea","text":"Multiply each randomly chosen number by √2\u002F√n where n is the number of incoming connections coming into a given layer from the previous layer’s output (also known as the “fan-in”).","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":40,"end":41,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":43,"end":45,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":52,"end":53,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_75":{"id":"74b5e907e58e_75","__typename":"Paragraph","name":"0e8d","text":"Bias tensors are initialized to zero.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_76":{"id":"74b5e907e58e_76","__typename":"Paragraph","name":"bc86","text":"We can follow these directions to implement our own version of Kaiming initialization and verify that it can indeed prevent activation outputs from exploding or vanishing if ReLU is used at all layers of our hypothetical 100-layer network.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_77":{"id":"74b5e907e58e_77","__typename":"Paragraph","name":"78a5","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*RD_c4ayThCkRvODAIMfOEQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_78":{"id":"74b5e907e58e_78","__typename":"Paragraph","name":"dbc9","text":"As a final comparison, here’s what would happen if we were to use Xavier initialization, instead.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_79":{"id":"74b5e907e58e_79","__typename":"Paragraph","name":"9770","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*_1RKMYJbwsIa4SYAj8ol-A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_80":{"id":"74b5e907e58e_80","__typename":"Paragraph","name":"ff7b","text":"Ouch! When using Xavier to initialize weights, activation outputs have almost completely vanished by the 100th layer!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_81":{"id":"74b5e907e58e_81","__typename":"Paragraph","name":"944b","text":"Incidentally, when they trained even deeper networks that used ReLUs, He et. al. found that a 30-layer CNN using Xavier initialization stalled completely and didn’t learn at all. However, when the same network was initialized according to the three-step procedure outlined above, it enjoyed substantially greater convergence.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_82":{"id":"74b5e907e58e_82","__typename":"Paragraph","name":"b853","text":"Convergence of a 30-layer CNN thanks to Kaiming init. Credit: He et. al.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*AcZIzXFAJm_ZafRKleF_0g.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":62,"end":72,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1502.01852.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":17,"end":25,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_83":{"id":"74b5e907e58e_83","__typename":"Paragraph","name":"049e","text":"The moral of the story for us is that any network we train from scratch, especially for computer vision applications, will almost certainly contain ReLU activation functions and be several layers deep. In such cases, Kaiming should be our go-to weight init strategy.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_84":{"id":"74b5e907e58e_84","__typename":"Paragraph","name":"e55f","text":"Yes, You Too Can Be a Researcher","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_85":{"id":"74b5e907e58e_85","__typename":"Paragraph","name":"343f","text":"Even more importantly, I’m not ashamed to admit that I felt intimidated when I saw the Xavier and Kaiming formulas for the first time. What with their respective square roots of six and two, part of me couldn’t help but feel like they must have been the result of some sort of oracular wisdom I couldn’t hope to fathom on my own. And let’s face it, sometimes the math in deep learning papers can look a lot like hieroglyphics, except with no Rosetta Stone to aid in translation.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":401,"end":425,"type":"A","href":"https:\u002F\u002Ftwitter.com\u002Fseluappendix","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":442,"end":455,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FRosetta_Stone","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:74b5e907e58e_86":{"id":"74b5e907e58e_86","__typename":"Paragraph","name":"4889","text":"But I think the journey we took here showed us that this knee-jerk response of feeling of intimidated, while wholly understandable, is by no means unavoidable. Although the Kaiming and (especially) the Xavier papers do contain their fair share of math, we saw firsthand how experiments, empirical observation, and some straightforward common sense were enough to help derive the core set of principals underpinning what is currently the most widely-used weight initialization scheme.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:74b5e907e58e_87":{"id":"74b5e907e58e_87","__typename":"Paragraph","name":"c773","text":"Alternately put: when in doubt, be courageous, try things out, and see what happens!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"ImageMetadata:1*ne6gE3vogzJp53lwAYxtGw.png":{"id":"1*ne6gE3vogzJp53lwAYxtGw.png","__typename":"ImageMetadata","originalHeight":90,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*IK15xc15E1LJGlDP1FEXFA.png":{"id":"1*IK15xc15E1LJGlDP1FEXFA.png","__typename":"ImageMetadata","originalHeight":270,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*_yr6lnXY-1aXVUWb1MJIYA.png":{"id":"1*_yr6lnXY-1aXVUWb1MJIYA.png","__typename":"ImageMetadata","originalHeight":398,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*z89-cC4jCGPOkp3cnbyr8A.png":{"id":"1*z89-cC4jCGPOkp3cnbyr8A.png","__typename":"ImageMetadata","originalHeight":360,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*K3AIFVUelCr0z646zrQjPw.png":{"id":"1*K3AIFVUelCr0z646zrQjPw.png","__typename":"ImageMetadata","originalHeight":212,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*oJDRa2HOPhe5JV7co5Ig-A.png":{"id":"1*oJDRa2HOPhe5JV7co5Ig-A.png","__typename":"ImageMetadata","originalHeight":616,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*wMTdWrSPOXh8C6XxoSO7pg.png":{"id":"1*wMTdWrSPOXh8C6XxoSO7pg.png","__typename":"ImageMetadata","originalHeight":444,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*qDpw22Z8XSDZe5MLLOLUew.png":{"id":"1*qDpw22Z8XSDZe5MLLOLUew.png","__typename":"ImageMetadata","originalHeight":618,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*5LAW2b1nDhPB9k7gt68yZg.png":{"id":"1*5LAW2b1nDhPB9k7gt68yZg.png","__typename":"ImageMetadata","originalHeight":444,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*8-f9bmfeVVWwLLqSoQADSA.png":{"id":"1*8-f9bmfeVVWwLLqSoQADSA.png","__typename":"ImageMetadata","originalHeight":360,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*39Dm-zzV98YO-WKCfXgeVg.png":{"id":"1*39Dm-zzV98YO-WKCfXgeVg.png","__typename":"ImageMetadata","originalHeight":264,"originalWidth":371,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*5Ko4_sp9-58wK_hFL2GCHQ.png":{"id":"1*5Ko4_sp9-58wK_hFL2GCHQ.png","__typename":"ImageMetadata","originalHeight":464,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*RJ2AxAObnSszBlJ6_LZKrQ.png":{"id":"1*RJ2AxAObnSszBlJ6_LZKrQ.png","__typename":"ImageMetadata","originalHeight":330,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*H6t3yYBLlinNRUwmL-d7vw.png":{"id":"1*H6t3yYBLlinNRUwmL-d7vw.png","__typename":"ImageMetadata","originalHeight":226,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Gelf2ZcKYowsLf5FT4n0BQ.png":{"id":"1*Gelf2ZcKYowsLf5FT4n0BQ.png","__typename":"ImageMetadata","originalHeight":474,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*mDilNz4ADDbbr8Qb4d4RqQ.png":{"id":"1*mDilNz4ADDbbr8Qb4d4RqQ.png","__typename":"ImageMetadata","originalHeight":480,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*3NiBW8yi-gYrOpsy70PNEg.png":{"id":"1*3NiBW8yi-gYrOpsy70PNEg.png","__typename":"ImageMetadata","originalHeight":450,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*njuH4XVXf-l9pR_RorUOrA.png":{"id":"1*njuH4XVXf-l9pR_RorUOrA.png","__typename":"ImageMetadata","originalHeight":278,"originalWidth":357,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*0C5Beclgsv-_HEaOfV8eJA.png":{"id":"1*0C5Beclgsv-_HEaOfV8eJA.png","__typename":"ImageMetadata","originalHeight":552,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*dZ5UcHx2RTeLXL46YxKgbw.png":{"id":"1*dZ5UcHx2RTeLXL46YxKgbw.png","__typename":"ImageMetadata","originalHeight":146,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*1G5dke8TAGumS88OKa3TfA.png":{"id":"1*1G5dke8TAGumS88OKa3TfA.png","__typename":"ImageMetadata","originalHeight":446,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*RD_c4ayThCkRvODAIMfOEQ.png":{"id":"1*RD_c4ayThCkRvODAIMfOEQ.png","__typename":"ImageMetadata","originalHeight":506,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*_1RKMYJbwsIa4SYAj8ol-A.png":{"id":"1*_1RKMYJbwsIa4SYAj8ol-A.png","__typename":"ImageMetadata","originalHeight":366,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*AcZIzXFAJm_ZafRKleF_0g.png":{"id":"1*AcZIzXFAJm_ZafRKleF_0g.png","__typename":"ImageMetadata","originalHeight":810,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null},"User:895063a310f4":{"id":"895063a310f4","__typename":"User","name":"Ludovic Benistant"},"Tag:deep-learning":{"id":"deep-learning","__typename":"Tag","displayTitle":"Deep Learning","normalizedTagSlug":""},"Tag:neural-networks":{"id":"neural-networks","__typename":"Tag","displayTitle":"Neural Networks","normalizedTagSlug":""},"Tag:weight-initialization":{"id":"weight-initialization","__typename":"Tag","displayTitle":"Weight Initialization","normalizedTagSlug":""},"Tag:towards-data-science":{"id":"towards-data-science","__typename":"Tag","displayTitle":"Towards Data Science","normalizedTagSlug":""},"Post:954fb9b47c79":{"id":"954fb9b47c79","__typename":"Post","canonicalUrl":"","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{\"referrer\":\"\",\"sk\":null,\"source\":null}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","isCacheableContent":false,"bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:74b5e907e58e_0"},{"__ref":"Paragraph:74b5e907e58e_1"},{"__ref":"Paragraph:74b5e907e58e_2"},{"__ref":"Paragraph:74b5e907e58e_3"},{"__ref":"Paragraph:74b5e907e58e_4"},{"__ref":"Paragraph:74b5e907e58e_5"},{"__ref":"Paragraph:74b5e907e58e_6"},{"__ref":"Paragraph:74b5e907e58e_7"},{"__ref":"Paragraph:74b5e907e58e_8"},{"__ref":"Paragraph:74b5e907e58e_9"},{"__ref":"Paragraph:74b5e907e58e_10"},{"__ref":"Paragraph:74b5e907e58e_11"},{"__ref":"Paragraph:74b5e907e58e_12"},{"__ref":"Paragraph:74b5e907e58e_13"},{"__ref":"Paragraph:74b5e907e58e_14"},{"__ref":"Paragraph:74b5e907e58e_15"},{"__ref":"Paragraph:74b5e907e58e_16"},{"__ref":"Paragraph:74b5e907e58e_17"},{"__ref":"Paragraph:74b5e907e58e_18"},{"__ref":"Paragraph:74b5e907e58e_19"},{"__ref":"Paragraph:74b5e907e58e_20"},{"__ref":"Paragraph:74b5e907e58e_21"},{"__ref":"Paragraph:74b5e907e58e_22"},{"__ref":"Paragraph:74b5e907e58e_23"},{"__ref":"Paragraph:74b5e907e58e_24"},{"__ref":"Paragraph:74b5e907e58e_25"},{"__ref":"Paragraph:74b5e907e58e_26"},{"__ref":"Paragraph:74b5e907e58e_27"},{"__ref":"Paragraph:74b5e907e58e_28"},{"__ref":"Paragraph:74b5e907e58e_29"},{"__ref":"Paragraph:74b5e907e58e_30"},{"__ref":"Paragraph:74b5e907e58e_31"},{"__ref":"Paragraph:74b5e907e58e_32"},{"__ref":"Paragraph:74b5e907e58e_33"},{"__ref":"Paragraph:74b5e907e58e_34"},{"__ref":"Paragraph:74b5e907e58e_35"},{"__ref":"Paragraph:74b5e907e58e_36"},{"__ref":"Paragraph:74b5e907e58e_37"},{"__ref":"Paragraph:74b5e907e58e_38"},{"__ref":"Paragraph:74b5e907e58e_39"},{"__ref":"Paragraph:74b5e907e58e_40"},{"__ref":"Paragraph:74b5e907e58e_41"},{"__ref":"Paragraph:74b5e907e58e_42"},{"__ref":"Paragraph:74b5e907e58e_43"},{"__ref":"Paragraph:74b5e907e58e_44"},{"__ref":"Paragraph:74b5e907e58e_45"},{"__ref":"Paragraph:74b5e907e58e_46"},{"__ref":"Paragraph:74b5e907e58e_47"},{"__ref":"Paragraph:74b5e907e58e_48"},{"__ref":"Paragraph:74b5e907e58e_49"},{"__ref":"Paragraph:74b5e907e58e_50"},{"__ref":"Paragraph:74b5e907e58e_51"},{"__ref":"Paragraph:74b5e907e58e_52"},{"__ref":"Paragraph:74b5e907e58e_53"},{"__ref":"Paragraph:74b5e907e58e_54"},{"__ref":"Paragraph:74b5e907e58e_55"},{"__ref":"Paragraph:74b5e907e58e_56"},{"__ref":"Paragraph:74b5e907e58e_57"},{"__ref":"Paragraph:74b5e907e58e_58"},{"__ref":"Paragraph:74b5e907e58e_59"},{"__ref":"Paragraph:74b5e907e58e_60"},{"__ref":"Paragraph:74b5e907e58e_61"},{"__ref":"Paragraph:74b5e907e58e_62"},{"__ref":"Paragraph:74b5e907e58e_63"},{"__ref":"Paragraph:74b5e907e58e_64"},{"__ref":"Paragraph:74b5e907e58e_65"},{"__ref":"Paragraph:74b5e907e58e_66"},{"__ref":"Paragraph:74b5e907e58e_67"},{"__ref":"Paragraph:74b5e907e58e_68"},{"__ref":"Paragraph:74b5e907e58e_69"},{"__ref":"Paragraph:74b5e907e58e_70"},{"__ref":"Paragraph:74b5e907e58e_71"},{"__ref":"Paragraph:74b5e907e58e_72"},{"__ref":"Paragraph:74b5e907e58e_73"},{"__ref":"Paragraph:74b5e907e58e_74"},{"__ref":"Paragraph:74b5e907e58e_75"},{"__ref":"Paragraph:74b5e907e58e_76"},{"__ref":"Paragraph:74b5e907e58e_77"},{"__ref":"Paragraph:74b5e907e58e_78"},{"__ref":"Paragraph:74b5e907e58e_79"},{"__ref":"Paragraph:74b5e907e58e_80"},{"__ref":"Paragraph:74b5e907e58e_81"},{"__ref":"Paragraph:74b5e907e58e_82"},{"__ref":"Paragraph:74b5e907e58e_83"},{"__ref":"Paragraph:74b5e907e58e_84"},{"__ref":"Paragraph:74b5e907e58e_85"},{"__ref":"Paragraph:74b5e907e58e_86"},{"__ref":"Paragraph:74b5e907e58e_87"}],"sections":[{"__typename":"Section","name":"4e73","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"creator":{"__ref":"User:97c4870a6508"},"customStyleSheet":{"__ref":"CustomStyleSheet:63d23b36fcaa"},"firstPublishedAt":1554281025838,"isLocked":false,"isPublished":true,"isShortform":false,"layerCake":3,"primaryTopic":{"__ref":"Topic:1eca0103fff3"},"title":"Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming","isMarkedPaywallOnly":false,"readCreatorPostsCount":0,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fweight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79","isLimitedState":false,"visibility":"PUBLIC","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"newsletterId":"","sequence":null,"tags":[{"__ref":"Tag:deep-learning"},{"__ref":"Tag:neural-networks"},{"__ref":"Tag:weight-initialization"},{"__ref":"Tag:towards-data-science"}],"topics":[{"__typename":"Topic","topicId":"1eca0103fff3","name":"Machine Learning"}],"viewerClapCount":0,"showSubscribeToProfilePromo":false,"showSubscribeToCollectionNewsletterV3Promo":true,"inResponseToPostResult":null,"isNewsletter":false,"socialTitle":"","socialDek":"","noIndex":null,"curationStatus":null,"metaDescription":"","latestPublishedAt":1554397717064,"readingTime":10.814150943396225,"previewContent":{"__typename":"PreviewContent","subtitle":"Exploring the evolution of initializing layer weights in neural networks: from old-school to Xavier, and arriving finally at Kaiming init."},"previewImage":{"__ref":"ImageMetadata:1*AcZIzXFAJm_ZafRKleF_0g.png"},"creatorPartnerProgramEnrollmentStatus":"PERMISSION_DENIED","clapCount":3671,"lockedSource":"LOCKED_POST_SOURCE_NONE","isSuspended":false,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":0,"responseDistribution":"NOT_DISTRIBUTED","shareKey":null,"collaborators":[],"translationSourcePost":null,"inResponseToMediaResource":null,"audioVersionUrl":"","seoTitle":"","updatedAt":1554397717166,"shortformType":"SHORTFORM_TYPE_LINK","structuredData":"","seoDescription":"","postResponses":{"__typename":"PostResponses","count":23},"latestPublishedVersion":"74b5e907e58e","isPublishToEmail":false,"readingList":"READING_LIST_NONE","voterCount":931,"recommenders":[]}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.ae72f892.js"></script><script src="https://cdn-client.medium.com/lite/static/js/8464.d0ef046c.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.2b220e26.js"></script><script src="https://cdn-client.medium.com/lite/static/js/5573.159bf40f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/instrumentation.8ea15b6a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.be183697.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1752.a348f767.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2833.383a48e6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8342.6aa0b45e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4930.d16bc692.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9692.aa4b76ec.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4586.06957e16.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5064.e0fb94df.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9046.c85ec5d4.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2846.78dd20ea.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9990.4f008794.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7012.1bb4e9a4.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9972.9b66804a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4379.7456fbc7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5127.59840da2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8751.eca11143.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2955.4ef7996b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7131.f2494ca5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6163.8b10fb5b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8127.1c99358b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2514.eb9a5ab7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6371.e8059cd7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7496.e26cbb9b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1725.155e62d8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3874.32302d9b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8953.20430471.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8286.cb05fae9.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9454.fe0523e9.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/Post.aa5fea13.chunk.js"></script><script>window.main();</script></body></html>