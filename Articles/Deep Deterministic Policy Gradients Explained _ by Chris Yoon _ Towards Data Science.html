<!doctype html><html lang="en"><head><script defer src="https://cdn.optimizely.com/js/16180790160.js"></script><title data-rh="true">Deep Deterministic Policy Gradients Explained | by Chris Yoon | Towards Data Science</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2019-05-23T17:10:07.156Z"/><meta data-rh="true" name="title" content="Deep Deterministic Policy Gradients Explained | by Chris Yoon | Towards Data Science"/><meta data-rh="true" property="og:title" content="Deep Deterministic Policy Gradients Explained"/><meta data-rh="true" property="twitter:title" content="Deep Deterministic Policy Gradients Explained"/><meta data-rh="true" name="twitter:site" content="@TDataScience"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/2d94655a9b7b"/><meta data-rh="true" property="al:android:url" content="medium://p/2d94655a9b7b"/><meta data-rh="true" property="al:ios:url" content="medium://p/2d94655a9b7b"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="This post is a thorough review of Deepmind’s publication “Continuous Control With Deep Reinforcement Learning” (Lillicrap et al, 2015), in which the Deep Deterministic Policy Gradients (DDPG) is…"/><meta data-rh="true" property="og:description" content="Reinforcement Learning in Continuous Action Spaces"/><meta data-rh="true" property="twitter:description" content="Reinforcement Learning in Continuous Action Spaces"/><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b"/><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*-87grz5iUZK4i7NCH1ldbw.png"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*-87grz5iUZK4i7NCH1ldbw.png"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" property="article:author" content="https://medium.com/@thechrisyoon"/><meta data-rh="true" name="author" content="Chris Yoon"/><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="6 min read"/><meta data-rh="true" name="parsely-post-id" content="2d94655a9b7b"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://medium.com/@thechrisyoon"/><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/2d94655a9b7b"/><link data-rh="true" rel="icon" href="https://miro.medium.com/fit/c/256/256/1*ChFMdf--f5jbm-AYv6VdYA@2x.png"/><link data-rh="true" rel="preload" href="https://miro.medium.com/max/2802/1*sfUruIusLq6tbpLx0sDYZQ.png" as="image"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*-87grz5iUZK4i7NCH1ldbw.png"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b","dateCreated":"2019-03-20T13:07:57.366Z","datePublished":"2019-03-20T13:07:57.366Z","dateModified":"2019-05-23T17:10:07.386Z","headline":"Deep Deterministic Policy Gradients Explained - Towards Data Science","name":"Deep Deterministic Policy Gradients Explained - Towards Data Science","description":"This post is a thorough review of Deepmind’s publication “Continuous Control With Deep Reinforcement Learning” (Lillicrap et al, 2015), in which the Deep Deterministic Policy Gradients (DDPG) is…","identifier":"2d94655a9b7b","keywords":["Lite:true","Tag:Reinforcement Learning","Tag:Machine Learning","Tag:Artificial Intelligence","Tag:Control","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:3"],"author":{"@type":"Person","name":"Chris Yoon","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@thechrisyoon"},"creator":["Chris Yoon"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F330\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b"}</script><link rel="preload" href="https://cdn.optimizely.com/js/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="578" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-moz-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-webkit-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-moz-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-webkit-keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{height:25px}.r{fill:rgba(41, 41, 41, 1)}.s{display:block}.t{margin-bottom:36px}.v{width:100%}.w{overflow-x:scroll}.x{white-space:nowrap}.y{scrollbar-width:none}.z{-ms-overflow-style:none}.ab::-webkit-scrollbar{display:none}.ac{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.ae{min-height:184px}.ah{flex-direction:column}.ai{background-color:#355876}.aj{display:none}.al{border-bottom:none}.am{position:relative}.an{z-index:500}.at{max-width:1192px}.au{min-width:0}.av{height:62px}.aw{flex-direction:row}.ax{flex:1 0 auto}.ay{margin-left:0px}.az{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.ba{font-size:14px}.bb{line-height:20px}.bc{color:rgba(197, 210, 225, 1)}.bd{color:rgba(233, 241, 250, 1)}.be{fill:rgba(233, 241, 250, 1)}.bf{font-size:inherit}.bg{border:inherit}.bh{font-family:inherit}.bi{letter-spacing:inherit}.bj{font-weight:inherit}.bk{padding:0}.bl{margin:0}.bp:disabled{cursor:default}.bq:disabled{color:rgba(163, 208, 162, 0.5)}.br:disabled{fill:rgba(163, 208, 162, 0.5)}.bs{min-height:115px}.bt{justify-content:space-between}.bz{align-items:flex-start}.ca{margin-bottom:0px}.cb{margin-top:-32px}.cc{flex-wrap:wrap}.cf{margin-top:32px}.cg{margin-right:24px}.ci{height:35px}.cj{width:112px}.ck{flex:0 0 auto}.cl{justify-self:flex-end}.cm{margin-right:12px}.cn{height:32px}.co{overflow:visible}.cp{border-radius:1000px}.cq{background-color:rgba(53, 88, 118, 0.8)}.cr{fill:rgba(197, 210, 225, 1)}.cs{color:inherit}.ct{fill:rgba(117, 117, 117, 1)}.cu{outline:none}.cv{padding:4px}.cw{margin-left:8px}.cx{margin-right:10px}.cy{display:inline-block}.cz{border:none}.da{font:inherit}.db{font-size:16px}.dc{opacity:0}.dd{background-color:transparent}.de::placeholder{color:rgba(197, 210, 225, 1)}.df{padding:0px}.dg{width:0px}.dh{transition:width 140ms ease-in, padding 140ms ease-in}.di{fill:inherit}.dl:disabled{color:rgba(197, 210, 225, 1)}.dm:disabled{fill:rgba(197, 210, 225, 1)}.dn{padding:4px 12px 6px}.do{background:0}.dp{border-color:rgba(215, 226, 238, 1)}.dr:disabled{cursor:inherit}.ds:disabled{opacity:0.3}.dt:disabled:hover{color:rgba(233, 241, 250, 1)}.du:disabled:hover{fill:rgba(233, 241, 250, 1)}.dv:disabled:hover{border-color:rgba(215, 226, 238, 1)}.dw{border-radius:99em}.dx{border-width:1px}.dy{border-style:solid}.dz{box-sizing:border-box}.ea{text-decoration:none}.eb{fill:rgba(251, 255, 255, 1)}.ec{padding-top:1px}.ed{height:70px}.ef{line-height:24px}.eg:before{margin-bottom:-10px}.eh:before{content:""}.ei:before{display:table}.ej:before{border-collapse:collapse}.ek:after{margin-top:-6px}.el:after{content:""}.em:after{display:table}.en:after{border-collapse:collapse}.eo{color:rgba(117, 117, 117, 1)}.ep{margin-right:32px}.eq{margin-bottom:-16px}.er{margin-top:-14px}.es{color:rgba(255, 255, 255, 1)}.et{padding:7px 16px 9px}.eu{fill:rgba(255, 255, 255, 1)}.ev{background:rgba(102, 138, 170, 1)}.ew{border-color:rgba(102, 138, 170, 1)}.ez:disabled:hover{background:rgba(102, 138, 170, 1)}.fa:disabled:hover{border-color:rgba(102, 138, 170, 1)}.fb{display:inline-flex}.fe:disabled{color:rgba(117, 117, 117, 1)}.ff:disabled{fill:rgba(117, 117, 117, 1)}.fg{margin-left:12px}.fh{margin:0 12px}.fi{position:absolute}.fj{right:24px}.fk{margin:0px}.fl{border:0px}.fm{cursor:pointer}.fn{stroke:rgba(117, 117, 117, 1)}.fq{border-top:none}.fr{left:0}.fs{position:fixed}.ft{right:0}.fu{top:0}.fv{visibility:hidden}.fx{height:60px}.ga{color:rgba(102, 138, 170, 1)}.gb{fill:rgba(102, 138, 170, 1)}.ge{padding-left:24px}.gf{padding-right:24px}.gg{margin-left:auto}.gh{margin-right:auto}.gi{max-width:728px}.gj{background:rgba(255, 255, 255, 1)}.gk{border:1px solid rgba(230, 230, 230, 1)}.gl{border-radius:4px}.gm{box-shadow:0 1px 4px rgba(230, 230, 230, 1)}.gn{max-height:100vh}.go{overflow-y:auto}.gp{top:calc(100vh + 100px)}.gq{bottom:calc(100vh + 100px)}.gr{width:10px}.gs{pointer-events:none}.gt{word-break:break-word}.gu{word-wrap:break-word}.gv:after{display:block}.gw:after{clear:both}.gx{max-width:680px}.gy{line-height:1.23}.gz{letter-spacing:0}.ha{font-style:normal}.hb{font-weight:700}.hw{margin-bottom:-0.27em}.hx{color:rgba(41, 41, 41, 1)}.hy{line-height:1.394}.io{margin-bottom:-0.42em}.is{border-radius:50%}.it{height:28px}.iu{width:28px}.iv{margin:0 4px}.iw{margin:0 7px}.ix{align-items:flex-end}.jg{padding-right:8px}.jh{margin-right:8px}.ji{fill:rgba(61, 61, 61, 1)}.jj{margin-right:-4px}.jm{line-height:1.58}.jn{letter-spacing:-0.004em}.jo{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.kc{margin-top:24px}.kd{margin-bottom:-0.46em}.ke{font-style:italic}.kf{box-shadow:inset 3px 0 0 0 rgba(41, 41, 41, 1)}.kg{padding-left:23px}.kh{margin-left:-20px}.kn{text-decoration:underline}.ko{margin-bottom:14px}.kp{padding-top:24px}.kq{padding-bottom:10px}.kr{background-color:rgba(8, 8, 8, 1)}.ks{height:3px}.kt{width:3px}.ku{margin-right:20px}.kv{line-height:1.12}.kw{letter-spacing:-0.022em}.kx{font-weight:500}.lq{margin-bottom:-0.28em}.lr{line-height:1.18}.lz{margin-bottom:-0.31em}.mf{max-width:1250px}.ml{clear:both}.mn{cursor:zoom-in}.mo{z-index:auto}.mq{transition:opacity 100ms 400ms}.mr{height:100%}.ms{overflow:hidden}.mt{will-change:transform}.mu{transform:translateZ(0)}.mv{margin:auto}.mw{background-color:rgba(242, 242, 242, 1)}.mx{padding-bottom:50.64%}.my{height:0}.mz{filter:blur(20px)}.na{transform:scale(1.1)}.nb{visibility:visible}.nc{max-width:921px}.nd{padding-bottom:16.286644951140065%}.ne{padding-bottom:NaN%}.nf{max-width:542px}.ng{padding-bottom:73.61623616236162%}.nh{margin-top:10px}.ni{text-align:center}.nl{font-style:inherit}.nm{list-style-type:decimal}.nn{margin-left:30px}.no{padding-left:0px}.nu{max-width:1292px}.nv{padding-bottom:8.359133126934983%}.nw{max-width:1208px}.nx{padding-bottom:17.9635761589404%}.ny{max-width:1067px}.nz{padding-bottom:8.622305529522025%}.oa{max-width:1175px}.ob{padding-bottom:7.0638297872340425%}.oc{max-width:2167px}.od{padding-bottom:10.013844023996308%}.oe{max-width:858px}.of{padding-bottom:45.687645687645684%}.og{max-width:792px}.oh{padding-bottom:11.616161616161618%}.oi{padding:2px 4px}.oj{font-size:75%}.ok> strong{font-family:inherit}.ol{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.om{max-width:640px}.on{padding-bottom:75%}.ot{box-shadow:inset 0 0 0 1px rgba(230, 230, 230, 1)}.ou{padding:16px 20px}.ov{flex:1 1 auto}.ox{max-height:40px}.oy{text-overflow:ellipsis}.oz{display:-webkit-box}.pa{-webkit-line-clamp:2}.pb{-webkit-box-orient:vertical}.pd{margin-top:8px}.pe{margin-top:12px}.pf{font-size:13px}.pg{width:160px}.ph{background-image:url(https://miro.medium.com/max/320/0*qBUhy8WSI23EEXme)}.pi{background-origin:border-box}.pj{background-size:cover}.pk{height:167px}.pl{background-position:50% 50%}.pm{max-width:100%}.pn{will-change:opacity}.po{width:188px}.pp{left:50%}.pq{transform:translateX(406px)}.pr{top:calc(65px + 54px + 14px)}.pu{will-change:opacity, transform}.pv{transform:translateY(159px)}.px{width:131px}.py{padding-bottom:28px}.pz{border-bottom:1px solid rgba(230, 230, 230, 1)}.qa{padding-bottom:5px}.qb{padding-top:2px}.qc{padding-top:14px}.qd{padding-top:28px}.qe{margin-bottom:19px}.qf{margin-left:-3px}.ql{outline:0}.qm{border:0}.qn{user-select:none}.qo> svg{pointer-events:none}.qq{-webkit-user-select:none}.ra button{text-align:left}.rb{opacity:1}.rc{padding-right:9px}.rl{margin-top:40px}.rm{border-top:3px solid rgba(102, 138, 170, 1)}.rn{padding:32px 32px 26px 32px}.ro{margin-bottom:25px}.rp{background-color:rgba(250, 250, 250, 1)}.rr{padding-bottom:0px}.rs{padding-top:4px}.rt{padding-top:8px}.se{margin:10px 20px 10px 0}.sg{padding:7px 20px 9px}.sh{margin:10px 0 10px 0}.si{max-width:380px}.sj{padding-bottom:25px}.sk{margin-top:25px}.sl{max-width:155px}.sp{top:1px}.td{margin-left:-1px}.te{margin-left:-4px}.tm{padding-bottom:40px}.tn{list-style-type:none}.to{margin-bottom:8px}.tp{line-height:22px}.tq{border-radius:3px}.tr{padding:5px 10px}.ts{background:rgba(242, 242, 242, 1)}.tt{padding-bottom:4px}.tu{padding-top:32px}.uf{-webkit-line-clamp:1}.ug{padding-top:5px}.uh{padding-right:168px}.ui{padding-top:25px}.uo{margin-bottom:96px}.up{padding:32px 0}.uq{background-color:rgba(0, 0, 0, 0.9)}.uu:disabled{color:rgba(255, 255, 255, 0.7)}.uv:disabled{fill:rgba(255, 255, 255, 0.7)}.uw{height:22px}.ux{width:200px}.uz{color:rgba(255, 255, 255, 0.98)}.vd{color:rgba(255, 255, 255, 0.7)}.vg{margin-right:16px}.bm:hover{cursor:pointer}.bn:hover{color:rgba(251, 255, 255, 1)}.bo:hover{fill:rgba(251, 255, 255, 1)}.dj:hover{color:rgba(242, 248, 253, 1)}.dk:hover{fill:rgba(242, 248, 253, 1)}.dq:hover{border-color:rgba(251, 255, 255, 1)}.ex:hover{background:rgba(90, 118, 144, 1)}.ey:hover{border-color:rgba(90, 118, 144, 1)}.fc:hover{color:rgba(25, 25, 25, 1)}.fd:hover{fill:rgba(25, 25, 25, 1)}.gc:hover{color:rgba(90, 118, 144, 1)}.gd:hover{fill:rgba(90, 118, 144, 1)}.jk:hover{fill:rgba(8, 8, 8, 1)}.qs:hover{fill:rgba(117, 117, 117, 1)}.us:hover{color:rgba(255, 255, 255, 0.99)}.ut:hover{fill:rgba(255, 255, 255, 0.99)}.va:hover{text-decoration:underline}.jl:focus{fill:rgba(8, 8, 8, 1)}.mp:focus{transform:scale(1.01)}.qr:focus{fill:rgba(117, 117, 117, 1)}.qp:active{border-style:none}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.as{margin:0 64px}.hs{font-size:46px}.ht{margin-top:0.6em}.hu{line-height:56px}.hv{letter-spacing:-0.011em}.il{font-size:22px}.im{margin-top:0.92em}.in{line-height:28px}.je{margin-left:30px}.jz{font-size:21px}.ka{line-height:32px}.kb{letter-spacing:-0.003em}.km{margin-top:2em}.lm{font-size:30px}.ln{margin-top:1.25em}.lo{line-height:36px}.lp{letter-spacing:0}.ly{margin-top:1.72em}.me{margin-top:0.86em}.mk{margin-top:56px}.nt{margin-top:1.05em}.os{margin-top:32px}.qk{margin-right:5px}.qz{margin-top:5px}.rk{padding-left:6px}.sc{font-size:16px}.sd{line-height:24px}.sr{display:inline-block}.sw{margin-left:7px}.sx{margin-top:8px}.tc{width:25px}.tk{padding-left:7px}.tl{top:3px}.ud{font-size:20px}.ue{max-height:24px}.un{margin:0}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.jd{margin-left:30px}.nj{margin-left:auto}.nk{text-align:center}.qj{margin-right:5px}.qy{margin-top:5px}.rj{padding-left:6px}.sq{display:inline-block}.su{margin-left:7px}.sv{margin-top:8px}.tb{width:25px}.ti{padding-left:7px}.tj{top:3px}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.jc{margin-left:30px}.qi{margin-right:5px}.qx{margin-top:5px}.rh{padding-left:6px}.ri{top:3px}.so{display:inline-block}.ss{margin-left:7px}.st{margin-top:8px}.ta{width:15px}.th{padding-left:3px}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.u{margin-bottom:20px}.af{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.ag{min-height:230px}.ak{display:block}.bu{min-height:98px}.bv{display:flex}.bw{align-items:flex-start}.bx{flex-direction:column}.by{justify-content:flex-end}.cd{margin-bottom:28px}.ce{margin-top:0px}.ch{margin-top:28px}.ee{margin:0}.fo{border-top:1px solid rgba(230, 230, 230, 1)}.fp{border-bottom:1px solid rgba(230, 230, 230, 1)}.fy{align-items:center}.fz{flex:1 0 auto}.iq{margin-top:32px}.ir{flex-direction:column-reverse}.ja{margin-bottom:30px}.jb{margin-left:0px}.ow{padding:10px 12px 10px}.qh{margin-left:8px}.qv{margin-top:2px}.qw{margin-right:8px}.rf{padding-left:6px}.rg{top:3px}.rq{padding:24px 24px 28px 24px}.sn{display:inline-block}.sz{width:15px}.tg{padding-left:3px}.ur{padding:32px 0}.uy{width:140px}.vb{margin-bottom:16px}.vc{margin-top:30px}.ve{width:100%}.vf{flex-direction:row}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ao{margin:0 24px}.hc{font-size:32px}.hd{margin-top:0.64em}.he{line-height:40px}.hf{letter-spacing:-0.016em}.hz{font-size:18px}.ia{margin-top:0.79em}.ib{line-height:24px}.ip{margin-top:32px}.iy{margin-bottom:30px}.iz{margin-left:0px}.jp{line-height:28px}.jq{letter-spacing:-0.003em}.ki{margin-top:1.56em}.ky{font-size:22px}.kz{margin-top:0.93em}.la{letter-spacing:0}.ls{font-size:20px}.lt{margin-top:1.23em}.ma{margin-top:0.67em}.mg{margin-top:40px}.np{margin-top:1.34em}.oo{margin-top:24px}.qg{margin-left:8px}.qt{margin-top:2px}.qu{margin-right:8px}.rd{padding-left:6px}.re{top:3px}.ru{font-size:14px}.rv{line-height:20px}.sf{margin:10px 0 0 0}.sm{display:inline-block}.sy{width:15px}.tf{padding-left:3px}.tv{font-size:16px}.tw{max-height:20px}.uj{margin:0}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ar{margin:0 64px}.ho{font-size:46px}.hp{margin-top:0.6em}.hq{line-height:56px}.hr{letter-spacing:-0.011em}.ii{font-size:22px}.ij{margin-top:0.92em}.ik{line-height:28px}.jw{font-size:21px}.jx{line-height:32px}.jy{letter-spacing:-0.003em}.kl{margin-top:2em}.li{font-size:30px}.lj{margin-top:1.25em}.lk{line-height:36px}.ll{letter-spacing:0}.lx{margin-top:1.72em}.md{margin-top:0.86em}.mj{margin-top:56px}.ns{margin-top:1.05em}.or{margin-top:32px}.sa{font-size:16px}.sb{line-height:24px}.ub{font-size:20px}.uc{max-height:24px}.um{margin:0}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.aq{margin:0 48px}.hk{font-size:46px}.hl{margin-top:0.6em}.hm{line-height:56px}.hn{letter-spacing:-0.011em}.if{font-size:22px}.ig{margin-top:0.92em}.ih{line-height:28px}.jt{font-size:21px}.ju{line-height:32px}.jv{letter-spacing:-0.003em}.kk{margin-top:2em}.le{font-size:30px}.lf{margin-top:1.25em}.lg{line-height:36px}.lh{letter-spacing:0}.lw{margin-top:1.72em}.mc{margin-top:0.86em}.mi{margin-top:56px}.nr{margin-top:1.05em}.oq{margin-top:32px}.ry{font-size:16px}.rz{line-height:24px}.tz{font-size:20px}.ua{max-height:24px}.ul{margin:0}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ap{margin:0 24px}.hg{font-size:32px}.hh{margin-top:0.64em}.hi{line-height:40px}.hj{letter-spacing:-0.016em}.ic{font-size:18px}.id{margin-top:0.79em}.ie{line-height:24px}.jr{line-height:28px}.js{letter-spacing:-0.003em}.kj{margin-top:1.56em}.lb{font-size:22px}.lc{margin-top:0.93em}.ld{letter-spacing:0}.lu{font-size:20px}.lv{margin-top:1.23em}.mb{margin-top:0.67em}.mh{margin-top:40px}.nq{margin-top:1.34em}.op{margin-top:24px}.rw{font-size:14px}.rx{line-height:20px}.tx{font-size:16px}.ty{max-height:20px}.uk{margin:0}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="print">.jf{display:none}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.fw{animation:k2 .2s ease-in-out both}.mm{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.ps{transition:opacity 200ms}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.pc{max-height:none}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (max-width: 1230px)">.pt{display:none}</style><style type="text/css" data-fela-rehydration="578" data-fela-type="RULE" media="all and (max-width: 1198px)">.pw{display:none}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><div class="s"><div class="t s u"><div class="ac ae s af ag"><div class="n ah ai"><div class="aj ak"><div class="al s am an"><div class="n p"><div class="ao ap aq ar as at au v"><div class="av n o"><div class="n o aw ax"><div class="ay aj ak"><span class="az b ba bb bc"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2d94655a9b7b&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----2d94655a9b7b--------------------------------" class="bd be bf bg bh bi bj bk bl bm bn bo bp bq br" rel="noopener nofollow">Open in app</a></span></div></div><a href="https://medium.com/?source=post_page-----2d94655a9b7b--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q be"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><div class="n p"><div class="ao ap aq ar as at au v"><div class="bs n o aw bt bu bv bw bx by"><div class="v n bz bt"><div class="n v"><div class="ca cb v n o aw cc cd ce bv bw bx"><div class="cf cg s ch"><a href="/?source=post_page-----2d94655a9b7b--------------------------------" aria-label="Publication Homepage" rel="noopener"><div class="ci cj s"><img alt="Towards Data Science" class="" src="https://miro.medium.com/max/224/1*AGyTPCaRzVqL77kFwUwHKg.png" width="112" height="35"/></div></a></div></div></div><div class="n o ck cl an g"><div class="cm cn co n o"><div class="cp cq"><div class="n" aria-hidden="false" aria-describedby="publicationMenu" aria-labelledby="publicationMenu"><button aria-controls="publicationMenu" aria-expanded="false" aria-label="Publication Menu" class="cs ct bf bg bh bi bj bk bl cu bm"><div class="cv s"><svg class="cr bo" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></div></button></div></div><div class="cw cx s"><div class="cp cq"><div class="cy" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"><div class="n"><button class="cs di bf bg bh bi bj bk bl bm dj dk bp dl dm"><span class="cv s"><svg width="25" height="25" viewBox="0 0 25 25" class="cr"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></span></button><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-hidden="true" tabindex="-1" class="cz cu da db bb dc dd bd de am df dg dh" placeholder="Search" value=""/></div></div></div></div><div class="cm s g"><div><a href="https://medium.com/plans?source=upgrade_membership---nav_full----------------------------------" class="az b ba bb bd dn be do dp bn bo dq bm dr ds dt du dv dw dx dy dz cy ea" rel="noopener">Upgrade</a></div></div></div><a href="https://medium.com/?source=post_page-----2d94655a9b7b--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q eb"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><div class="s ak"><div class="n p"><div class="ao ap aq ar as at au v"><div class="w x y z ab"><div class="ec ed n o"><div class="s ee"><span class="az b db ef eg eh ei ej ek el em en eo"><div class="n o"><div class="ep s"><div class="eq er s"><div class="cy" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><button class="az b ba bb es et eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea" aria-controls="collectionFollowPopover" aria-expanded="false"><div class="n aw">Follow</div></button></div></div></div><div class="cm fb ah"><a class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener" href="/followers?source=post_page-----2d94655a9b7b--------------------------------">581K Followers</a></div><div class="fg s g">·</div><div class="fg s g"><nav class="n o"><span class="fh n ah"><a href="https://towardsdatascience.com/tagged/editors-pick?source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">Editors&#x27; Picks</a></span><span class="fh n ah"><a href="https://towardsdatascience.com/tagged/tds-features?source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">Features</a></span><span class="fh n ah"><a href="https://towardsdatascience.com/tagged/deep-dives?source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">Deep Dives</a></span><span class="fh n ah"><a class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener" href="/how-to-get-the-most-out-of-towards-data-science-3bf37f75a345?source=post_page-----2d94655a9b7b--------------------------------">Grow</a></span><span class="fh n ah"><a class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener" href="/questions-96667b06af5?source=post_page-----2d94655a9b7b--------------------------------">Contribute</a></span></nav></div><div class="fg n ah g"><a href="/about?source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">About</a></div></div></span></div><div class="aj fi fj ak"><button class="n o p fk fl df fm" aria-label="Expand navbar"><svg width="14" height="14" class="fn"><path d="M0 .5h14M0 7h14M0 13.5h14"></path></svg></button></div></div></div></div></div></div></div><div class="fo fp fq al c fr dc fs ft fu fv an fw"><div class="n p"><div class="ao ap aq ar as at au v"><div class="fx v aj fu an bv fy"><div class="aj bv fy fz"><div class="ay aj ak"><span class="az b ba bb eo"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2d94655a9b7b&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----2d94655a9b7b--------------------------------" class="ga gb bf bg bh bi bj bk bl bm gc gd bp bq br" rel="noopener nofollow">Open in app</a></span></div></div><a href="https://medium.com/?source=post_page-----2d94655a9b7b--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><article><section class="ge gf gg gh v gi dz s"></section><span class="s"></span><div><div class="fi fr gp gq gr gs"></div><section class="gt gu gv el gw"><div class="n p"><div class="ao ap aq ar as gx au v"><div class=""><h1 id="00d2" class="gy gz ha az hb hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx">Deep Deterministic Policy Gradients Explained</h1></div><div class=""><h2 id="c1b7" class="hy gz ha az b hz ia ib ic id ie if ig ih ii ij ik il im in io eo"><strong class="bj">Reinforcement Learning in Continuous Action Spaces</strong></h2><div class="cf"><div class="n bt ip iq ir"><div class="o n"><div><a href="https://medium.com/@thechrisyoon?source=post_page-----2d94655a9b7b--------------------------------" rel="noopener"><img alt="Chris Yoon" class="s is it iu" src="https://miro.medium.com/fit/c/56/56/1*mNiVdAV2aE_fZF9dLG09Ew.jpeg" width="28" height="28"/></a></div><div class="fg v n cc"><div class="n"><div style="flex:1"><span class="az b ba bb hx"><a href="https://medium.com/@thechrisyoon?source=post_page-----2d94655a9b7b--------------------------------" class="" rel="noopener"><p class="az b ba bb ga">Chris Yoon</p></a></span></div></div><span class="az b ba bb eo"><a class="" rel="noopener" href="/deep-deterministic-policy-gradients-explained-2d94655a9b7b?source=post_page-----2d94655a9b7b--------------------------------"><p class="az b ba bb eo"><span class="iv"></span>Mar 20, 2019<span class="iw">·</span>6 min read</p></a></span></div></div><div class="n ix iy iz ja jb jc jd je jf"><div class="n o"><div class="jg s"><div class="cy" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="jh s"><div><div class="ji"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="s ax"><div class="cy" aria-hidden="false" aria-describedby="creatorActionOverflowMenu" aria-labelledby="creatorActionOverflowMenu"><div class="cy" aria-hidden="false" aria-describedby="removeFromPublicationPopover" aria-labelledby="removeFromPublicationPopover"><div class="jj s ck"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-controls="creatorActionOverflowMenu" aria-expanded="false" aria-label="More options"><svg class="r jk jl" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><p id="e127" class="jm jn ha jo b hz jp jq ic jr js jt ju jv jw jx jy jz ka kb kc kd gt hx">This post is a <strong class="jo hb">thorough</strong> review of Deepmind’s publication <em class="ke">“Continuous Control With Deep Reinforcement Learning”</em> (Lillicrap et al, 2015), in which the Deep Deterministic Policy Gradients (DDPG) is presented, and is written for people who wish to understand the DDPG algorithm. If you are interested only in the implementation, you can skip to the final section of this post.</p><blockquote class="kf kg kh"><p id="80a6" class="jm jn ke jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">This post is written with the assumption that the reader is familiar with basic reinforcement learning concepts, value &amp; policy learning, and actor critic methods. If you are not completely familiar with those concepts, I have also written about <a href="https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63" class="cs kn" rel="noopener">policy gradients</a> and <a class="cs kn" rel="noopener" href="/understanding-actor-critic-methods-931b97b6df3f">actor critic methods</a>.</p><p id="5173" class="jm jn ke jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">Familiarity with python and PyTorch will also be really helpful for reading through this post. If you are not familiar with PyTorch, try to follow the code snippets as if they are pseudo-code.</p></blockquote></div></div></section><div class="n p cf ko kp kq" role="separator"><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt"></span></div><section class="gt gu gv el gw"><div class="n p"><div class="ao ap aq ar as gx au v"><h1 id="b762" class="kv kw ha az kx ky kz jp la lb lc jr ld le lf lg lh li lj lk ll lm ln lo lp lq hx">Going through the paper</h1><h2 id="f6ec" class="lr kw ha az kx ls lt ib la lu lv ie ld if lw ih lh ii lx ik ll il ly in lp lz hx">Network Schematics</h2><p id="d6a9" class="jm jn ha jo b hz ma jp jq ic mb jr js jt mc ju jv jw md jx jy jz me ka kb kd gt hx">DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh mf"><div class="mv s am mw"><div class="mx my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*-87grz5iUZK4i7NCH1ldbw.png?q=20" width="1250" height="633" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="1250" height="633" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/2500/1*-87grz5iUZK4i7NCH1ldbw.png" width="1250" height="633" srcSet="https://miro.medium.com/max/552/1*-87grz5iUZK4i7NCH1ldbw.png 276w, https://miro.medium.com/max/1104/1*-87grz5iUZK4i7NCH1ldbw.png 552w, https://miro.medium.com/max/1280/1*-87grz5iUZK4i7NCH1ldbw.png 640w, https://miro.medium.com/max/1400/1*-87grz5iUZK4i7NCH1ldbw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="7e9f" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">The Q network and po<span id="rmm">l</span>icy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space</p><p id="8c05" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improve stability in learning. Here’s why: In methods that do not use target networks, the update equations of the network are interdependent on the values calculated by the network itself, which makes it prone to divergence. For example:</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh nc"><div class="mv s am mw"><div class="nd my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*rWEAu4HKQIzFTJw3i70Mkg.png?q=20" width="921" height="150" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="921" height="150" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/1842/1*rWEAu4HKQIzFTJw3i70Mkg.png" width="921" height="150" srcSet="https://miro.medium.com/max/552/1*rWEAu4HKQIzFTJw3i70Mkg.png 276w, https://miro.medium.com/max/1104/1*rWEAu4HKQIzFTJw3i70Mkg.png 552w, https://miro.medium.com/max/1280/1*rWEAu4HKQIzFTJw3i70Mkg.png 640w, https://miro.medium.com/max/1400/1*rWEAu4HKQIzFTJw3i70Mkg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure></div></div></section><div class="n p cf ko kp kq" role="separator"><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt"></span></div><section class="gt gu gv el gw"><div class="n p"><div class="ao ap aq ar as gx au v"><p id="7df4" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">So, we have the standard Actor &amp; Critic architecture for the deterministic policy network and the Q network:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="607d" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">And we initialize the networks and target networks as:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure></div></div></section><div class="n p cf ko kp kq" role="separator"><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt"></span></div><section class="gt gu gv el gw"><div class="n p"><div class="ao ap aq ar as gx au v"><h1 id="ca72" class="kv kw ha az kx ky kz jp la lb lc jr ld le lf lg lh li lj lk ll lm ln lo lp lq hx">Learning</h1><p id="f488" class="jm jn ha jo b hz ma jp jq ic mb jr js jt mc ju jv jw md jx jy jz me ka kb kd gt hx">So, here’s the pseudo-code of the algorithm that we want to implement:</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div class="gg gh nf"><div class="mv s am mw"><div class="ng my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*BVST6rlxL2csw3vxpeBS8Q.png?q=20" width="542" height="399" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="542" height="399" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/1084/1*BVST6rlxL2csw3vxpeBS8Q.png" width="542" height="399" srcSet="https://miro.medium.com/max/552/1*BVST6rlxL2csw3vxpeBS8Q.png 276w, https://miro.medium.com/max/1084/1*BVST6rlxL2csw3vxpeBS8Q.png 542w" sizes="542px" role="presentation"/></noscript></div></div></div><figcaption class="nh ni gi gg gh nj nk az b ba bb eo">Taken from <em class="nl">“Continuous Control With Deep Reinforcement Learning”</em> (Lillicrap et al, 2015)</figcaption></figure><p id="c6fd" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">We are going to break this down into:</p><ol class=""><li id="f31c" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd nm nn no hx">Experience replay</li><li id="3946" class="jm jn ha jo b hz np jp jq ic nq jr js jt nr ju jv jw ns jx jy jz nt ka kb kd nm nn no hx">Actor &amp; Critic network updates</li><li id="ea4f" class="jm jn ha jo b hz np jp jq ic nq jr js jt nr ju jv jw ns jx jy jz nt ka kb kd nm nn no hx">Target network updates</li><li id="2ff1" class="jm jn ha jo b hz np jp jq ic nq jr js jt nr ju jv jw ns jx jy jz nt ka kb kd nm nn no hx">Exploration</li></ol><h2 id="7e7b" class="lr kw ha az kx ls lt ib la lu lv ie ld if lw ih lh ii lx ik ll il ly in lp lz hx">Replay Buffer</h2><p id="8060" class="jm jn ha jo b hz ma jp jq ic mb jr js jt mc ju jv jw md jx jy jz me ka kb kd gt hx">As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters. During each trajectory roll-out, we save all the experience tuples (state, action, reward, next_state) and store them in a finite-sized cache — a “replay buffer.” Then, we sample random mini-batches of experience from the replay buffer when we update the value and policy networks.</p><p id="e359" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">Here’s how the replay buffer looks like:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="23e4" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">Why do we use experience replay? In optimization asks, we want the data to be independently distributed. This fails to be the case when we optimize a sequential decision process in an on-policy way, because the data then would not be independent of each other. When we store them in a replay buffer and take random batches for training, we overcome this issue.</p><h2 id="795e" class="lr kw ha az kx ls lt ib la lu lv ie ld if lw ih lh ii lx ik ll il ly in lp lz hx">Actor (Policy) &amp; Critic (Value) Network Updates</h2><p id="bf97" class="jm jn ha jo b hz ma jp jq ic mb jr js jt mc ju jv jw md jx jy jz me ka kb kd gt hx">The value network is updated similarly as is done in Q-learning. The updated Q value is obtained by the Bellman equation:</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh nu"><div class="mv s am mw"><div class="nv my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*UJ5fl6SemqEjoMX9Ns0jSA.png?q=20" width="1292" height="108" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="1292" height="108" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/2584/1*UJ5fl6SemqEjoMX9Ns0jSA.png" width="1292" height="108" srcSet="https://miro.medium.com/max/552/1*UJ5fl6SemqEjoMX9Ns0jSA.png 276w, https://miro.medium.com/max/1104/1*UJ5fl6SemqEjoMX9Ns0jSA.png 552w, https://miro.medium.com/max/1280/1*UJ5fl6SemqEjoMX9Ns0jSA.png 640w, https://miro.medium.com/max/1400/1*UJ5fl6SemqEjoMX9Ns0jSA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="3de1" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">However, in DDPG, the <strong class="jo hb">next-state Q values are calculated with the target value network and target policy network</strong>. Then, we minimize the mean-squared loss between the updated Q value and the original Q value:</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh nw"><div class="mv s am mw"><div class="nx my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*96LU4dbopEncLQ9vc3pCzg.png?q=20" width="1208" height="217" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="1208" height="217" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/2416/1*96LU4dbopEncLQ9vc3pCzg.png" width="1208" height="217" srcSet="https://miro.medium.com/max/552/1*96LU4dbopEncLQ9vc3pCzg.png 276w, https://miro.medium.com/max/1104/1*96LU4dbopEncLQ9vc3pCzg.png 552w, https://miro.medium.com/max/1280/1*96LU4dbopEncLQ9vc3pCzg.png 640w, https://miro.medium.com/max/1400/1*96LU4dbopEncLQ9vc3pCzg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="1486" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx"><strong class="jo hb">* Note that the original Q value is calculated with the value network, not the target value network.</strong></p><p id="bc02" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">In code, this looks like:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="2947" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">For the policy function, our objective is to maximize the expected return:</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh ny"><div class="mv s am mw"><div class="nz my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*PF4odMpSZi1LWmcf7JhBZQ.png?q=20" width="1067" height="92" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="1067" height="92" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/2134/1*PF4odMpSZi1LWmcf7JhBZQ.png" width="1067" height="92" srcSet="https://miro.medium.com/max/552/1*PF4odMpSZi1LWmcf7JhBZQ.png 276w, https://miro.medium.com/max/1104/1*PF4odMpSZi1LWmcf7JhBZQ.png 552w, https://miro.medium.com/max/1280/1*PF4odMpSZi1LWmcf7JhBZQ.png 640w, https://miro.medium.com/max/1400/1*PF4odMpSZi1LWmcf7JhBZQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="2b5c" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">To calculate the policy loss, we take the derivative of the objective function with respect to the policy parameter. Keep in mind that the actor (policy) function is differentiable, so we have to apply the chain rule.</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh oa"><div class="mv s am mw"><div class="ob my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*IFFNXK4CZMcmJ4g5nYstiw.png?q=20" width="1175" height="83" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="1175" height="83" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/2350/1*IFFNXK4CZMcmJ4g5nYstiw.png" width="1175" height="83" srcSet="https://miro.medium.com/max/552/1*IFFNXK4CZMcmJ4g5nYstiw.png 276w, https://miro.medium.com/max/1104/1*IFFNXK4CZMcmJ4g5nYstiw.png 552w, https://miro.medium.com/max/1280/1*IFFNXK4CZMcmJ4g5nYstiw.png 640w, https://miro.medium.com/max/1400/1*IFFNXK4CZMcmJ4g5nYstiw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="53c5" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh oc"><div class="mv s am mw"><div class="od my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*ta1Nn1sI6juWuPMJc3qepA.png?q=20" width="2167" height="217" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="2167" height="217" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/4334/1*ta1Nn1sI6juWuPMJc3qepA.png" width="2167" height="217" srcSet="https://miro.medium.com/max/552/1*ta1Nn1sI6juWuPMJc3qepA.png 276w, https://miro.medium.com/max/1104/1*ta1Nn1sI6juWuPMJc3qepA.png 552w, https://miro.medium.com/max/1280/1*ta1Nn1sI6juWuPMJc3qepA.png 640w, https://miro.medium.com/max/1400/1*ta1Nn1sI6juWuPMJc3qepA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="5ee0" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">In code, this looks like:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="a1e1" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">Where the optimizers use Adaptive Moment Estimation (ADAM):</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><h2 id="f77d" class="lr kw ha az kx ls lt ib la lu lv ie ld if lw ih lh ii lx ik ll il ly in lp lz hx">Target Network Updates</h2><p id="a63c" class="jm jn ha jo b hz ma jp jq ic mb jr js jt mc ju jv jw md jx jy jz me ka kb kd gt hx">We make a copy of the target network parameters and have them slowly track those of the learned networks via “soft updates,” as illustrated below:</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh oe"><div class="mv s am mw"><div class="of my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*LBlJpAQBLF95LsheOusmuA.png?q=20" width="858" height="392" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="858" height="392" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/1716/1*LBlJpAQBLF95LsheOusmuA.png" width="858" height="392" srcSet="https://miro.medium.com/max/552/1*LBlJpAQBLF95LsheOusmuA.png 276w, https://miro.medium.com/max/1104/1*LBlJpAQBLF95LsheOusmuA.png 552w, https://miro.medium.com/max/1280/1*LBlJpAQBLF95LsheOusmuA.png 640w, https://miro.medium.com/max/1400/1*LBlJpAQBLF95LsheOusmuA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="7a47" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">This can be implemented very simply:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><h2 id="8b92" class="lr kw ha az kx ls lt ib la lu lv ie ld if lw ih lh ii lx ik ll il ly in lp lz hx">Exploration</h2><p id="4892" class="jm jn ha jo b hz ma jp jq ic mb jr js jt mc ju jv jw md jx jy jz me ka kb kd gt hx">In Reinforcement learning for discrete action spaces, exploration is done via probabilistically selecting a random action (such as epsilon-greedy or Boltzmann exploration). For continuous action spaces, exploration is done via adding noise to the action itself (there is also the parameter space noise but we will skip that for now). In the DDPG paper, the authors use <em class="ke">Ornstein-Uhlenbeck Process </em>to add noise<em class="ke"> </em>to the action output (Uhlenbeck &amp; Ornstein, 1930):</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div role="button" tabindex="0" class="mm mn am mo v mp"><div class="gg gh og"><div class="mv s am mw"><div class="oh my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*LmL-P-_o10NWtQcF2PmkBg.png?q=20" width="792" height="92" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="792" height="92" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/1584/1*LmL-P-_o10NWtQcF2PmkBg.png" width="792" height="92" srcSet="https://miro.medium.com/max/552/1*LmL-P-_o10NWtQcF2PmkBg.png 276w, https://miro.medium.com/max/1104/1*LmL-P-_o10NWtQcF2PmkBg.png 552w, https://miro.medium.com/max/1280/1*LmL-P-_o10NWtQcF2PmkBg.png 640w, https://miro.medium.com/max/1400/1*LmL-P-_o10NWtQcF2PmkBg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="772f" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">The <em class="ke">Ornstein-Uhlenbeck Process</em> generates noise that is correlated with the previous noise, as to prevent the noise from canceling out or “freezing” the overall dynamics <strong class="jo hb">[1]</strong>. <a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" class="cs kn" rel="noopener nofollow">Wikipedia provides a thorough explanation of the <em class="ke">Ornstein-Uhlenbeck Process</em>.</a></p><p id="19ce" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">Here’s a python implementation written by Pong et al:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="7bad" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">So we input the action produced by the actor network into <code class="mw oi oj ok ol b">get_action()</code> function, and get a new action to which the temporally correlated noise is added.</p><p id="0336" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">We are all set now!</p></div></div></section><div class="n p cf ko kp kq" role="separator"><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt"></span></div><section class="gt gu gv el gw"><div class="n p"><div class="ao ap aq ar as gx au v"><h1 id="dd17" class="kv kw ha az kx ky kz jp la lb lc jr ld le lf lg lh li lj lk ll lm ln lo lp lq hx">Putting them all together</h1><p id="dbe0" class="jm jn ha jo b hz ma jp jq ic mb jr js jt mc ju jv jw md jx jy jz me ka kb kd gt hx">We have here the Replay Buffer, the Ornstein-Uhlenbeck Process, and the normalized Action Wrapper for OpenAI Gym continuous control environments in <em class="ke">utils.py</em>:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="899e" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">And the Actor &amp; Critic networks in <em class="ke">models.py</em>:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="c8c8" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">And the DDPG agent in <em class="ke">ddpg.py</em>:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="6536" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">And the test in <em class="ke">main.py</em>:</p><figure class="mg mh mi mj mk ml"><div class="mv s am"><div class="ne my s"></div></div></figure><p id="0549" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">And we can see if the DDPG agent learns optimal policy for the classic Inverted Pendulum task:</p><figure class="mg mh mi mj mk ml gg gh paragraph-image"><div class="gg gh om"><div class="mv s am mw"><div class="on my s"><div class="dc mq fi fu fr mr v ms mt mu"><img alt="" class="fi fu fr mr v mz na nb" src="https://miro.medium.com/max/60/1*miNYafAHXGXMyZx_-NKV0A.png?q=20" width="640" height="480" role="presentation"/></div><img alt="" class="dc mq fi fu fr mr v c" width="640" height="480" role="presentation"/><noscript><img alt="" class="fi fu fr mr v" src="https://miro.medium.com/max/1280/1*miNYafAHXGXMyZx_-NKV0A.png" width="640" height="480" srcSet="https://miro.medium.com/max/552/1*miNYafAHXGXMyZx_-NKV0A.png 276w, https://miro.medium.com/max/1104/1*miNYafAHXGXMyZx_-NKV0A.png 552w, https://miro.medium.com/max/1280/1*miNYafAHXGXMyZx_-NKV0A.png 640w" sizes="640px" role="presentation"/></noscript></div></div></div></figure><p id="48d2" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">That’s it for DDPG!</p><p id="3ee2" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx">Find full implementation here:</p><div class="oo op oq or os ot"><a href="https://github.com/thechrisyoon08/Reinforcement-Learning" target="_blank" rel="noopener nofollow"><div class="df n ck"><div class="ou n ah p ov ow"><h2 class="az hb db bb ms ox oy oz pa pb pc gz hx">thechrisyoon08/Reinforcement-Learning</h2><div class="pd s"><h3 class="az b db bb ms ox oy oz pa pb pc eo">Modular implementations of reinforcement learning algorithms with Python and PyTorch …</h3></div><div class="pe s"><p class="az b pf bb ms ox oy oz pa pb pc eo">github.com</p></div></div><div class="pg s"><div class="ph s pi pj pk pg pl pm ot"></div></div></div></a></div></div></div></section><div class="n p cf ko kp kq" role="separator"><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt ku"></span><span class="kr is cy ks kt"></span></div><section class="gt gu gv el gw"><div class="n p"><div class="ao ap aq ar as gx au v"><h1 id="aaac" class="kv kw ha az kx ky kz jp la lb lc jr ld le lf lg lh li lj lk ll lm ln lo lp lq hx">References</h1><p id="474d" class="jm jn ha jo b hz ma jp jq ic mb jr js jt mc ju jv jw md jx jy jz me ka kb kd gt hx"><a href="https://arxiv.org/abs/1509.02971" class="cs kn" rel="noopener nofollow">Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra, <em class="ke">Continuous control with deep reinforcement learning</em>, CoRR abs/1509.02971 (2015).</a></p><p id="e577" class="jm jn ha jo b hz ki jp jq ic kj jr js jt kk ju jv jw kl jx jy jz km ka kb kd gt hx"><a href="https://qr.ae/TW8NAa" class="cs kn" rel="noopener nofollow"><strong class="jo hb">[1] </strong>Edouard Leurent’s answer to Quora post<strong class="jo hb"> “</strong><em class="ke">Why do we use the Ornstein Uhlenbeck Process in the exploration of DDPG?”</em></a></p></div></div></section></div></article><div class="dc gs fs pu v pv fu ps pw" data-test-id="post-sidebar"><div class="n p"><div class="ao ap aq ar as at au v"><div class="px n ah"><div class="gs"><div><div class="py pz s"><div class="qa s"><a href="https://medium.com/@thechrisyoon?source=post_sidebar--------------------------post_sidebar-----------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener"><h2 class="az kx db bb gz hx gt">Chris Yoon</h2></a></div><div class="qb s"><p class="az b ba bb eo">Student in NYC. <a href="https://www.linkedin.com/in/chris-yoon-75847418b/" class="cs di bf bg bh bi bj bk bl bm bp fe ff kn gu" rel="noopener nofollow">https://www.linkedin.com/in/chris-yoon-75847418b/</a></p></div><div class="qc s"><button class="az b ba bb es et eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea">Follow</button></div></div><div class="qd qe qf n"><div class="n o"><div class="s am qg qh qi qj qk"><div class=""><button class="bk ql qm qn fm qo qp qq r qr qs"><svg width="29" height="29" aria-label="clap"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s qt qu qv qw qx qy qz"><div class="ra"><p class="az b ba bb eo"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff">1K<!-- --> </button></p></div></div></div></div><div class="qe s"><button class="fm qm bk"><div class="rc n o aw"><svg width="25" height="25" class="r rb fm qs" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg><div class="s am rd re rf rg rh ri rj rk"><p class="az b ba bb eo">9<!-- --> </p></div></div></button></div><div><div class="ji"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="dc gs pn fs po pp pq pr ps pt"></div><div><div class="rl ml n ah p"><div class="n p"><div class="ao ap aq ar as gx au v"><div class="n cc"></div><div class="n o cc"></div><div class="rm rn pd ro rp rq"><div class="rr s"><h2 class="az kx ls ib la lu ie ld if ih lh ii ik ll il in lp hx">Sign up for The Variable</h2></div><div class="rs s"><h3 class="az b pf bb hx">By Towards Data Science</h3></div><div class="kq rt s"><p class="az b ru rv rw rx ry rz sa sb sc sd hx">Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don&#x27;t want to miss.<!-- --> <a href="https://medium.com/towards-data-science/newsletters/the-variable?source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="cs di bf bg bh bi bj bk bl bm bp fe ff kn" rel="noopener">Take a look.</a></p></div><div class="n cc"><div class="se s sf"><button class="az b db ef es sg eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea"><span class="jh" aria-hidden="true"><svg width="20" height="16" viewBox="0 0 20 16"><path d="M0 .35v15.3h20V.35H0zm6.95 9.38l3.05 2.5 3.05-2.5 4.88 4.73H2.07l4.88-4.73zM1.2 13.64V5.02l4.82 3.94-4.82 4.68zm12.78-4.68l4.82-3.94v8.62l-4.82-4.68zm4.82-7.42v1.94l-8.8 7.2-8.8-7.2V1.54h17.6z"></path></svg></span>Get this newsletter</button></div><div class="sh si s"><p class="az b pf bb hx">Emails will be sent to <!-- -->mat0503@hotmail.com<!-- -->.<div class="s"><span><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b&amp;source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener"><button class="cs di bf bg bh bi bj bk bl bm bp fe ff kn" target="_blank">Not you?</button></a></span></div></p></div></div></div><div class="sj rl s"><div class="sk n bt jf"><div class="n aw"><div class="sl s"><span class="s sm sn so e d"><div class="n o"><div class="s am qg qh qi qj qk"><div class=""><div><div class="cy" role="tooltip" aria-hidden="false"><button class="bk ql qm qn fm qo qp qq r qr qs"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s qt qu qv qw qx qy qz"><div class="am sp ra"><p class="az b ba bb hx"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff">1K<span class="s h g f sq sr"> </span></button><span class="s h g f sq sr"></span></p></div></div></div></span><span class="s h g f sq sr"><div class="n bz"><div class="s am qg qh"><div class=""><div><div class="cy" role="tooltip" aria-hidden="false"><button class="bk ql qm qn fm qo qp qq r qr qs"><svg width="33" height="33" viewBox="0 0 33 33" aria-label="clap"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="s qt qu qv qw ss st su sv sw sx"><div class="am sp ra"><p class="az b ba bb hx"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff">1K<span class="s h g f sq sr"> </span></button><span class="s h g f sq sr"></span></p></div></div></div></span></div><div class="s sy sz ta tb tc"></div><button class="fm qm bk"><div class="rc n o aw"><span class="td s h g f sq sr"><svg width="33" height="33" viewBox="0 0 33 33" fill="none" class="r rb fm qs" aria-label="responses"><path fill-rule="evenodd" clip-rule="evenodd" d="M24.28 25.5l.32-.29c2.11-1.94 3.4-4.61 3.4-7.56C28 11.83 22.92 7 16.5 7S5 11.83 5 17.65s5.08 10.66 11.5 10.66c1.22 0 2.4-.18 3.5-.5l.5-.15.41.33a8.86 8.86 0 0 0 4.68 2.1 7.34 7.34 0 0 1-1.3-4.15v-.43zm1 .45c0 1.5.46 2.62 1.69 4.44.22.32.01.75-.38.75a9.69 9.69 0 0 1-6.31-2.37c-1.2.35-2.46.54-3.78.54C9.6 29.3 4 24.09 4 17.65 4 11.22 9.6 6 16.5 6S29 11.22 29 17.65c0 3.25-1.42 6.18-3.72 8.3z"></path></svg></span><span class="te s sm sn so e d"><svg width="25" height="25" class="r rb fm qs" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></span><div class="s am tf re tg rg th ri ti tj tk tl"><p class="az b ba bb eo">9<!-- --> </p></div></div></button></div><div class="n o"><div class="jg s"><div class="cy" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="jg s ck"><div><div class="ji"><div><div class="cy" role="tooltip" aria-hidden="false"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="cy" aria-hidden="false" aria-describedby="creatorActionOverflowMenu" aria-labelledby="creatorActionOverflowMenu"><div class="cy" aria-hidden="false" aria-describedby="removeFromPublicationPopover" aria-labelledby="removeFromPublicationPopover"><div class="jj s ck"><button class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" aria-controls="creatorActionOverflowMenu" aria-expanded="false" aria-label="More options"><svg class="r jk jl" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div><div class="tm sk s"><ul class="bk bl"><li class="cy tn jh to"><a href="/tagged/reinforcement-learning" class="az b pf tp eo tq tr ea s ts">Reinforcement Learning</a></li><li class="cy tn jh to"><a href="/tagged/machine-learning" class="az b pf tp eo tq tr ea s ts">Machine Learning</a></li><li class="cy tn jh to"><a href="/tagged/artificial-intelligence" class="az b pf tp eo tq tr ea s ts">Artificial Intelligence</a></li><li class="cy tn jh to"><a href="/tagged/control" class="az b pf tp eo tq tr ea s ts">Control</a></li></ul></div></div></div><div><div class="n p"><div class="ao ap aq ar as gx au v"></div></div><div class="s jf"><div class="tt tu s rp"><div class="n p"><div class="ao ap aq ar as gx au v"><div class="n o bt"><h2 class="az kx tv rv tw la tx rx ty ld tz rz ua lh ub sb uc ll ud sd ue lp ms oy oz uf pb pc hx"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="cs di bf bg bh bi bj bk bl bm fc fd bp fe ff" rel="noopener">More from Towards Data Science</a></h2><div class="cy" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><button class="az b ba bb es et eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea" aria-controls="collectionFollowPopover" aria-expanded="false"><div class="n aw">Follow</div></button></div></div><div class="ug uh s"><p class="az b ba bb eo">Your home for data science. A Medium publication sharing concepts, ideas and codes.</p></div></div></div></div></div><div class="ui s rp jf"><div class="n p"><div class="uj uk ul um un pm au v"><div class="uo rl s"><div class="pm s ni"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="az b ba bb es et eu ev ew ex ey bm dr ds ez fa dw dx dy dz cy ea" rel="noopener">Read more from <!-- -->Towards Data Science</a></div></div></div></div></div><div class="s gj jf"><div class="n p"><div class="ao ap aq ar as at au v"></div></div></div></div></div></div><div class="up s uq ur"><div class="n p"><div class="ao ap aq ar as at au v"><div class="n ah"><div class="n o bt"><a href="https://medium.com/?source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm us ut bp uu uv" rel="noopener"><svg viewBox="0 0 3940 610" class="eu uw"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><div class="rt ux n bt uy bv"><p class="az b db ef uz"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm va bp uu uv" rel="noopener">About</a></p><p class="az b db ef uz"><a href="https://help.medium.com/hc/en-us?source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm va bp uu uv" rel="noopener">Help</a></p><p class="az b db ef uz"><a href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm va bp uu uv" rel="noopener">Legal</a></p></div></div><div class="aj vb vc bv"><p class="az b db ef vd">Get the Medium app</p></div><div class="aj vb ve bv vf"><div class="vg s"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm us ut bp uu uv" rel="noopener nofollow"><img alt="A button that says &#x27;Download on the App Store&#x27;, and if clicked it will lead you to the iOS App store" class="" src="https://miro.medium.com/max/270/1*Crl55Tm6yDNMoucPo1tvDg.png" width="135" height="41"/></a></div><div class="s"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----2d94655a9b7b--------------------------------" class="cs di bf bg bh bi bj bk bl bm us ut bp uu uv" rel="noopener nofollow"><img alt="A button that says &#x27;Get it on, Google Play&#x27;, and if clicked it will lead you to the Google Play store" class="" src="https://miro.medium.com/max/270/1*W_RAPQ62h0em559zluJLdQ.png" width="135" height="41"/></a></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__ = "main-20210416-233736-e40e16886c"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"auroraPage":{"isAuroraPageEnabled":true},"bookReader":{"assets":{},"reader":{"currentAsset":null,"settingsPanelIsOpen":false,"settings":{"fontFamily":"CHARTER","fontScale":"M","publisherStyling":true,"textAlignment":"start","theme":"White","lineSpacing":0,"wordSpacing":0,"letterSpacing":0},"internalNavCounter":0,"currentSelection":null,"highlights":[]}},"cache":{"experimentGroupSet":false,"reason":"This request is not using the cache middleware worker","group":"disabled","tags":[],"serverVariantState":"","middlewareEnabled":false,"cacheStatus":"DYNAMIC"},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true}},"debug":{"requestId":"8526f0b4-5b55-46f1-942a-df5e69d8a5ef","branchDeployConfig":null,"hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"1ecc89f743d45f63","ot-tracer-traceid":"2e462ae0487e8510","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"tracing":{},"config":{"nodeEnv":"production","version":"main-20210416-233736-e40e16886c","isTaggedVersion":false,"target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"main-20210416-233736-e40e16886c"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"main-20210416-233736-e40e16886c","commit":"e40e16886cfaae60b10df8d14b12e22abd7846b3"}},"datacenter":"us"},"googleAnalyticsCode":"UA-24232453-2","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium"},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e","9dc80918cc93","8a9336e5bb4","cef6983b292","54c98c43354d","193b68bd4fba","b7e45b22fec3","55760f21cdc5"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl"},"session":{"xsrf":"5c509f07f5e6"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"android_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"assign_default_topic_to_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"bane_add_user","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"bane_verify_domain","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"branch_seo_metadata","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"default_seo_post_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"deindex_from_external_search_threshold","valueType":{"__typename":"VariantFlagString","value":"1577865600000"}},{"__typename":"VariantFlag","name":"disable_android_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_resume_reading_toast","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_mobile_featured_chunk","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_post_recommended_from_friends_provider","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_local_currency","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_annual_renewal_reminder_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_parse_expires_at","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook_renewal_failure","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_about_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_general_admission","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_profile_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_sticky_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_tag_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_autotier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automated_mission_control_triggers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_blogrolls","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_text_me_the_app","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding_fonts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cleansweep_double_writes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_client_error_tracking","valueType":{"__typename":"VariantFlagString","value":"none"}},{"__typename":"VariantFlag","name":"enable_confirm_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cta_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_domain_v2_settings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_dedicated_series_tab_api_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_feature_logging","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_generation_pipeline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_earn_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_edit_alt_text","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_email_sign_in_captcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_embedding_based_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_end_of_post_cleanup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_evhead_com_to_ev_medium_com_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expanded_feature_chunk_pool","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_by_resend_rules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_expire_processor","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_from_creators_you_are_enjoying_below_todays_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_global_susi_modal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook_subscription_cancelled","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_highlander_member_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hightower_user_minimum_guarantee","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_who_to_follow_module","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_write_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_post_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_json_logs_trained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex_app_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kiln_for_digest_followed_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_notifications","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pay_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_homepage_for_selected_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_stories","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_unread_notification_count_mutation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_login_code_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_media_resource_try_catch","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_member_only_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_membership_remove_section_a","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_miro_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mission_control","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_digest_rendering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_welcome_digest_rendering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mute","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_checkout_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_collaborative_filtering_data","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_login_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_member_welcome_email_enhancement","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_three_dot_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_user_onboarding_email_enhancement","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_nsfw_filtering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_parsely","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_patronus_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_popularity_feature","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_page_nav_stickiness_removal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_tax_status_clarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_primary_topic_for_mobile","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_design_reminder","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_page_seo_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_publish_to_email_for_publication_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_receipt_notes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_all","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_edit_and_delete","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_moderation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_follow_feed_cache","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rtr_channel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_s3_sites","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_save_to_medium","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_signup_friction","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace_ranker_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_stripegate","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipalti_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trending_posts_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_triton_predictions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trumpland_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_twitter_auth_suggestions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_unfiltered_cf","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"entity_driven_subscription_milestone_2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"filter_last_seen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound"}},{"__typename":"VariantFlag","name":"google_sign_in_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_generic_home_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_pub_follow_email_opt_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"is_not_medium_subscriber","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_fastrak","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_stripe_express","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"low_signal_writer_level","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"make_nav_sticky","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"new_transition_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"provider_for_credit_card_form","valueType":{"__typename":"VariantFlagString","value":"BRAINTREE"}},{"__typename":"VariantFlag","name":"pub_sidebar","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefine_average_post_reading_time","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_evergreen_section","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_low_quality_content_from_tags","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_low_quality_posts_from_internal_search","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_post_post_similarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_suggested_topics_section","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"retrained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"sign_up_with_email_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"skip_sign_in_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"suppress_apple_missing_expires_date_alert","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"use_new_admin_topic_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":{"__ref":"User:dc52fe4ca374"},"meterPost({\"postId\":\"2d94655a9b7b\",\"postMeteringOptions\":{\"referrer\":\"\",\"sk\":null,\"source\":null}})":{"__ref":"MeteringInfo:{}"},"postResult({\"id\":\"2d94655a9b7b\"})":{"__ref":"Post:2d94655a9b7b"}},"User:dc52fe4ca374":{"id":"dc52fe4ca374","__typename":"User","username":"matildedeplace","name":"Matilde de Place","imageId":"0*4ukGw7lV6x1VWFVv","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"mat0503@hotmail.com","unverifiedEmail":"","createdAt":1590164250674,"isAuroraVisible":true,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"styleEditorOnboardingVersionSeen":0,"allowEmailAddressSharingEditorWriter":false,"hasSubdomain":false,"dismissableFlags":[],"hasWebMembershipTrialEnabled":false,"twitterScreenName":"","geolocation":{"__typename":"Geolocation","country":"DK"},"atsQualifiedAt":0},"MeteringInfo:{}":{"__typename":"MeteringInfo","postIds":[],"maxUnlockCount":3,"unlocksRemaining":0},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"CustomStyleSheet:63d23b36fcaa":{"id":"63d23b36fcaa","__typename":"CustomStyleSheet","global":{"__typename":"GlobalStyles","colorPalette":{"__typename":"StyleSheetColorPalette","primary":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}}},"background":null},"fonts":{"__typename":"StyleSheetFonts","font1":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font2":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font3":{"__typename":"StyleSheetFont","name":"SERIF_2"}}},"header":{"__typename":"HeaderStyles","backgroundColor":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"rgb":"355876","alpha":"99"},"postBackgroundColor":null,"backgroundImage":{"__ref":"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png"},"headerScale":"HEADER_SCALE_MEDIUM","horizontalAlignment":"CENTER","backgroundImageDisplayMode":"IMAGE_DISPLAY_MODE_FILL","backgroundImageVerticalAlignment":"END","backgroundColorDisplayMode":"COLOR_DISPLAY_MODE_SOLID","secondaryBackgroundColor":null,"nameColor":null,"nameTreatment":"NAME_TREATMENT_LOGO","postNameTreatment":"NAME_TREATMENT_LOGO","logoImage":{"__ref":"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png"},"logoScale":"HEADER_SCALE_LARGE","taglineColor":{"__typename":"ColorValue","rgb":"ffffff","alpha":"ff"},"taglineTreatment":"TAGLINE_TREATMENT_HEADER"},"navigation":{"__typename":"HeaderNavigation","navItems":[{"__typename":"HeaderNavigationItem","name":"Editors' Picks","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:editors-pick"}],"tagSlugs":["editors-pick"]},{"__typename":"HeaderNavigationItem","name":"Features","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:tds-features"}],"tagSlugs":["tds-features"]},{"__typename":"HeaderNavigationItem","name":"Deep Dives","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:deep-dives"}],"tagSlugs":["deep-dives"]},{"__typename":"HeaderNavigationItem","name":"Grow","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-get-the-most-out-of-towards-data-science-3bf37f75a345","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]},{"__typename":"HeaderNavigationItem","name":"Contribute","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fquestions-96667b06af5","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]}]},"postBody":null,"blogroll":null},"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png":{"id":"1*sfUruIusLq6tbpLx0sDYZQ.png","__typename":"ImageMetadata","originalWidth":1401},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","__typename":"ImageMetadata","originalWidth":337,"originalHeight":122},"User:7e12c71dfa81":{"id":"7e12c71dfa81","__typename":"User","atsQualifiedAt":1612205680542},"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png":{"id":"1*eLxNtw6hQ4-3HrHda5BCCw.png","__typename":"ImageMetadata"},"NewsletterV3:d6fe9076899":{"id":"d6fe9076899","__typename":"NewsletterV3","slug":"the-variable","isSubscribed":false,"showPromo":true,"name":"The Variable","description":"Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.","type":"NEWSLETTER_TYPE_COLLECTION","user":{"__ref":"User:895063a310f4"},"collection":{"__ref":"Collection:7f60cf5620c9"}},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","__typename":"Collection","domain":"towardsdatascience.com","googleAnalyticsId":null,"slug":"towards-data-science","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","isAuroraVisible":true,"favicon":{"__ref":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png"},"name":"Towards Data Science","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"customStyleSheet":{"__ref":"CustomStyleSheet:63d23b36fcaa"},"tagline":"A Medium publication sharing concepts, ideas and codes.","isAuroraEligible":true,"viewerIsEditor":false,"logo":{"__ref":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"},"navItems":[{"__typename":"NavItem","title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Video","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fvideo\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"★","url":"https:\u002F\u002Ftowardsdatascience.com\u002Feditors-picks\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"About","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fabout-us\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM"}],"creator":{"__ref":"User:7e12c71dfa81"},"subscriberCount":581456,"avatar":{"__ref":"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png"},"newsletterV3":{"__ref":"NewsletterV3:d6fe9076899"},"viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"canToggleEmail":true,"isUserSubscribedToCollectionEmails":false,"viewerIsMuting":false,"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","status":"ACTIVE","isSubdomain":false}},"ptsQualifiedAt":1616092952992},"User:b24112d01863":{"id":"b24112d01863","__typename":"User","isFollowing":false,"viewerIsUser":false,"customStyleSheet":null,"isSuspended":false,"name":"Chris Yoon","hasCompletedProfile":false,"bio":"Student in NYC. https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fchris-yoon-75847418b\u002F","imageId":"1*mNiVdAV2aE_fZF9dLG09Ew.jpeg","username":"thechrisyoon","customDomainState":null,"isAuroraVisible":true,"createdAt":0,"mediumMemberAt":0,"lastPostCreatedAt":0,"socialStats":{"__typename":"SocialStats","followerCount":463,"followingCount":13},"hasSubdomain":false,"isAllowEdsEnabled":false,"isBlocking":false,"isMuting":false,"allowNotes":true,"newsletterV3":null,"twitterScreenName":"","followedCollections":7,"atsQualifiedAt":1612205469501},"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png":{"id":"1*AGyTPCaRzVqL77kFwUwHKg.png","__typename":"ImageMetadata","originalWidth":1376,"originalHeight":429},"Tag:editors-pick":{"id":"editors-pick","__typename":"Tag","normalizedTagSlug":""},"Tag:tds-features":{"id":"tds-features","__typename":"Tag","normalizedTagSlug":""},"Tag:deep-dives":{"id":"deep-dives","__typename":"Tag","normalizedTagSlug":""},"Topic:1eca0103fff3":{"id":"1eca0103fff3","__typename":"Topic","name":"Machine Learning","slug":"machine-learning","isFollowing":null},"Paragraph:835f716a2f5a_0":{"id":"835f716a2f5a_0","__typename":"Paragraph","name":"00d2","text":"Deep Deterministic Policy Gradients Explained","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_1":{"id":"835f716a2f5a_1","__typename":"Paragraph","name":"c1b7","text":"Reinforcement Learning in Continuous Action Spaces","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":50,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_2":{"id":"835f716a2f5a_2","__typename":"Paragraph","name":"e127","text":"This post is a thorough review of Deepmind’s publication “Continuous Control With Deep Reinforcement Learning” (Lillicrap et al, 2015), in which the Deep Deterministic Policy Gradients (DDPG) is presented, and is written for people who wish to understand the DDPG algorithm. If you are interested only in the implementation, you can skip to the final section of this post.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":15,"end":23,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":57,"end":110,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_3":{"id":"835f716a2f5a_3","__typename":"Paragraph","name":"80a6","text":"This post is written with the assumption that the reader is familiar with basic reinforcement learning concepts, value & policy learning, and actor critic methods. If you are not completely familiar with those concepts, I have also written about policy gradients and actor critic methods.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":246,"end":262,"type":"A","href":"https:\u002F\u002Fmedium.com\u002F@thechrisyoon\u002Fderiving-policy-gradients-and-implementing-reinforce-f887949bd63","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":267,"end":287,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Funderstanding-actor-critic-methods-931b97b6df3f","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_4":{"id":"835f716a2f5a_4","__typename":"Paragraph","name":"5173","text":"Familiarity with python and PyTorch will also be really helpful for reading through this post. If you are not familiar with PyTorch, try to follow the code snippets as if they are pseudo-code.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_5":{"id":"835f716a2f5a_5","__typename":"Paragraph","name":"b762","text":"Going through the paper","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_6":{"id":"835f716a2f5a_6","__typename":"Paragraph","name":"f6ec","text":"Network Schematics","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_7":{"id":"835f716a2f5a_7","__typename":"Paragraph","name":"d6a9","text":"DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_8":{"id":"835f716a2f5a_8","__typename":"Paragraph","name":"7f02","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*-87grz5iUZK4i7NCH1ldbw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_9":{"id":"835f716a2f5a_9","__typename":"Paragraph","name":"7e9f","text":"The Q network and policy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_10":{"id":"835f716a2f5a_10","__typename":"Paragraph","name":"8c05","text":"The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improve stability in learning. Here’s why: In methods that do not use target networks, the update equations of the network are interdependent on the values calculated by the network itself, which makes it prone to divergence. For example:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_11":{"id":"835f716a2f5a_11","__typename":"Paragraph","name":"c207","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*rWEAu4HKQIzFTJw3i70Mkg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_12":{"id":"835f716a2f5a_12","__typename":"Paragraph","name":"7df4","text":"So, we have the standard Actor & Critic architecture for the deterministic policy network and the Q network:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_13":{"id":"835f716a2f5a_13","__typename":"Paragraph","name":"a3d5","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:27f5a4d2f2d73aaf21fb3f867ad300fb"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_14":{"id":"835f716a2f5a_14","__typename":"Paragraph","name":"607d","text":"And we initialize the networks and target networks as:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_15":{"id":"835f716a2f5a_15","__typename":"Paragraph","name":"ea2b","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:7243d944c75c21f3a379ee1a76388431"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_16":{"id":"835f716a2f5a_16","__typename":"Paragraph","name":"ca72","text":"Learning","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_17":{"id":"835f716a2f5a_17","__typename":"Paragraph","name":"f488","text":"So, here’s the pseudo-code of the algorithm that we want to implement:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_18":{"id":"835f716a2f5a_18","__typename":"Paragraph","name":"86dd","text":"Taken from “Continuous Control With Deep Reinforcement Learning” (Lillicrap et al, 2015)","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*BVST6rlxL2csw3vxpeBS8Q.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":11,"end":64,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_19":{"id":"835f716a2f5a_19","__typename":"Paragraph","name":"c6fd","text":"We are going to break this down into:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_20":{"id":"835f716a2f5a_20","__typename":"Paragraph","name":"f31c","text":"Experience replay","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_21":{"id":"835f716a2f5a_21","__typename":"Paragraph","name":"3946","text":"Actor & Critic network updates","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_22":{"id":"835f716a2f5a_22","__typename":"Paragraph","name":"ea4f","text":"Target network updates","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_23":{"id":"835f716a2f5a_23","__typename":"Paragraph","name":"2ff1","text":"Exploration","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_24":{"id":"835f716a2f5a_24","__typename":"Paragraph","name":"7e7b","text":"Replay Buffer","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_25":{"id":"835f716a2f5a_25","__typename":"Paragraph","name":"8060","text":"As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters. During each trajectory roll-out, we save all the experience tuples (state, action, reward, next_state) and store them in a finite-sized cache — a “replay buffer.” Then, we sample random mini-batches of experience from the replay buffer when we update the value and policy networks.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_26":{"id":"835f716a2f5a_26","__typename":"Paragraph","name":"e359","text":"Here’s how the replay buffer looks like:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_27":{"id":"835f716a2f5a_27","__typename":"Paragraph","name":"8e64","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:09eec8f3a2a599881c2bd8b20b1daca8"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_28":{"id":"835f716a2f5a_28","__typename":"Paragraph","name":"23e4","text":"Why do we use experience replay? In optimization asks, we want the data to be independently distributed. This fails to be the case when we optimize a sequential decision process in an on-policy way, because the data then would not be independent of each other. When we store them in a replay buffer and take random batches for training, we overcome this issue.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_29":{"id":"835f716a2f5a_29","__typename":"Paragraph","name":"795e","text":"Actor (Policy) & Critic (Value) Network Updates","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_30":{"id":"835f716a2f5a_30","__typename":"Paragraph","name":"bf97","text":"The value network is updated similarly as is done in Q-learning. The updated Q value is obtained by the Bellman equation:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_31":{"id":"835f716a2f5a_31","__typename":"Paragraph","name":"fb08","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*UJ5fl6SemqEjoMX9Ns0jSA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_32":{"id":"835f716a2f5a_32","__typename":"Paragraph","name":"3de1","text":"However, in DDPG, the next-state Q values are calculated with the target value network and target policy network. Then, we minimize the mean-squared loss between the updated Q value and the original Q value:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":22,"end":112,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_33":{"id":"835f716a2f5a_33","__typename":"Paragraph","name":"ba36","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*96LU4dbopEncLQ9vc3pCzg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_34":{"id":"835f716a2f5a_34","__typename":"Paragraph","name":"1486","text":"* Note that the original Q value is calculated with the value network, not the target value network.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":100,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_35":{"id":"835f716a2f5a_35","__typename":"Paragraph","name":"bc02","text":"In code, this looks like:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_36":{"id":"835f716a2f5a_36","__typename":"Paragraph","name":"5b88","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:26da469d1fbf44e1c93af456cbcdfa81"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_37":{"id":"835f716a2f5a_37","__typename":"Paragraph","name":"2947","text":"For the policy function, our objective is to maximize the expected return:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_38":{"id":"835f716a2f5a_38","__typename":"Paragraph","name":"8d7f","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*PF4odMpSZi1LWmcf7JhBZQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_39":{"id":"835f716a2f5a_39","__typename":"Paragraph","name":"2b5c","text":"To calculate the policy loss, we take the derivative of the objective function with respect to the policy parameter. Keep in mind that the actor (policy) function is differentiable, so we have to apply the chain rule.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_40":{"id":"835f716a2f5a_40","__typename":"Paragraph","name":"ede5","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*IFFNXK4CZMcmJ4g5nYstiw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_41":{"id":"835f716a2f5a_41","__typename":"Paragraph","name":"53c5","text":"But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_42":{"id":"835f716a2f5a_42","__typename":"Paragraph","name":"d2a9","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ta1Nn1sI6juWuPMJc3qepA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_43":{"id":"835f716a2f5a_43","__typename":"Paragraph","name":"5ee0","text":"In code, this looks like:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_44":{"id":"835f716a2f5a_44","__typename":"Paragraph","name":"ba5e","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:59b8aca2dd205d7dd236a63f0fbb4c66"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_45":{"id":"835f716a2f5a_45","__typename":"Paragraph","name":"a1e1","text":"Where the optimizers use Adaptive Moment Estimation (ADAM):","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_46":{"id":"835f716a2f5a_46","__typename":"Paragraph","name":"2116","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:555f4db7bec0d33274cfc2d423dba095"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_47":{"id":"835f716a2f5a_47","__typename":"Paragraph","name":"f77d","text":"Target Network Updates","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_48":{"id":"835f716a2f5a_48","__typename":"Paragraph","name":"a63c","text":"We make a copy of the target network parameters and have them slowly track those of the learned networks via “soft updates,” as illustrated below:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_49":{"id":"835f716a2f5a_49","__typename":"Paragraph","name":"9f48","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*LBlJpAQBLF95LsheOusmuA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_50":{"id":"835f716a2f5a_50","__typename":"Paragraph","name":"7a47","text":"This can be implemented very simply:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_51":{"id":"835f716a2f5a_51","__typename":"Paragraph","name":"057b","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:1ee73b7fd42343e8dfd0b78e0a7a43a6"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_52":{"id":"835f716a2f5a_52","__typename":"Paragraph","name":"8b92","text":"Exploration","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_53":{"id":"835f716a2f5a_53","__typename":"Paragraph","name":"4892","text":"In Reinforcement learning for discrete action spaces, exploration is done via probabilistically selecting a random action (such as epsilon-greedy or Boltzmann exploration). For continuous action spaces, exploration is done via adding noise to the action itself (there is also the parameter space noise but we will skip that for now). In the DDPG paper, the authors use Ornstein-Uhlenbeck Process to add noise to the action output (Uhlenbeck & Ornstein, 1930):","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":369,"end":396,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":408,"end":409,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_54":{"id":"835f716a2f5a_54","__typename":"Paragraph","name":"5739","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*LmL-P-_o10NWtQcF2PmkBg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_55":{"id":"835f716a2f5a_55","__typename":"Paragraph","name":"772f","text":"The Ornstein-Uhlenbeck Process generates noise that is correlated with the previous noise, as to prevent the noise from canceling out or “freezing” the overall dynamics [1]. Wikipedia provides a thorough explanation of the Ornstein-Uhlenbeck Process.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":174,"end":250,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FOrnstein%E2%80%93Uhlenbeck_process","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":169,"end":172,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":4,"end":30,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":223,"end":249,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_56":{"id":"835f716a2f5a_56","__typename":"Paragraph","name":"19ce","text":"Here’s a python implementation written by Pong et al:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_57":{"id":"835f716a2f5a_57","__typename":"Paragraph","name":"7a70","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:4a3ff43f00c655f8cff5c126930711f5"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_58":{"id":"835f716a2f5a_58","__typename":"Paragraph","name":"7bad","text":"So we input the action produced by the actor network into get_action() function, and get a new action to which the temporally correlated noise is added.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":58,"end":70,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_59":{"id":"835f716a2f5a_59","__typename":"Paragraph","name":"0336","text":"We are all set now!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_60":{"id":"835f716a2f5a_60","__typename":"Paragraph","name":"dd17","text":"Putting them all together","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_61":{"id":"835f716a2f5a_61","__typename":"Paragraph","name":"dbe0","text":"We have here the Replay Buffer, the Ornstein-Uhlenbeck Process, and the normalized Action Wrapper for OpenAI Gym continuous control environments in utils.py:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":148,"end":156,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_62":{"id":"835f716a2f5a_62","__typename":"Paragraph","name":"56eb","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:6c3a98c8c90ecd7828a2a409861cf83f"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_63":{"id":"835f716a2f5a_63","__typename":"Paragraph","name":"899e","text":"And the Actor & Critic networks in models.py:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":35,"end":44,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_64":{"id":"835f716a2f5a_64","__typename":"Paragraph","name":"8d43","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:6f3c48fca9e2f10238af633845ace59a"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_65":{"id":"835f716a2f5a_65","__typename":"Paragraph","name":"c8c8","text":"And the DDPG agent in ddpg.py:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":22,"end":29,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_66":{"id":"835f716a2f5a_66","__typename":"Paragraph","name":"b3cc","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:0150382cab5f8d9d58c606e521ecf5bc"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_67":{"id":"835f716a2f5a_67","__typename":"Paragraph","name":"6536","text":"And the test in main.py:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":16,"end":23,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_68":{"id":"835f716a2f5a_68","__typename":"Paragraph","name":"8937","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:e31c44256559b442be98a01fc10682c9"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_69":{"id":"835f716a2f5a_69","__typename":"Paragraph","name":"0549","text":"And we can see if the DDPG agent learns optimal policy for the classic Inverted Pendulum task:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_70":{"id":"835f716a2f5a_70","__typename":"Paragraph","name":"6727","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*miNYafAHXGXMyZx_-NKV0A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_71":{"id":"835f716a2f5a_71","__typename":"Paragraph","name":"48d2","text":"That’s it for DDPG!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_72":{"id":"835f716a2f5a_72","__typename":"Paragraph","name":"3ee2","text":"Find full implementation here:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_73":{"id":"835f716a2f5a_73","__typename":"Paragraph","name":"d03d","text":"thechrisyoon08\u002FReinforcement-Learning\nModular implementations of reinforcement learning algorithms with Python and PyTorch …github.com","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Fgithub.com\u002Fthechrisyoon08\u002FReinforcement-Learning","thumbnailImageId":"0*qBUhy8WSI23EEXme"},"markups":[{"__typename":"Markup","start":0,"end":134,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fthechrisyoon08\u002FReinforcement-Learning","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":37,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":38,"end":124,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_74":{"id":"835f716a2f5a_74","__typename":"Paragraph","name":"aaac","text":"References","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:835f716a2f5a_75":{"id":"835f716a2f5a_75","__typename":"Paragraph","name":"474d","text":"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra, Continuous control with deep reinforcement learning, CoRR abs\u002F1509.02971 (2015).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":210,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1509.02971","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":130,"end":181,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:835f716a2f5a_76":{"id":"835f716a2f5a_76","__typename":"Paragraph","name":"e577","text":"[1] Edouard Leurent’s answer to Quora post “Why do we use the Ornstein Uhlenbeck Process in the exploration of DDPG?”","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":117,"type":"A","href":"https:\u002F\u002Fqr.ae\u002FTW8NAa","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":4,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":42,"end":44,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":44,"end":117,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"ImageMetadata:1*-87grz5iUZK4i7NCH1ldbw.png":{"id":"1*-87grz5iUZK4i7NCH1ldbw.png","__typename":"ImageMetadata","originalHeight":633,"originalWidth":1250,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*rWEAu4HKQIzFTJw3i70Mkg.png":{"id":"1*rWEAu4HKQIzFTJw3i70Mkg.png","__typename":"ImageMetadata","originalHeight":150,"originalWidth":921,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:27f5a4d2f2d73aaf21fb3f867ad300fb":{"id":"27f5a4d2f2d73aaf21fb3f867ad300fb","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"models.py"},"MediaResource:7243d944c75c21f3a379ee1a76388431":{"id":"7243d944c75c21f3a379ee1a76388431","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"initialize.py"},"ImageMetadata:1*BVST6rlxL2csw3vxpeBS8Q.png":{"id":"1*BVST6rlxL2csw3vxpeBS8Q.png","__typename":"ImageMetadata","originalHeight":399,"originalWidth":542,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:09eec8f3a2a599881c2bd8b20b1daca8":{"id":"09eec8f3a2a599881c2bd8b20b1daca8","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"replay_buffer.py"},"ImageMetadata:1*UJ5fl6SemqEjoMX9Ns0jSA.png":{"id":"1*UJ5fl6SemqEjoMX9Ns0jSA.png","__typename":"ImageMetadata","originalHeight":108,"originalWidth":1292,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*96LU4dbopEncLQ9vc3pCzg.png":{"id":"1*96LU4dbopEncLQ9vc3pCzg.png","__typename":"ImageMetadata","originalHeight":217,"originalWidth":1208,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:26da469d1fbf44e1c93af456cbcdfa81":{"id":"26da469d1fbf44e1c93af456cbcdfa81","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"critic_update.py"},"ImageMetadata:1*PF4odMpSZi1LWmcf7JhBZQ.png":{"id":"1*PF4odMpSZi1LWmcf7JhBZQ.png","__typename":"ImageMetadata","originalHeight":92,"originalWidth":1067,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*IFFNXK4CZMcmJ4g5nYstiw.png":{"id":"1*IFFNXK4CZMcmJ4g5nYstiw.png","__typename":"ImageMetadata","originalHeight":83,"originalWidth":1175,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*ta1Nn1sI6juWuPMJc3qepA.png":{"id":"1*ta1Nn1sI6juWuPMJc3qepA.png","__typename":"ImageMetadata","originalHeight":217,"originalWidth":2167,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:59b8aca2dd205d7dd236a63f0fbb4c66":{"id":"59b8aca2dd205d7dd236a63f0fbb4c66","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"policy_update.py"},"MediaResource:555f4db7bec0d33274cfc2d423dba095":{"id":"555f4db7bec0d33274cfc2d423dba095","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"optimizers.py"},"ImageMetadata:1*LBlJpAQBLF95LsheOusmuA.png":{"id":"1*LBlJpAQBLF95LsheOusmuA.png","__typename":"ImageMetadata","originalHeight":392,"originalWidth":858,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:1ee73b7fd42343e8dfd0b78e0a7a43a6":{"id":"1ee73b7fd42343e8dfd0b78e0a7a43a6","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"target_update.py"},"ImageMetadata:1*LmL-P-_o10NWtQcF2PmkBg.png":{"id":"1*LmL-P-_o10NWtQcF2PmkBg.png","__typename":"ImageMetadata","originalHeight":92,"originalWidth":792,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:4a3ff43f00c655f8cff5c126930711f5":{"id":"4a3ff43f00c655f8cff5c126930711f5","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"OUnoise.py"},"MediaResource:6c3a98c8c90ecd7828a2a409861cf83f":{"id":"6c3a98c8c90ecd7828a2a409861cf83f","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"utils.py"},"MediaResource:6f3c48fca9e2f10238af633845ace59a":{"id":"6f3c48fca9e2f10238af633845ace59a","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"models.py"},"MediaResource:0150382cab5f8d9d58c606e521ecf5bc":{"id":"0150382cab5f8d9d58c606e521ecf5bc","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"ddpg.py"},"MediaResource:e31c44256559b442be98a01fc10682c9":{"id":"e31c44256559b442be98a01fc10682c9","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"test.py"},"ImageMetadata:1*miNYafAHXGXMyZx_-NKV0A.png":{"id":"1*miNYafAHXGXMyZx_-NKV0A.png","__typename":"ImageMetadata","originalHeight":480,"originalWidth":640,"focusPercentX":null,"focusPercentY":null,"alt":null},"User:895063a310f4":{"id":"895063a310f4","__typename":"User","name":"Ludovic Benistant"},"Tag:reinforcement-learning":{"id":"reinforcement-learning","__typename":"Tag","displayTitle":"Reinforcement Learning","normalizedTagSlug":""},"Tag:machine-learning":{"id":"machine-learning","__typename":"Tag","displayTitle":"Machine Learning","normalizedTagSlug":""},"Tag:artificial-intelligence":{"id":"artificial-intelligence","__typename":"Tag","displayTitle":"Artificial Intelligence","normalizedTagSlug":""},"Tag:control":{"id":"control","__typename":"Tag","displayTitle":"Control","normalizedTagSlug":""},"Post:2d94655a9b7b":{"id":"2d94655a9b7b","__typename":"Post","canonicalUrl":"","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{\"referrer\":\"\",\"sk\":null,\"source\":null}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","isCacheableContent":false,"bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:835f716a2f5a_0"},{"__ref":"Paragraph:835f716a2f5a_1"},{"__ref":"Paragraph:835f716a2f5a_2"},{"__ref":"Paragraph:835f716a2f5a_3"},{"__ref":"Paragraph:835f716a2f5a_4"},{"__ref":"Paragraph:835f716a2f5a_5"},{"__ref":"Paragraph:835f716a2f5a_6"},{"__ref":"Paragraph:835f716a2f5a_7"},{"__ref":"Paragraph:835f716a2f5a_8"},{"__ref":"Paragraph:835f716a2f5a_9"},{"__ref":"Paragraph:835f716a2f5a_10"},{"__ref":"Paragraph:835f716a2f5a_11"},{"__ref":"Paragraph:835f716a2f5a_12"},{"__ref":"Paragraph:835f716a2f5a_13"},{"__ref":"Paragraph:835f716a2f5a_14"},{"__ref":"Paragraph:835f716a2f5a_15"},{"__ref":"Paragraph:835f716a2f5a_16"},{"__ref":"Paragraph:835f716a2f5a_17"},{"__ref":"Paragraph:835f716a2f5a_18"},{"__ref":"Paragraph:835f716a2f5a_19"},{"__ref":"Paragraph:835f716a2f5a_20"},{"__ref":"Paragraph:835f716a2f5a_21"},{"__ref":"Paragraph:835f716a2f5a_22"},{"__ref":"Paragraph:835f716a2f5a_23"},{"__ref":"Paragraph:835f716a2f5a_24"},{"__ref":"Paragraph:835f716a2f5a_25"},{"__ref":"Paragraph:835f716a2f5a_26"},{"__ref":"Paragraph:835f716a2f5a_27"},{"__ref":"Paragraph:835f716a2f5a_28"},{"__ref":"Paragraph:835f716a2f5a_29"},{"__ref":"Paragraph:835f716a2f5a_30"},{"__ref":"Paragraph:835f716a2f5a_31"},{"__ref":"Paragraph:835f716a2f5a_32"},{"__ref":"Paragraph:835f716a2f5a_33"},{"__ref":"Paragraph:835f716a2f5a_34"},{"__ref":"Paragraph:835f716a2f5a_35"},{"__ref":"Paragraph:835f716a2f5a_36"},{"__ref":"Paragraph:835f716a2f5a_37"},{"__ref":"Paragraph:835f716a2f5a_38"},{"__ref":"Paragraph:835f716a2f5a_39"},{"__ref":"Paragraph:835f716a2f5a_40"},{"__ref":"Paragraph:835f716a2f5a_41"},{"__ref":"Paragraph:835f716a2f5a_42"},{"__ref":"Paragraph:835f716a2f5a_43"},{"__ref":"Paragraph:835f716a2f5a_44"},{"__ref":"Paragraph:835f716a2f5a_45"},{"__ref":"Paragraph:835f716a2f5a_46"},{"__ref":"Paragraph:835f716a2f5a_47"},{"__ref":"Paragraph:835f716a2f5a_48"},{"__ref":"Paragraph:835f716a2f5a_49"},{"__ref":"Paragraph:835f716a2f5a_50"},{"__ref":"Paragraph:835f716a2f5a_51"},{"__ref":"Paragraph:835f716a2f5a_52"},{"__ref":"Paragraph:835f716a2f5a_53"},{"__ref":"Paragraph:835f716a2f5a_54"},{"__ref":"Paragraph:835f716a2f5a_55"},{"__ref":"Paragraph:835f716a2f5a_56"},{"__ref":"Paragraph:835f716a2f5a_57"},{"__ref":"Paragraph:835f716a2f5a_58"},{"__ref":"Paragraph:835f716a2f5a_59"},{"__ref":"Paragraph:835f716a2f5a_60"},{"__ref":"Paragraph:835f716a2f5a_61"},{"__ref":"Paragraph:835f716a2f5a_62"},{"__ref":"Paragraph:835f716a2f5a_63"},{"__ref":"Paragraph:835f716a2f5a_64"},{"__ref":"Paragraph:835f716a2f5a_65"},{"__ref":"Paragraph:835f716a2f5a_66"},{"__ref":"Paragraph:835f716a2f5a_67"},{"__ref":"Paragraph:835f716a2f5a_68"},{"__ref":"Paragraph:835f716a2f5a_69"},{"__ref":"Paragraph:835f716a2f5a_70"},{"__ref":"Paragraph:835f716a2f5a_71"},{"__ref":"Paragraph:835f716a2f5a_72"},{"__ref":"Paragraph:835f716a2f5a_73"},{"__ref":"Paragraph:835f716a2f5a_74"},{"__ref":"Paragraph:835f716a2f5a_75"},{"__ref":"Paragraph:835f716a2f5a_76"}],"sections":[{"__typename":"Section","name":"6946","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"2221","startIndex":5,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"a2f9","startIndex":12,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"1ebd","startIndex":16,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"a4ae","startIndex":60,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"68fe","startIndex":74,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"creator":{"__ref":"User:b24112d01863"},"customStyleSheet":{"__ref":"CustomStyleSheet:63d23b36fcaa"},"firstPublishedAt":1553087277366,"isLocked":false,"isPublished":true,"isShortform":false,"layerCake":3,"primaryTopic":{"__ref":"Topic:1eca0103fff3"},"title":"Deep Deterministic Policy Gradients Explained","isMarkedPaywallOnly":false,"readCreatorPostsCount":1,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fdeep-deterministic-policy-gradients-explained-2d94655a9b7b","isLimitedState":false,"visibility":"PUBLIC","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"newsletterId":"","sequence":null,"tags":[{"__ref":"Tag:reinforcement-learning"},{"__ref":"Tag:machine-learning"},{"__ref":"Tag:artificial-intelligence"},{"__ref":"Tag:control"}],"topics":[{"__typename":"Topic","topicId":"1eca0103fff3","name":"Machine Learning"}],"viewerClapCount":0,"showSubscribeToProfilePromo":false,"showSubscribeToCollectionNewsletterV3Promo":true,"inResponseToPostResult":null,"isNewsletter":false,"socialTitle":"","socialDek":"","noIndex":null,"curationStatus":null,"metaDescription":"","latestPublishedAt":1558631407156,"readingTime":5.005660377358491,"previewContent":{"__typename":"PreviewContent","subtitle":"Reinforcement Learning in Continuous Action Spaces"},"previewImage":{"__ref":"ImageMetadata:1*-87grz5iUZK4i7NCH1ldbw.png"},"creatorPartnerProgramEnrollmentStatus":"PERMISSION_DENIED","clapCount":1044,"lockedSource":"LOCKED_POST_SOURCE_NONE","isSuspended":false,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":0,"responseDistribution":"NOT_DISTRIBUTED","shareKey":null,"collaborators":[],"translationSourcePost":null,"inResponseToMediaResource":null,"audioVersionUrl":"","seoTitle":"","updatedAt":1558631407386,"shortformType":"SHORTFORM_TYPE_LINK","structuredData":"","seoDescription":"","postResponses":{"__typename":"PostResponses","count":9},"latestPublishedVersion":"835f716a2f5a","isPublishToEmail":false,"readingList":"READING_LIST_NONE","voterCount":190,"recommenders":[]}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.ae72f892.js"></script><script src="https://cdn-client.medium.com/lite/static/js/8464.d0ef046c.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.2b220e26.js"></script><script src="https://cdn-client.medium.com/lite/static/js/5573.159bf40f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/instrumentation.8ea15b6a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.be183697.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1752.a348f767.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2833.383a48e6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8342.6aa0b45e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4930.d16bc692.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9692.aa4b76ec.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4586.06957e16.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5064.e0fb94df.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9046.c85ec5d4.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2846.78dd20ea.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9990.4f008794.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7012.1bb4e9a4.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9972.9b66804a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4379.7456fbc7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5127.59840da2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8751.eca11143.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2955.4ef7996b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7131.f2494ca5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6163.8b10fb5b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8127.1c99358b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2514.eb9a5ab7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6371.e8059cd7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7496.e26cbb9b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1725.155e62d8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3874.32302d9b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8953.20430471.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8286.cb05fae9.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9454.fe0523e9.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/Post.aa5fea13.chunk.js"></script><script>window.main();</script></body></html>